{
  "context": [
    {
      "node_id": "tb1_node91",
      "content": "It remains only to describe how to identify and add all the safe edges to $F$ . Suppose $F$ has more than one component, since otherwise we’re already done. The following subroutine computes an array $s a f e [ 1 ldots V ]$ of safe edges, where safe[i] is the minimum-weight edge with one endpoint in the ith component of $F$ , by a brute force examination of every edge in $G$ . For each edge $u nu$ , if $u$ and $nu$ are in the same component, then $u nu$ is either useless or already an edge in $F$ . Otherwise, we compare the weight of $u nu$ to the weights of safe $left[ c o m p ( u ) right]$ and sa $f e [ c o m p ( nu ) ]$ and update the array entries if necessary. Once we have identified all the safe edges, we add each edge safe[i] to $F$ . \nEach call to CountAndLabel runs in $O ( V )$ time, because the forest $F$ has at most $V - 1$ edges. AddAllSafeEdges runs in $O ( V + E )$ time, because we spend constant time on each vertex, each edge of $G$ , and each component of $F$ . Because the input graph is connected, we have $V leq E + 1$ . It follows that each iteration of the while loop of Borůvka takes $O ( E )$ time. \nEach iteration reduces the number of components of $F$ by at least a factor of two—in the worst case, the components of $F$ coalesce in pairs. Because $F$ initially has $V$ components, the while loop iterates at most $O ( log V )$ times. We conclude that the overall running time of Borůvka’s algorithm is $ { boldsymbol { O } } ( E log V )$ . \nThis is the MST Algorithm You Want \nDespite its relatively obscure origin, early Western algorithms researchers were aware of Borůvka’s algorithm, but dismissed it as being “too complicated”. \nAs a result, despite its simplicity and efficiency, most algorithms and data structures textbooks unfortunately do not even mention Borůvka’s algorithm. This omission is a serious mistake; Borůvka’s algorithm has several distinct advantages over other classical MST algorithms. \n• Borůvka’s algorithm often runs faster than its $O ( E log V )$ worst-case running time. The number of components in $F$ can drop by significantly more than a factor of 2 in a single iteration, reducing the number of iterations below the worst-case $lceil log _ { 2 } V rceil$ .   \n• A slight reformulation of Borůvka’s algorithm (actually closer to Borůvka’s original presentation) actually runs in $O ( E )$ time for a broad class of interesting graphs, including graphs that can be drawn in the plane without edge crossings. In contrast, the time analysis for the other two algorithms applies to all graphs.   \n• Borůvka’s algorithm allows for significant parallelism; in each iteration, each component of $F$ can be handled in a separate independent thread. This implicit parallelism allows for even faster performance on multicore or distributed systems. In contrast, the other two classical MST algorithms are intrinsically serial.   \n• Several more recent minimum-spanning-tree algorithms are faster even in the worst case than the classical algorithms described here. All of these faster algorithms are generalizations of Borůvka’s algorithm. \nIn short, if you ever need to implement a minimum-spanning-tree algorithm, use Borůvka. On the other hand, if you want to prove things about minimum spanning trees effectively, you really need to know the next two algorithms as well. \n7.4 Jarník’s (“Prim’s”) Algorithm \nThe next oldest minimum spanning tree algorithm was first described by the Czech mathematician Vojtěch Jarník in a 1929 letter to Borůvka; Jarník published his discovery the following year. The algorithm was independently rediscovered by Joseph Kruskal in 1956, (arguably) by Robert Prim in 1957, by Harry Loberman and Arnold Weinberger in 1957, and finally by Edsger Dijkstra in 1958. Prim, Lobermand and Weinberger, and Dijkstra all (eventually) knew of and even cited Kruskal’s paper, but since Kruskal also described two other minimumspanning-tree algorithms in the same paper, this algorithm is usually called “Prim’s algorithm”, or sometimes “the Prim/Dijkstra algorithm”, even though by 1958 Dijkstra already had another algorithm (inappropriately) named after him. \nIn Jarník’s algorithm, the intermediate forest $F$ has only one nontrivial component $T$ ; all the other components are isolated vertices. Initially, $T$ consists of a single arbitrary vertex of the graph. The algorithm repeats the following step until $T$ spans the whole graph:",
      "metadata": {
        "content": "It remains only to describe how to identify and add all the safe edges to $F$ . Suppose $F$ has more than one component, since otherwise we’re already done. The following subroutine computes an array $s a f e [ 1 ldots V ]$ of safe edges, where safe[i] is the minimum-weight edge with one endpoint in the ith component of $F$ , by a brute force examination of every edge in $G$ . For each edge $u nu$ , if $u$ and $nu$ are in the same component, then $u nu$ is either useless or already an edge in $F$ . Otherwise, we compare the weight of $u nu$ to the weights of safe $left[ c o m p ( u ) right]$ and sa $f e [ c o m p ( nu ) ]$ and update the array entries if necessary. Once we have identified all the safe edges, we add each edge safe[i] to $F$ . \nEach call to CountAndLabel runs in $O ( V )$ time, because the forest $F$ has at most $V - 1$ edges. AddAllSafeEdges runs in $O ( V + E )$ time, because we spend constant time on each vertex, each edge of $G$ , and each component of $F$ . Because the input graph is connected, we have $V leq E + 1$ . It follows that each iteration of the while loop of Borůvka takes $O ( E )$ time. \nEach iteration reduces the number of components of $F$ by at least a factor of two—in the worst case, the components of $F$ coalesce in pairs. Because $F$ initially has $V$ components, the while loop iterates at most $O ( log V )$ times. We conclude that the overall running time of Borůvka’s algorithm is $ { boldsymbol { O } } ( E log V )$ . \nThis is the MST Algorithm You Want \nDespite its relatively obscure origin, early Western algorithms researchers were aware of Borůvka’s algorithm, but dismissed it as being “too complicated”. \nAs a result, despite its simplicity and efficiency, most algorithms and data structures textbooks unfortunately do not even mention Borůvka’s algorithm. This omission is a serious mistake; Borůvka’s algorithm has several distinct advantages over other classical MST algorithms. \n• Borůvka’s algorithm often runs faster than its $O ( E log V )$ worst-case running time. The number of components in $F$ can drop by significantly more than a factor of 2 in a single iteration, reducing the number of iterations below the worst-case $lceil log _ { 2 } V rceil$ .   \n• A slight reformulation of Borůvka’s algorithm (actually closer to Borůvka’s original presentation) actually runs in $O ( E )$ time for a broad class of interesting graphs, including graphs that can be drawn in the plane without edge crossings. In contrast, the time analysis for the other two algorithms applies to all graphs.   \n• Borůvka’s algorithm allows for significant parallelism; in each iteration, each component of $F$ can be handled in a separate independent thread. This implicit parallelism allows for even faster performance on multicore or distributed systems. In contrast, the other two classical MST algorithms are intrinsically serial.   \n• Several more recent minimum-spanning-tree algorithms are faster even in the worst case than the classical algorithms described here. All of these faster algorithms are generalizations of Borůvka’s algorithm. \nIn short, if you ever need to implement a minimum-spanning-tree algorithm, use Borůvka. On the other hand, if you want to prove things about minimum spanning trees effectively, you really need to know the next two algorithms as well. \n7.4 Jarník’s (“Prim’s”) Algorithm \nThe next oldest minimum spanning tree algorithm was first described by the Czech mathematician Vojtěch Jarník in a 1929 letter to Borůvka; Jarník published his discovery the following year. The algorithm was independently rediscovered by Joseph Kruskal in 1956, (arguably) by Robert Prim in 1957, by Harry Loberman and Arnold Weinberger in 1957, and finally by Edsger Dijkstra in 1958. Prim, Lobermand and Weinberger, and Dijkstra all (eventually) knew of and even cited Kruskal’s paper, but since Kruskal also described two other minimumspanning-tree algorithms in the same paper, this algorithm is usually called “Prim’s algorithm”, or sometimes “the Prim/Dijkstra algorithm”, even though by 1958 Dijkstra already had another algorithm (inappropriately) named after him. \nIn Jarník’s algorithm, the intermediate forest $F$ has only one nontrivial component $T$ ; all the other components are isolated vertices. Initially, $T$ consists of a single arbitrary vertex of the graph. The algorithm repeats the following step until $T$ spans the whole graph:",
        "chapter": "Minimum Spanning Trees",
        "section": "Borůvka’s Algorithm",
        "subsection": "This is the MST Algorithm You Want",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 91
      }
    },
    {
      "node_id": "tb1_node87",
      "content": "nTarjan’s Algorithm \nAn earlier but considerably more subtle linear-time algorithm to compute strong components was published by Bob Tarjan in 1972.6 Intuitively, Tarjan’s algorithm identifies a source component of $G$ , “deletes” it, and then “recursively” finds the remaining strong components; however, the entire computation happens during a single depth-first search. \nFix an arbitrary depth-first traversal of some directed graph $G$ . For each vertex $nu$ , let $boldsymbol { l o w } ( nu )$ denote the smallest starting time among all vertices reachable from $nu$ by a path of tree edges followed by at most one non-tree edge. Trivially, $l o w ( nu ) leq nu . p r e .$ , because $nu$ can reach itself through zero tree edges followed by zero non-tree edges. Tarjan observed that sink components can be characterized in terms of this low function. \nLemma 6.4. A vertex $nu$ is the root of a sink component of $G$ if and only if $l o w ( nu ) = nu .$ .pre and $l o w ( w ) < w$ .pre for every proper descendant w of $nu$ . \nProof: First, let $nu$ be a vertex such that $l o w ( nu ) = nu .$ pre. Then there is no edge $w {  } x$ where $w$ is a descendant of $nu$ and $x . p r e < nu$ .pre. On the other hand, $nu$ ca\u0001nnot reach any vertex y such that $y . p r e > nu . p o s t$ . It follows that $nu$ can reach only its descendants, and therefore any descendant of $nu$ can reach only descendants of $nu$ . In particular, $nu$ cannot reach its parent (if it has one), so $nu$ is the root of its strong component. \nNow suppose in addition that $l o w ( w ) < w . p r e$ for every descendant $w$ of $nu$ . Then each descendant $w$ can reach another vertex $x$ (which must be another descendant of $nu$ ) such that $x . p r e < w . p r e$ . Thus, by induction, every descendant of $nu$ can reach $nu$ . It follows that the descendants of $nu$ comprise the strong component $C$ whose root is $nu$ . Moreover, $C$ must be a sink component, because $nu$ cannot reach any vertex outside of $C$ . \nOn the other hand, suppose $nu$ is the root of a sink component $C$ . Then $nu$ can reach another vertex $w$ if and only if $w in C$ . But $nu$ can reach all of its descendants, and every vertex in $C$ is a descendant of $nu$ , so v’s descendants comprise C. If $l o w ( w ) = w . p r $ for any other node $w in C$ , then $w$ would be another root of $C$ , which is impossible. □ \nComputing $l o w ( nu )$ for every vertex $nu$ via depth-first search is straightforward; see Figure 6.18. \nLemma 6.4 implies that after running FindLow, we can identify the root of every sink component in $O ( V + E )$ time (by a global whatever-first search), and then mark and delete those sink components in $O ( V + E )$ additional time (by calling whatever-first search at each root), and then recurse. Unfortunately, the resulting algorithm might require $V$ iterations, each removing only a single vertex, naively giving us a total running time of $O ( V E )$ . \n\nTo speed up this strategy, Tarjan’s algorithm maintains an auxiliary stack of vertices (separate from the recursion stack). Whenever we start a new vertex $nu$ , we push it onto the stack. Whenever we finish a vertex $nu$ , we compare $nu$ .low with $nu$ .pre. Then the first time we discover that $nu . l o w = nu$ .pre, we know three things: \n• Vertex $nu$ is the root of a sink component $C$ .   \n• All vertices in $C$ appear consecutively at the top of the auxiliary stack.   \n• The deepest vertex in $C$ on the auxiliary stack is $nu$ . \nAt this point, we can identify the vertices in $C$ by popping them off the auxiliary stack one by one, stopping when we pop $nu$ . \nWe could delete the vertices in $C$ and recursively compute the strong components of the remaining graph, but that would be wasteful, because we would repeat verbatim all computation done before visiting $nu$ . Instead, we label each vertex in $C$ , identifying $nu$ as the root of its strong component, and then ignore labeled vertices for the rest of the depth-first search. Formally, this modification changes the definition of $l o w ( nu )$ to the smallest starting time among all vertices in the same strong component as $nu$ that $nu$ can reach by a path of tree edges followed by at most one non-tree edge. But to prove correctness, it’s easier to observe that ignoring labeled vertices leads the algorithm to exactly the same behavior as actually deleting them. \nFinally, Tarjan’s algorithm is shown in Figure 6.19, with the necessary modifications from FindLow (Figure 6.18) indicated in bold red. The running time of the algorithm can be split into two parts. Each vertex is pushed onto $s$ once and popped off $s$ once, so the total time spent maintaining the auxiliary stack (the red stuff) is $O ( V )$ . If we ignore the auxiliary stack maintenance, the rest of the algorithm is just a standard depth-first search. We conclude that the algorithm runs in $O ( V + E )$ time. \n\nExercises \nDepth-first search, topological sort, and strong components \n0. (a) Describe an algorithm to compute the reversal $r e nu ( G )$ of a directed graph in $O ( V + E )$ time. (b) Prove that for every directed graph $G$ , the strong component graph $s c c ( G )$ is acyclic. (c) Prove that $s c c ( r e nu ( G ) ) = r e nu ( s c c ( G ) )$ for every directed graph $G$ . (d) Fix an arbitrary directed graph $G$ . For any vertex $nu$ of $G$ , let $S ( nu )$ denote the strong component of $G$ that contains $nu$ . For all vertices $u$ and $nu$ of $G$ , prove that $u$ can reach $nu$ in $G$ if and only if $boldsymbol { S } ( boldsymbol { u } )$ can reach $S ( nu )$ in $s c c ( G )$ . \n1. A directed graph $G$ is semi-connected if, for every pair of vertices $u$ and $nu$ , either $u$ is reachable from $nu$ or $nu$ is reachable from $u$ (or both). \n(a) Give an example of a directed acyclic graph with a unique source that is not semi-connected.   \n(b) Describe and analyze an algorithm to determine whether a given directed acyclic graph is semi-connected.",
      "metadata": {
        "content": "nTarjan’s Algorithm \nAn earlier but considerably more subtle linear-time algorithm to compute strong components was published by Bob Tarjan in 1972.6 Intuitively, Tarjan’s algorithm identifies a source component of $G$ , “deletes” it, and then “recursively” finds the remaining strong components; however, the entire computation happens during a single depth-first search. \nFix an arbitrary depth-first traversal of some directed graph $G$ . For each vertex $nu$ , let $boldsymbol { l o w } ( nu )$ denote the smallest starting time among all vertices reachable from $nu$ by a path of tree edges followed by at most one non-tree edge. Trivially, $l o w ( nu ) leq nu . p r e .$ , because $nu$ can reach itself through zero tree edges followed by zero non-tree edges. Tarjan observed that sink components can be characterized in terms of this low function. \nLemma 6.4. A vertex $nu$ is the root of a sink component of $G$ if and only if $l o w ( nu ) = nu .$ .pre and $l o w ( w ) < w$ .pre for every proper descendant w of $nu$ . \nProof: First, let $nu$ be a vertex such that $l o w ( nu ) = nu .$ pre. Then there is no edge $w {  } x$ where $w$ is a descendant of $nu$ and $x . p r e < nu$ .pre. On the other hand, $nu$ ca\u0001nnot reach any vertex y such that $y . p r e > nu . p o s t$ . It follows that $nu$ can reach only its descendants, and therefore any descendant of $nu$ can reach only descendants of $nu$ . In particular, $nu$ cannot reach its parent (if it has one), so $nu$ is the root of its strong component. \nNow suppose in addition that $l o w ( w ) < w . p r e$ for every descendant $w$ of $nu$ . Then each descendant $w$ can reach another vertex $x$ (which must be another descendant of $nu$ ) such that $x . p r e < w . p r e$ . Thus, by induction, every descendant of $nu$ can reach $nu$ . It follows that the descendants of $nu$ comprise the strong component $C$ whose root is $nu$ . Moreover, $C$ must be a sink component, because $nu$ cannot reach any vertex outside of $C$ . \nOn the other hand, suppose $nu$ is the root of a sink component $C$ . Then $nu$ can reach another vertex $w$ if and only if $w in C$ . But $nu$ can reach all of its descendants, and every vertex in $C$ is a descendant of $nu$ , so v’s descendants comprise C. If $l o w ( w ) = w . p r $ for any other node $w in C$ , then $w$ would be another root of $C$ , which is impossible. □ \nComputing $l o w ( nu )$ for every vertex $nu$ via depth-first search is straightforward; see Figure 6.18. \nLemma 6.4 implies that after running FindLow, we can identify the root of every sink component in $O ( V + E )$ time (by a global whatever-first search), and then mark and delete those sink components in $O ( V + E )$ additional time (by calling whatever-first search at each root), and then recurse. Unfortunately, the resulting algorithm might require $V$ iterations, each removing only a single vertex, naively giving us a total running time of $O ( V E )$ . \n\nTo speed up this strategy, Tarjan’s algorithm maintains an auxiliary stack of vertices (separate from the recursion stack). Whenever we start a new vertex $nu$ , we push it onto the stack. Whenever we finish a vertex $nu$ , we compare $nu$ .low with $nu$ .pre. Then the first time we discover that $nu . l o w = nu$ .pre, we know three things: \n• Vertex $nu$ is the root of a sink component $C$ .   \n• All vertices in $C$ appear consecutively at the top of the auxiliary stack.   \n• The deepest vertex in $C$ on the auxiliary stack is $nu$ . \nAt this point, we can identify the vertices in $C$ by popping them off the auxiliary stack one by one, stopping when we pop $nu$ . \nWe could delete the vertices in $C$ and recursively compute the strong components of the remaining graph, but that would be wasteful, because we would repeat verbatim all computation done before visiting $nu$ . Instead, we label each vertex in $C$ , identifying $nu$ as the root of its strong component, and then ignore labeled vertices for the rest of the depth-first search. Formally, this modification changes the definition of $l o w ( nu )$ to the smallest starting time among all vertices in the same strong component as $nu$ that $nu$ can reach by a path of tree edges followed by at most one non-tree edge. But to prove correctness, it’s easier to observe that ignoring labeled vertices leads the algorithm to exactly the same behavior as actually deleting them. \nFinally, Tarjan’s algorithm is shown in Figure 6.19, with the necessary modifications from FindLow (Figure 6.18) indicated in bold red. The running time of the algorithm can be split into two parts. Each vertex is pushed onto $s$ once and popped off $s$ once, so the total time spent maintaining the auxiliary stack (the red stuff) is $O ( V )$ . If we ignore the auxiliary stack maintenance, the rest of the algorithm is just a standard depth-first search. We conclude that the algorithm runs in $O ( V + E )$ time. \n\nExercises \nDepth-first search, topological sort, and strong components \n0. (a) Describe an algorithm to compute the reversal $r e nu ( G )$ of a directed graph in $O ( V + E )$ time. (b) Prove that for every directed graph $G$ , the strong component graph $s c c ( G )$ is acyclic. (c) Prove that $s c c ( r e nu ( G ) ) = r e nu ( s c c ( G ) )$ for every directed graph $G$ . (d) Fix an arbitrary directed graph $G$ . For any vertex $nu$ of $G$ , let $S ( nu )$ denote the strong component of $G$ that contains $nu$ . For all vertices $u$ and $nu$ of $G$ , prove that $u$ can reach $nu$ in $G$ if and only if $boldsymbol { S } ( boldsymbol { u } )$ can reach $S ( nu )$ in $s c c ( G )$ . \n1. A directed graph $G$ is semi-connected if, for every pair of vertices $u$ and $nu$ , either $u$ is reachable from $nu$ or $nu$ is reachable from $u$ (or both). \n(a) Give an example of a directed acyclic graph with a unique source that is not semi-connected.   \n(b) Describe and analyze an algorithm to determine whether a given directed acyclic graph is semi-connected.",
        "chapter": "Depth-First Search",
        "section": "Strong Components in Linear Time",
        "subsection": "♥Tarjan’s Algorithm",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 87
      }
    },
    {
      "node_id": "tb1_node92",
      "content": "Jarník: Repeatedly add T’s safe edge to T. \nTo implement Jarník’s algorithm, we keep all the edges adjacent to $T$ in a priority queue. When we pull the minimum-weight edge out of the priority queue, we first check whether both of its endpoints are in $T$ . If not, we add the edge to $T$ and then add the new neighboring edges to the priority queue. In other words, Jarník’s algorithm is a variant of “best-first search”, as described at the end of Chapter 5! If we implement the underlying priority queue using a standard binary heap, Jarník’s algorithm runs in $O ( E log E ) = O ( E log V )$ time. \nnImproving Jarník’s Algorithm \nWe can improve Jarník’s algorithm using a more complex priority queue data structure called a Fibonacci heap, first described by Michael Fredman and Robert Tarjan in 1984. Just like binary heaps, Fibonacci heaps support the standard priority queue operations Insert, ExtractMin, and DecreaseKey. However, unlike standard binary heaps, which require $O ( log n )$ time for every operation, Fibonacci heaps support Insert and DecreaseKey in constant amortized time. The amortized cost of ExtractMin is still $O ( log n )$ .4 \nTo apply this faster data structure, we keep the vertices of $G$ in the priority queue instead of edges, where the priority of each vertex $nu$ is either the minimumweight edge between $nu$ and the evolving tree $T$ , or $infty$ if there is no such edge. We can Insert all the vertices into the priority queue at the beginning of the algorithm; then, whenever we add a new edge to $T$ , we may need to decrease the priorities of some neighboring vertices. \nTo make the description easier, we break the algorithm into two parts. JarníkInit initializes the priority queue; JarníkLoop is the main algorithm. The input consists of the vertices and edges of the graph, along with the start vertex s. For each vertex $nu$ , we maintain both its priority priority $( nu )$ and the incident edge edge $( nu )$ such that $w ( e d g e ( nu ) ) = p r i o r i t y ( nu )$ . \nThe operations Insert and ExtractMin are each called $O ( V )$ times once for each vertex except $s$ , and DecreaseKey is called $O ( E )$ times, at most twice for each edge. Thus, if we use a Fibonacci heap, the improved algorithm runs in $O ( E + V log V )$ time, which is faster than Borůvka’s algorithm unless $E = O ( V )$ . \nIn practice, however, this improvement is rarely faster than the naive implementation using a binary heap, unless the graph is extremely large and dense. The Fibonacci heap algorithms are quite complex, and the hidden constants in both the running time and space are significant—not outrageous, but certainly bigger than the hidden constant 1 in the $O ( log n )$ time bound for binary heap operations. \n7.5 Kruskal’s Algorithm \nThe last minimum spanning tree algorithm we’ll consider was first described by Joseph Kruskal in 1956, in the same paper where he rediscovered Jarnik’s algorithm. Kruskal was motivated by “a typewritten translation (of obscure origin)” of Borůvka’s original paper that had been “floating around” the Princeton math department. Kruskal found Borůvka’s algorithm “unnecessarily elaborate”.5 The same algorithm was rediscovered in 1957 by Harry Loberman and Arnold Weinberger, but somehow avoided being renamed after them.",
      "metadata": {
        "content": "Jarník: Repeatedly add T’s safe edge to T. \nTo implement Jarník’s algorithm, we keep all the edges adjacent to $T$ in a priority queue. When we pull the minimum-weight edge out of the priority queue, we first check whether both of its endpoints are in $T$ . If not, we add the edge to $T$ and then add the new neighboring edges to the priority queue. In other words, Jarník’s algorithm is a variant of “best-first search”, as described at the end of Chapter 5! If we implement the underlying priority queue using a standard binary heap, Jarník’s algorithm runs in $O ( E log E ) = O ( E log V )$ time. \nnImproving Jarník’s Algorithm \nWe can improve Jarník’s algorithm using a more complex priority queue data structure called a Fibonacci heap, first described by Michael Fredman and Robert Tarjan in 1984. Just like binary heaps, Fibonacci heaps support the standard priority queue operations Insert, ExtractMin, and DecreaseKey. However, unlike standard binary heaps, which require $O ( log n )$ time for every operation, Fibonacci heaps support Insert and DecreaseKey in constant amortized time. The amortized cost of ExtractMin is still $O ( log n )$ .4 \nTo apply this faster data structure, we keep the vertices of $G$ in the priority queue instead of edges, where the priority of each vertex $nu$ is either the minimumweight edge between $nu$ and the evolving tree $T$ , or $infty$ if there is no such edge. We can Insert all the vertices into the priority queue at the beginning of the algorithm; then, whenever we add a new edge to $T$ , we may need to decrease the priorities of some neighboring vertices. \nTo make the description easier, we break the algorithm into two parts. JarníkInit initializes the priority queue; JarníkLoop is the main algorithm. The input consists of the vertices and edges of the graph, along with the start vertex s. For each vertex $nu$ , we maintain both its priority priority $( nu )$ and the incident edge edge $( nu )$ such that $w ( e d g e ( nu ) ) = p r i o r i t y ( nu )$ . \nThe operations Insert and ExtractMin are each called $O ( V )$ times once for each vertex except $s$ , and DecreaseKey is called $O ( E )$ times, at most twice for each edge. Thus, if we use a Fibonacci heap, the improved algorithm runs in $O ( E + V log V )$ time, which is faster than Borůvka’s algorithm unless $E = O ( V )$ . \nIn practice, however, this improvement is rarely faster than the naive implementation using a binary heap, unless the graph is extremely large and dense. The Fibonacci heap algorithms are quite complex, and the hidden constants in both the running time and space are significant—not outrageous, but certainly bigger than the hidden constant 1 in the $O ( log n )$ time bound for binary heap operations. \n7.5 Kruskal’s Algorithm \nThe last minimum spanning tree algorithm we’ll consider was first described by Joseph Kruskal in 1956, in the same paper where he rediscovered Jarnik’s algorithm. Kruskal was motivated by “a typewritten translation (of obscure origin)” of Borůvka’s original paper that had been “floating around” the Princeton math department. Kruskal found Borůvka’s algorithm “unnecessarily elaborate”.5 The same algorithm was rediscovered in 1957 by Harry Loberman and Arnold Weinberger, but somehow avoided being renamed after them.",
        "chapter": "Minimum Spanning Trees",
        "section": "Jarník’s (“Prim’s”) Algorithm",
        "subsection": "♥Improving Jarník’s Algorithm",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 92
      }
    },
    {
      "node_id": "tb1_node90",
      "content": "In light of Lemma 7.1 and this tie-breaking rule, we will safely assume for the rest of this chapter that edge weights are always distinct, and therefore minimum spanning trees are always unique. In particular, we can freely discuss the minimum spanning tree with no confusion. \n7.2 The Only Minimum Spanning Tree Algorithm \nThere are many algorithms to compute minimum spanning trees, but almost all of them are instances of the following generic strategy. The situation is similar to graph traversal, where several different algorithms are all variants of the generic traversal algorithm whatever-first search. \nThe generic minimum spanning tree algorithm maintains an acyclic subgraph $F$ of the input graph $G$ , which we will call the intermediate spanning forest. At all times, $F$ satisfies the following invariant: \n$F$ is a subgraph of the minimum spanning tree of G. \nInitially, $F$ consists of $V$ one-vertex trees. The generic algorithm connects trees in $F$ by adding certain edges between them. When the algorithm halts, $F$ consists of a single spanning tree; our invariant implies that this must be the minimum spanning tree of $G$ . Obviously, we have to be careful about which edges we add to the evolving forest, because not every edge is in the minimum spanning tree. \nAt any stage of its evolution, the intermediate spanning forest $F$ induces two special types of edges in the rest of the graph. \n• An edge is useless if it is not an edge of $F$ , but both its endpoints are in the same component of $F$ .   \n• An edge is safe if it is the minimum-weight edge with exactly one endpoint in some component of $F$ . \nThe same edge could be safe for two different components of $F$ . Some edges of $G setminus F$ are neither safe nor useless; we call these edges undecided. \nAll minimum spanning tree algorithms are based on two simple observations. The first observation was proved by Robert Prim in 1957 (although it is implicit in several earlier algorithms), and the second is immediate. \nLemma 7.2 (Prim). The minimum spanning tree of $G$ contains every safe edge. \nProof: In fact we prove the following stronger statement: For any subset $s$ of the vertices of $G$ , the minimum spanning tree of $G$ contains the minimum-weight edge with exactly one endpoint in S. Like the previous lemma, we prove this claim using a greedy exchange argument. \nLet $s$ be an arbitrary subset of vertices of $G$ , and let $e$ be the lightest edge with exactly one endpoint in $s$ . (Our assumption that all edge weights are distinct implies that e is unique.) Let $T$ be an arbitrary spanning tree that does not contain $e$ ; we need to prove that $T$ is not the minimum spanning tree of $G$ . \nBecause $T$ is connected, it contains a path from one endpoint of $e$ to the other. Because this path starts at a vertex of $s$ and ends at a vertex not in $s$ , it must contain at least one edge with exactly one endpoint in $s$ ; let $e ^ { prime }$ be any such edge. Because $T$ is acyclic, removing $e ^ { prime }$ from $T$ yields a spanning forest with exactly two components, one containing each endpoint of $e$ . Thus, adding $e$ to this forest gives us a new spanning tree $T ^ { prime } = T - e ^ { prime } + e$ . The definition of $e$ implies $w ( e ^ { prime } ) > w ( e )$ , which implies that $T ^ { prime }$ has smaller total weight than $T$ . Thus, $T$ is not the minimum spanning tree of $G$ , which completes the proof. □ \nLemma 7.3. The minimum spanning tree contains no useless edge. \nProof: Adding any useless edge to $F$ would introduce a cycle. \nOur generic minimum spanning tree algorithm repeatedly adds safe edges to the evolving forest $F$ . If $F$ is not yet connected, there must be at least one safe edge, because the input graph $G$ is connected. Thus, no matter which safe edges we add in each iteration, our generic algorithm eventually connects $F$ . By induction, Lemma 7.2 implies that the resulting tree is in fact the minimum spanning tree. Whenever we add new edges to $F$ , some undecided edges may become safe, and other undecided edges may become useless. (Once an edge becomes useless, it stays useless forever.) To fully specify a particular algorithm, we must describe which safe edge(s) to add in each iteration, and how to find those edges. \n\n7.3 Borůvka’s Algorithm \nThe oldest and arguably simplest minimum spanning tree algorithm was discovered by the Czech mathematician Otakar Borůvka in 1926, about a year after Jindřich Saxel asked him how to construct an electrical network connecting several cities using the least amount of wire.2 The algorithm was rediscovered by Gustav Choquet in 1938, rediscovered again by a team of Polish mathematicians led by Józef Łukaszewicz in 1951, and rediscovered again by George Sollin in 1961. Although Sollin never published his rediscovery, it was carefully described and credited in one of the first textbooks on graph algorithms; as a result, this algorithm is sometimes called “Sollin’s algorithm”. \nThe Borůvka / Choquet / Florek-Łukaziewicz-Perkal-Steinhaus-Zubrzycki / Prim / Sollin / Brosh3 algorithm can be summarized in one line: \nHere is Borůvka’s algorithm in more detail. The algorithm calls the CountAndLabel algorithm from Chapter 5 (on page 204) to count the components of $F$ and label each vertex $nu$ with an integer comp $( nu )$ indicating its component.",
      "metadata": {
        "content": "In light of Lemma 7.1 and this tie-breaking rule, we will safely assume for the rest of this chapter that edge weights are always distinct, and therefore minimum spanning trees are always unique. In particular, we can freely discuss the minimum spanning tree with no confusion. \n7.2 The Only Minimum Spanning Tree Algorithm \nThere are many algorithms to compute minimum spanning trees, but almost all of them are instances of the following generic strategy. The situation is similar to graph traversal, where several different algorithms are all variants of the generic traversal algorithm whatever-first search. \nThe generic minimum spanning tree algorithm maintains an acyclic subgraph $F$ of the input graph $G$ , which we will call the intermediate spanning forest. At all times, $F$ satisfies the following invariant: \n$F$ is a subgraph of the minimum spanning tree of G. \nInitially, $F$ consists of $V$ one-vertex trees. The generic algorithm connects trees in $F$ by adding certain edges between them. When the algorithm halts, $F$ consists of a single spanning tree; our invariant implies that this must be the minimum spanning tree of $G$ . Obviously, we have to be careful about which edges we add to the evolving forest, because not every edge is in the minimum spanning tree. \nAt any stage of its evolution, the intermediate spanning forest $F$ induces two special types of edges in the rest of the graph. \n• An edge is useless if it is not an edge of $F$ , but both its endpoints are in the same component of $F$ .   \n• An edge is safe if it is the minimum-weight edge with exactly one endpoint in some component of $F$ . \nThe same edge could be safe for two different components of $F$ . Some edges of $G setminus F$ are neither safe nor useless; we call these edges undecided. \nAll minimum spanning tree algorithms are based on two simple observations. The first observation was proved by Robert Prim in 1957 (although it is implicit in several earlier algorithms), and the second is immediate. \nLemma 7.2 (Prim). The minimum spanning tree of $G$ contains every safe edge. \nProof: In fact we prove the following stronger statement: For any subset $s$ of the vertices of $G$ , the minimum spanning tree of $G$ contains the minimum-weight edge with exactly one endpoint in S. Like the previous lemma, we prove this claim using a greedy exchange argument. \nLet $s$ be an arbitrary subset of vertices of $G$ , and let $e$ be the lightest edge with exactly one endpoint in $s$ . (Our assumption that all edge weights are distinct implies that e is unique.) Let $T$ be an arbitrary spanning tree that does not contain $e$ ; we need to prove that $T$ is not the minimum spanning tree of $G$ . \nBecause $T$ is connected, it contains a path from one endpoint of $e$ to the other. Because this path starts at a vertex of $s$ and ends at a vertex not in $s$ , it must contain at least one edge with exactly one endpoint in $s$ ; let $e ^ { prime }$ be any such edge. Because $T$ is acyclic, removing $e ^ { prime }$ from $T$ yields a spanning forest with exactly two components, one containing each endpoint of $e$ . Thus, adding $e$ to this forest gives us a new spanning tree $T ^ { prime } = T - e ^ { prime } + e$ . The definition of $e$ implies $w ( e ^ { prime } ) > w ( e )$ , which implies that $T ^ { prime }$ has smaller total weight than $T$ . Thus, $T$ is not the minimum spanning tree of $G$ , which completes the proof. □ \nLemma 7.3. The minimum spanning tree contains no useless edge. \nProof: Adding any useless edge to $F$ would introduce a cycle. \nOur generic minimum spanning tree algorithm repeatedly adds safe edges to the evolving forest $F$ . If $F$ is not yet connected, there must be at least one safe edge, because the input graph $G$ is connected. Thus, no matter which safe edges we add in each iteration, our generic algorithm eventually connects $F$ . By induction, Lemma 7.2 implies that the resulting tree is in fact the minimum spanning tree. Whenever we add new edges to $F$ , some undecided edges may become safe, and other undecided edges may become useless. (Once an edge becomes useless, it stays useless forever.) To fully specify a particular algorithm, we must describe which safe edge(s) to add in each iteration, and how to find those edges. \n\n7.3 Borůvka’s Algorithm \nThe oldest and arguably simplest minimum spanning tree algorithm was discovered by the Czech mathematician Otakar Borůvka in 1926, about a year after Jindřich Saxel asked him how to construct an electrical network connecting several cities using the least amount of wire.2 The algorithm was rediscovered by Gustav Choquet in 1938, rediscovered again by a team of Polish mathematicians led by Józef Łukaszewicz in 1951, and rediscovered again by George Sollin in 1961. Although Sollin never published his rediscovery, it was carefully described and credited in one of the first textbooks on graph algorithms; as a result, this algorithm is sometimes called “Sollin’s algorithm”. \nThe Borůvka / Choquet / Florek-Łukaziewicz-Perkal-Steinhaus-Zubrzycki / Prim / Sollin / Brosh3 algorithm can be summarized in one line: \nHere is Borůvka’s algorithm in more detail. The algorithm calls the CountAndLabel algorithm from Chapter 5 (on page 204) to count the components of $F$ and label each vertex $nu$ with an integer comp $( nu )$ indicating its component.",
        "chapter": "Minimum Spanning Trees",
        "section": "The Only Minimum Spanning Tree Algorithm",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 90
      }
    },
    {
      "node_id": "tb1_node88",
      "content": "Exercises \nDepth-first search, topological sort, and strong components \n0. (a) Describe an algorithm to compute the reversal $r e nu ( G )$ of a directed graph in $O ( V + E )$ time. (b) Prove that for every directed graph $G$ , the strong component graph $s c c ( G )$ is acyclic. (c) Prove that $s c c ( r e nu ( G ) ) = r e nu ( s c c ( G ) )$ for every directed graph $G$ . (d) Fix an arbitrary directed graph $G$ . For any vertex $nu$ of $G$ , let $S ( nu )$ denote the strong component of $G$ that contains $nu$ . For all vertices $u$ and $nu$ of $G$ , prove that $u$ can reach $nu$ in $G$ if and only if $boldsymbol { S } ( boldsymbol { u } )$ can reach $S ( nu )$ in $s c c ( G )$ . \n1. A directed graph $G$ is semi-connected if, for every pair of vertices $u$ and $nu$ , either $u$ is reachable from $nu$ or $nu$ is reachable from $u$ (or both). \n(a) Give an example of a directed acyclic graph with a unique source that is not semi-connected.   \n(b) Describe and analyze an algorithm to determine whether a given directed acyclic graph is semi-connected. \n(c) Describe and analyze an algorithm to determine whether an arbitrary directed graph is semi-connected. \n2. The police department in the city of Sham-Poobanana has made every street in the city one-way. Despite widespread complaints from confused motorists, the mayor claims that it is possible to legally drive from any intersection in Sham-Poobanana to any other intersection. \n(a) The city needs to either verify or refute the mayor’s claim. Formalize this problem in terms of graphs, and then describe and analyze an algorithm to solve it.   \n(b) After running your algorithm from part (a), the mayor reluctantly admits that she was lying misinformed. Call an intersection $x$ good if, for any intersection $y$ that one can legally reach from $x$ , it is possible to legally drive from $y$ back to $x$ . Now the mayor claims that over $9 5 %$ of the intersections in Sham-Poobanana are good. Describe and analyze an efficient algorithm to verify or refute her claim. \nFor full credit, both algorithms should run in linear time. \n3. Suppose we are given a directed acyclic graph $G$ with a unique source s and a unique sink $t$ . A vertex $nu notin { s , t }$ is called an $( s , t )$ -cut vertex if every path from $s$ to $t$ passes through $nu$ , or equivalently, if deleting $nu$ makes $t$ unreachable from s. Describe and analyze an algorithm to find every $( s , t )$ -cut vertex in $G$ . \n4. A vertex $nu$ in a connected undirected graph $G$ is called a cut vertex if the subgraph $G - nu$ (obtained by removing $nu$ from $G$ ) is disconnected. \n(a) Describe a linear-time algorithm that determines, given an undirected graph $G$ and a vertex $nu$ , whether $nu$ is a cut vertex in $G$ . What is the running time to find all cut vertices by trying your algorithm for each vertex?   \n(b) Let $T$ be a depth-first spanning tree of an undirected graph $G$ . i. Prove that the root of $T$ is a cut vertex of $G$ if and only if it has more than one child in $T$ . ii. Prove that a non-root vertex $nu$ is a cut vertex of $G$ if and only if at least one descendant (in $T$ ) of each child of $nu$ (in $T$ ) is a neighbor (in $G$ ) of some proper ancestor of $nu$ (in $T$ ). [Hint: These claims no longer hold if $T$ not a depth-first spanning tree and/or $G$ is a directed graph.]   \n(c) Describe an algorithm that identifies every cut vertex in a given undirected graph in $O ( V + E )$ time. \n5. An edge e in a connected undirected graph $G$ is called a bridge (or a cut edge) if the subgraph $G - e$ (obtained by removing e from $G$ ) is disconnected. \n(a) Given $G$ and edge e describe a linear-time algorithm that determines whether $e$ is a bridge or not. What is the running time to find all bridges by trying your algorithm for each edge?   \n(b) Let $T$ be an arbitrary spanning tree of $G$ . Prove that every bridges of $G$ is also an edge in $T$ . This claim implies that $G$ has at most $V - 1$ bridges. How does this information improve your algorithm from part (a) to find all bridges?   \n(c) Now suppose we root $T$ at an arbitrary vertex $r$ . For any vertex $nu$ , let $T _ { nu }$ denote the subtree of $T$ rooted at $nu$ ; for example, $T _ { r } = T$ . Let $u nu$ be an arbitrary edge of $T$ , where $u$ is the parent of $nu$ . Prove that $u nu$ is a bridge of $G$ if and only if $u nu$ is the only edge in $G$ with exactly one endpoint in $T _ { nu }$ .   \n(d) Describe a linear-time algorithm to identify every bridge in $G$ . [Hint: Let $T$ be a depth-first spanning tree of G.] \n6. The transitive closure $G ^ { T }$ of a directed graph $G$ is a directed graph with the same vertices as $G$ , that contains any edge $u {  } nu$ if and only if there is a directed path from $u$ to $nu$ in $G$ . A transitive redu\u0001ction of $G$ is a graph with the smallest possible number of edges whose transitive closure is $G ^ { T }$ . The same graph may have several transitive reductions. \n(a) Describe an efficient algorithm to compute the transitive closure of a given directed graph. \n(b) Prove that a directed graph $G$ has a unique transitive reduction if and only if $G$ is acyclic.   \n(c) Describe an efficient algorithm to compute a transitive reduction of a given directed graph. \n7. One of the oldest algorithms for exploring arbitrary connected graphs was proposed by Gaston Tarry in 1895, as a systematic procedure for solving mazes.7 The input to Tarry’s algorithm is an undirected graph $G$ ; however, for ease of presentation, we formally split each undirected edge $u nu$ into two directed edges $u {  } nu$ and $nu {  } u$ . (In an actual implementation, this split is trivial; the algorit\u0001hm simply\u0001uses the given adjacency list for $G$ as though $G$ were directed.) \nWe informally say that Tarry’s algorithm “visits” vertex $nu$ every time it marks $nu$ , and it “traverses” edge $nu {  } w$ when it colors that edge red and recursively calls RecTarry $( w )$ . Unlike\u0001our earlier graph traversal algorithm, Tarry’s algorithm can mark same vertex multiple times. \n(a) Describe how to implement Tarry’s algorithm so that it runs in $O ( V + E )$ time.   \n(b) Prove that no directed edge in $G$ is traversed more than once.   \n(c) When the algorithm visits a vertex $nu$ for the $k$ th time, exactly how many edges into $nu$ are red, and exactly how many edges out of $nu$ are red? [Hint: Consider the starting vertex s separately from the other vertices.]   \n(d) Prove each vertex $nu$ is visited at most $deg ( nu )$ times, except the starting vertex s, which is visited at most $deg ( s ) + 1$ times. This claim immediately implies that Tarry $( G )$ terminates. \n(e) Prove that the last vertex visited by Tarry $( G )$ is the starting vertex s. \n(f) For every vertex $nu$ that Tarry $( G )$ visits, prove that all edges into $nu$ and out of $nu$ are red when Tarry $( G )$ halts. [Hint: Consider the vertices in the order that they are marked for the first time, starting with $s$ , and prove the claim by induction.]   \n(g) Prove that Tarry $( G )$ visits every vertex of $G$ . This claim and the previous claim imply that Tarry $( G )$ traverses every edge of $G$ exactly once. \n8. Consider the following variant of Tarry’s graph-traversal algorithm; this variant traverses green edges without recoloring them red and assigns two numerical labels to every vertex: \nProve or disprove the following claim: When Tarry2 $( G )$ halts, the green edges define a spanning tree and the labels $nu$ .pre and $nu$ .post define a preorder and postorder labeling that are all consistent with a single depth-first search of $G$ . In other words, prove or disprove that Tarry2 produces the same output as depth-first search, even though it visits the edges in a completely different order. \n9. You have a collection of $n$ lock-boxes and $m$ gold keys. Each key unlocks at most one box. However, each box might be unlocked by one key, by multiple keys, or by no keys at all. There are only two ways to open each box once it is locked: Unlock it properly (which requires having one matching key in your hand), or smash it to bits with a hammer. \nYour baby brother, who loves playing with shiny objects, has somehow managed to lock all your keys inside the boxes! Luckily, your home security system recorded everything, so you know exactly which keys (if any) are inside each box. You need to get all the keys back out of the boxes, because they are made of gold. Clearly you have to smash at least one box. \n(a) Your baby brother has found the hammer and is eagerly eyeing one of the boxes. Describe and analyze an algorithm to determine if it is \npossible to retrieve all the keys without smashing any box except the one your brother has chosen. (b) Describe and analyze an algorithm to compute the minimum number of boxes that must be smashed to retrieve all the keys. \n10. Suppose you are teaching an algorithms course. In your second midterm, you give your students a drawing of a graph and ask then to indicate a breadth-first search tree and a depth-first search tree rooted at a particular vertex. Unfortunately, once you start grading the exam, you realize that the graph you gave the students has several such spanning trees—far too many to list. Instead, you need a way to tell whether each student’s submission is correct! \nIn each of the following problems, suppose you are given a connected graph $G$ , a start vertex s, and a spanning tree $T$ of $G$ . \n(a) Suppose $G$ is undirected. Describe and analyze an algorithm to decide whether $T$ is a depth-first spanning tree rooted at $s$ .   \n(b) Suppose $G$ is undirected. Describe and analyze an algorithm to decide whether $T$ is a breadth-first spanning tree rooted at s. [Hint: It’s not enough for $T$ to be an unweighted shortest-path tree. Yes, this is the right chapter for this problem!]   \n(c) Suppose $G$ is directed. Describe and analyze an algorithm to decide whether $T$ is a breadth-first spanning tree rooted at s. [Hint: Solve part $( b )$ first.]   \n(d) Suppose $G$ is directed. Describe and analyze an algorithm to decide whether $T$ is a depth-first spanning tree rooted at $s$ . \n11. Several modern programming languages, including JavaScript, Python, Perl, and Ruby, include a feature called parallel assignment, which allows multiple assignment operations to be encoded in a single line of code. For example, the Python code $mathsf { x } , mathsf { y } = mathsf { 0 } , mathsf { 1 }$ simultaneously sets $x$ to 0 and y to 1. The values of the right-hand side of the assignment are all determined by the old values of the variables. Thus, the Python code a, ${ mathsf b } = { mathsf b }$ ,a swaps the values of a and b, and the following Python code computes the nth Fibonacci number: \ndef fib(n): prev, curr $= ~ 1$ , 0 while $n > 0$ : prev, curr, ${ mathfrak { n } } =$ curr, prev+curr, n-1 return curr \nSuppose the interpreter you are writing needs to convert every parallel assignment into an equivalent sequence of individual assignments. For example, the parallel assignment a, ${ bf b } = boldsymbol { 0 } , 1$ can be serialized in either order— either $a = 0$ ; $b = 1$ or $a = 0$ ; $b = 1$ —but the parallel assignment $mathsf { x } , mathsf { y } = mathsf { x } + 1$ , $x + y$ can only be serialized as $y = x + y$ ; $x = x + 1$ . Serialization may require one or more additional temporary variables; for example, serializing a, $b = 6$ ,a requires one temporary variable, and serializing $x , y = x { + } y$ , $x - y$ requires two temporary variables. \n\n(a) Describe an algorithm to determine whether a given parallel assignment can be serialized without additional temporary variables.   \n(b) Describe an algorithm to determine whether a given parallel assignment can be serialized with exactly one additional temporary variable. \nAssume that the given parallel assignment involves only simple integer variables (no indirection via pointers or arrays); no variable appears on the left side more than once; and expressions on the right side have no side effects. Don’t worry about the details of parsing the assignment statement; just assume (but describe!) an appropriate graph representation. \nDynamic Programming \n12. Suppose we are given a directed acyclic graph $G$ whose nodes represent jobs and whose edges represent precedence constraints; that is. each edge $u {  } nu$ indicates the job $u$ must be completed before job $nu$ begins. Each node $nu$ al\u0001so has a weight $T ( nu )$ indicating the time required to execute job $nu$ . \n(a) Describe an algorithm to determine the shortest interval of time in which all jobs in $G$ can be executed.   \n(b) Suppose the first job starts at time 0. Describe an algorithm to determine, for each vertex $nu$ , the earliest time when job $nu$ can begin.   \n(c) Now describe an algorithm to determine, for each vertex $nu$ , the latest time when job $nu$ can begin without violating the precedence constraints or increasing the overall completion time (computed in part (a)), assuming that every job except $nu$ starts at its earliest start time (computed in part (b)). \n13. Let $G$ be a directed acyclic graph with a unique source s and a unique sink $t$ . \n(a) A Hamiltonian path in $G$ is a directed path in $G$ that contains every vertex in $G$ . Describe an algorithm to determine whether $G$ has a Hamiltonian path.   \n(b) Suppose the vertices of $G$ have weights. Describe an efficient algorithm to find the path from $s$ to $t$ with maximum total weight.   \n(c) Suppose we are also given an integer $ell$ . Describe an efficient algorithm to find the maximum-weight path from $s$ to $t$ that contains at most $ell$ edges. (Assume there is at least one such path.)   \n(d) Suppose some of the vertices of $G$ are marked as important, and we are also given an integer $k$ . Describe an efficient algorithm to find the maximum-weight path from s to $t$ that visits at least $k$ important vertices. (Assume there is at least one such path.)   \n(e) Describe an algorithm to compute the number of paths from s to $t$ in $G$ . (Assume that you can add arbitrarily large integers in $O ( 1 )$ time.) \n\n14. Let $G$ be a directed acyclic graph whose vertices have labels from some fixed alphabet, and let $A [ 1 ldots ell ]$ be a string over the same alphabet. Any directed path in $G$ has a label, which is a string obtained by concatenating the labels of its vertices. \n(a) Describe an algorithm that either finds a path in $G$ whose label is $A$ or correctly reports that there is no such path.   \n(b) Describe an algorithm to find the number of paths in $G$ whose label is $A$ . (Assume that you can add arbitrarily large integers in $O ( 1 )$ time.)   \n(c) Describe an algorithm to find the longest path in $G$ whose label is a subsequence of A.   \n(d) Describe an algorithm to find the shortest path in $G$ whose label is a supersequence of A.   \n(e) Describe an algorithm to find a path in $G$ whose label has minimum edit distance from A. \n15. A polygonal path is a sequence of line segments joined end-to-end; the endpoints of these line segments are called the vertices of the path. The length of a polygonal path is the sum of the lengths of its segments. A polygonal path with vertices $( x _ { 1 } , y _ { 1 } ) , ( x _ { 2 } , y _ { 2 } ) , ldots , ( x _ { k } , y _ { k } )$ is monotonically increasing if $x _ { i } < x _ { i + 1 }$ and $y _ { i } < y _ { i + 1 }$ for every index $i$ —informally, each vertex of the path is above and to the right of its predecessor. \nSuppose you are given a set $s$ of $n$ points in the plane, represented as two arrays $X [ 1 ldots n ]$ and $Y [ 1 ldots n ]$ . Describe and analyze an algorithm to compute the length of the longest monotonically increasing path with vertices in $s$ . Assume you have a subroutine $mathrm { L E N G T H } ( x , y , x ^ { prime } , y ^ { prime } )$ that returns the length of the segment from $( x , y )$ to $( x ^ { prime } , y ^ { prime } )$ . \n16. For any two nodes $u$ and $w$ in a directed acyclic graph $G$ , the interval $G [ u , w ]$ is the union of all directed paths in $G$ from $u$ to $nu$ . Equivalently, $G [ u , w ]$ consists of all vertices $nu$ such that $nu in r e a c h ( u )$ and $w in r e a c h ( x )$ , together with all the edges in $G$ connecting those vertices. \nSuppose we are given a directed acyclic graph $G$ , in which every vertex has a numerical weight, which may be positive, negative, or zero. \n(a) Describe an efficient algorithm to find the maximum-weight interval in $G$ , where the weight of each interval is the sum of the weights of its vertices.   \n(b) Describe an efficient algorithm to find the largest vertex weight in every interval in $G$ . Your algorithm should compute a two-dimensional array $M a x W t [ 1 ldots V , 1 ldots V ]$ where each entry $M a x W t [ u , w ]$ is the maximum weight among all vertices in the interval $G [ u , w ]$ . In particular, if $G [ u , w ]$ is empty, then $M a x W t [ u , w ]$ should be $- infty$ . \n17. Let $G$ be a directed acyclic graph whose vertices have labels from some fixed alphabet. Any directed path in $G$ has a label, which is a string obtained by concatenating the labels of its vertices. Recall that a palindrome is a string that is equal to its reversal. \n(a) Describe and analyze an algorithm to find the length of the longest palindrome that is the label of a path in $G$ . For example, given the graph in Figure 6.23, your algorithm should return the integer 6, which is the length of the palindrome HANNAH. \n(b) Describe an algorithm to find the longest palindrome that is a subsequence of the label of a path in $G$ .   \n(c) Suppose $G$ has a single source s and a single sink t. Describe an algorithm to find the shortest palindrome that is a supersequence of the label of a path in $G$ from $s$ to $t$ . \n18. Suppose you are given two directed acyclic graphs $G$ and $H$ in which every node has a label from some finite alphabet; different nodes may have the same label. The label of a path in either dag is the string obtained by concatenating the labels of its vertices. \n(a) Describe and analyze an algorithm to compute the length of the longest string that is both the label of a path in $G$ and the label of a path in $H$ .   \n(b) Describe and analyze an algorithm to compute the length of the longest string that is both a subsequence of the label of a path in $G$ and a subsequence of the label of a path in $H$ .   \n(c) Describe and analyze an algorithm to compute the length of the shortest string that is both a supersequence of the label of a path in $G$ and a supersequence of the label of a path in $H$ . [Hint: This is easier than it looks.] \n19. Let $G$ be an arbitrary (not necessarily acyclic) directed graph in which every vertex $nu$ has an integer weight $w ( nu )$ . \n(a) Describe an algorithm to find the longest directed path in $G$ whose vertex weights define an increasing sequence.   \n(b) Describe and analyze an algorithm to determine the maximum-weight vertex reachable from each vertex in $G$ . That is, for each vertex $nu$ , your algorithm needs to compute maxreach $( nu ) : = operatorname* { m a x } { w ( x ) mid x in r e a c h ( nu ) }$ . \n20. (a) Suppose you are given a directed acyclic graph $G$ with $n$ vertices and an integer $k leq n$ . Describe an efficient algorithm to find a set of at most $k$ vertex-disjoint paths that visit every vertex in $G$ . (b) Now suppose the edges of the input dag $G$ have weights, which may be positive, negative, or zero. Describe an efficient algorithm to find a set of at most $k$ vertex-disjoint paths with minimum total weight that visit every vertex in $G$ . \nYour algorithms should run in $O ( n ^ { k + c } )$ time for some small constant c. A single vertex is a path with weight zero. (We will see a more efficient algorithm for part (a) in Chapter 11.) \n21. Kris is a professional rock climber who is competing in the U.S. climbing nationals. The competition requires Kris to use as many holds on the climbing wall as possible, using only transitions that have been explicitly allowed by the route-setter. \nThe climbing wall has $n$ holds. Kris is given a list of $m$ pairs $( x , y )$ of holds, each indicating that moving directly from hold $x$ to hold $y$ is allowed; however, moving directly from $y$ to $x$ is not allowed unless the list also includes the pair $( y , x )$ . Kris needs to figure out a sequence of allowed transitions that uses as many holds as possible, since each new hold increases his score by one point. The rules allow Kris to choose the first and last hold in his climbing route. The rules also allow him to use each hold as many times as he likes; however, only the first use of each hold increases Kris’s score. \n(a) Define the natural graph representing the input. Describe and analyze an algorithm to solve Kris’s climbing problem if you are guaranteed that the input graph is a dag.   \n(b) Describe and analyze an algorithm to solve Kris’s climbing problem with no restrictions on the input graph. \nBoth of your algorithms should output the maximum possible score that Kris can earn. \n22. There are $n$ galaxies connected by $m$ intergalactic teleport-ways. Each teleport-way joins two galaxies and can be traversed in both directions. However, the company that runs the teleport-ways has established an extremely lucrative cost structure: Anyone can teleport further from their home galaxy at no cost whatsoever, but teleporting toward their home galaxy is prohibitively expensive. \nJudy has decided to take a sabbatical tour of the universe by visiting as many galaxies as possible, starting at her home galaxy. To save on travel expenses, she wants to teleport away from her home galaxy at every step, except for the very last teleport home. \n(a) Describe and analyze an algorithm to compute the maximum number of galaxies that Judy can visit. Your input consists of an undirected graph $G$ with $n$ vertices and $m$ edges describing the teleport-way network, an integer $1 leq s leq n$ identifying Judy’s home galaxy, and an array $D [ 1 ldots n ]$ containing the distances of each galaxy from s.   \nn(b) Just before embarking on her universal tour, Judy wins the space lottery, giving her just enough money to afford two teleports toward her home galaxy. Describe a new algorithm to compute the maximum number of distinct galaxies Judy can visit. She can visit the same galaxy more than once, but crucially, only the first visit counts toward her total. \n23. The Doctor and River Song decide to play a game on a directed acyclic graph $G$ , which has one source $s$ and one sink $t$ .8 \nEach player has a token on one of the vertices of $G$ . At the start of the game, The Doctor’s token is on the source vertex s, and River’s token is on the sink vertex t. The players alternate turns, with The Doctor moving first. On each of his turns, the Doctor moves his token forward along a directed edge; on each of her turns, River moves her token backward along a directed edge. \nIf the two tokens ever meet on the same vertex, River wins the game. (“Hello, Sweetie!”) If the Doctor’s token reaches t or River’s token reaches s before the two tokens meet, then the Doctor wins the game. \nDescribe and analyze an algorithm to determine who wins this game, assuming both players play perfectly. That is, if the Doctor can win no matter how River moves, then your algorithm should output “Doctor”, and if River can win no matter how the Doctor moves, your algorithm should output “River”. (Why are these the only two possibilities?) The input to your algorithm is the graph $G$ . \n$a v _ { 2 4 }$ . Let $x = x _ { 1 } x _ { 2 } ldots x _ { n }$ be a given $n$ -character string over some finite alphabet $Sigma$ , and let $A$ be a deterministic finite-state machine with m states over the same alphabet. \n(a) Describe and analyze an algorithm to compute the length of the longest subsequence of $x$ that is accepted by $A .$ . For example, if A accepts the language $( mathsf { A R } ) ^ { * }$ and $x =$ ABRACADABRA, your algorithm should output the number 4, which is the length of the subsequence ARAR.   \n(b) Describe and analyze an algorithm to compute the length of the shortest supersequence of $x$ that is accepted by $A$ . For example, if $A$ accepts the language (ABCDR)∗ and $x =$ ABRACADABRA, your algorithm should output the number 25, which is the length of the supersequence ABCDRABCDRABCDRABCDRABCDR. \nAnalyze your algorithms in terms of the length $n$ of the input string, the number m of states in the finite-state machine, and the size of the alphabet $Sigma$ . \n25. Not every dynamic programming algorithm can be modeled as finding an optimal path through a directed acyclic graph, but every dynamic programming algorithm does process some underlying dependency graph in postorder. \n(a) Suppose we are given a directed acyclic graph $G$ where every node stores a numerical search key. Describe and analyze an algorithm to find the largest binary search tree that is a subgraph of $G$ .   \n(b) Suppose we are given a directed acyclic graph $G$ and two vertices $s$ and $t$ . Describe an algorithm to compute the number of directed paths in $G$ from s to $t$ . (Assume that any arithmetic operation requires $O ( 1 )$ time.) \n(c) Let $G$ be a directed acyclic graph with the following features: \n• $G$ has a single source $s$ and several sinks $t _ { 1 } , t _ { 2 } , ldots , t _ { k }$ . • Each edge $nu {  } w$ has an associated weight $p ( nu {  } w )$ between 0 and 1. • For each non\u0001-sink vertex $nu$ , the total weight \u0001of all edges leaving $nu$ is 1; that is, $begin{array} { r } { sum _ { w } p ( nu to w ) = 1 } end{array}$ . \nThe weights $p ( nu {  } w )$ defi\u0001ne a random walk in $G$ from the source $s$ to some sink $t _ { i }$ ; after reac\u0001hing any non-sink vertex $nu$ , the walk follows edge $nu {  } w$ with probability $p ( nu {  } w )$ . All probabilities are mutually independe\u0001nt. Describe and analyze\u0001an algorithm to compute the probability that this random walk reaches sink $t _ { i }$ , for every index i. (Assume that each arithmetic operation takes only $O ( 1 )$ time.) \nWe must all hang together, gentlemen, or else we shall most assuredly hang separately. \n— Benjamin Franklin, at the signing of the Declaration of Independence (July 4, 1776) \nI remember seeking advice from someone—who could it have been?—about whether this work was worth submitting for publication; the reasoning it uses is so very simple....Fortunately he advised me to go ahead, and many years passed before another of my publications became as well-known as this very simple one. \n— Joseph Kruskal, describing his shortest-spanning-subtree algorithm (1997) \nClean ALL the things! \n— Allie Brosh, “This is Why I’ll Never be an Adult”, Hyperbole and a Half, June 17, 2010. \nMinimum Spanning Trees \nSuppose we are given a connected, undirected, weighted graph. This is a graph $G = ( V , E )$ together with a function $boldsymbol { w } colon boldsymbol { E } to mathbb { R }$ that assigns a real weight $w ( e )$ to each edge $e$ , which may be positive, negative, or zero. This chapter describes several algorithms to find the minimum spanning tree of $G$ , that is, the spanning tree $T$ that minimizes the function \nSee Figure 7.1 for an example. \n7.1 Distinct Edge Weights \nAn annoying subtlety in the problem statement is that weighted graphs can have more than one spanning tree with the same minimum weight; in particular, if every edge in $G$ has weight 1, then every spanning tree of $G$ is a minimum spanning tree, with weight $V - 1$ . This ambiguity complicates the development of our algorithms; everything would be much simpler if we could simply assume that minimum spanning trees are unique.",
      "metadata": {
        "content": "Exercises \nDepth-first search, topological sort, and strong components \n0. (a) Describe an algorithm to compute the reversal $r e nu ( G )$ of a directed graph in $O ( V + E )$ time. (b) Prove that for every directed graph $G$ , the strong component graph $s c c ( G )$ is acyclic. (c) Prove that $s c c ( r e nu ( G ) ) = r e nu ( s c c ( G ) )$ for every directed graph $G$ . (d) Fix an arbitrary directed graph $G$ . For any vertex $nu$ of $G$ , let $S ( nu )$ denote the strong component of $G$ that contains $nu$ . For all vertices $u$ and $nu$ of $G$ , prove that $u$ can reach $nu$ in $G$ if and only if $boldsymbol { S } ( boldsymbol { u } )$ can reach $S ( nu )$ in $s c c ( G )$ . \n1. A directed graph $G$ is semi-connected if, for every pair of vertices $u$ and $nu$ , either $u$ is reachable from $nu$ or $nu$ is reachable from $u$ (or both). \n(a) Give an example of a directed acyclic graph with a unique source that is not semi-connected.   \n(b) Describe and analyze an algorithm to determine whether a given directed acyclic graph is semi-connected. \n(c) Describe and analyze an algorithm to determine whether an arbitrary directed graph is semi-connected. \n2. The police department in the city of Sham-Poobanana has made every street in the city one-way. Despite widespread complaints from confused motorists, the mayor claims that it is possible to legally drive from any intersection in Sham-Poobanana to any other intersection. \n(a) The city needs to either verify or refute the mayor’s claim. Formalize this problem in terms of graphs, and then describe and analyze an algorithm to solve it.   \n(b) After running your algorithm from part (a), the mayor reluctantly admits that she was lying misinformed. Call an intersection $x$ good if, for any intersection $y$ that one can legally reach from $x$ , it is possible to legally drive from $y$ back to $x$ . Now the mayor claims that over $9 5 %$ of the intersections in Sham-Poobanana are good. Describe and analyze an efficient algorithm to verify or refute her claim. \nFor full credit, both algorithms should run in linear time. \n3. Suppose we are given a directed acyclic graph $G$ with a unique source s and a unique sink $t$ . A vertex $nu notin { s , t }$ is called an $( s , t )$ -cut vertex if every path from $s$ to $t$ passes through $nu$ , or equivalently, if deleting $nu$ makes $t$ unreachable from s. Describe and analyze an algorithm to find every $( s , t )$ -cut vertex in $G$ . \n4. A vertex $nu$ in a connected undirected graph $G$ is called a cut vertex if the subgraph $G - nu$ (obtained by removing $nu$ from $G$ ) is disconnected. \n(a) Describe a linear-time algorithm that determines, given an undirected graph $G$ and a vertex $nu$ , whether $nu$ is a cut vertex in $G$ . What is the running time to find all cut vertices by trying your algorithm for each vertex?   \n(b) Let $T$ be a depth-first spanning tree of an undirected graph $G$ . i. Prove that the root of $T$ is a cut vertex of $G$ if and only if it has more than one child in $T$ . ii. Prove that a non-root vertex $nu$ is a cut vertex of $G$ if and only if at least one descendant (in $T$ ) of each child of $nu$ (in $T$ ) is a neighbor (in $G$ ) of some proper ancestor of $nu$ (in $T$ ). [Hint: These claims no longer hold if $T$ not a depth-first spanning tree and/or $G$ is a directed graph.]   \n(c) Describe an algorithm that identifies every cut vertex in a given undirected graph in $O ( V + E )$ time. \n5. An edge e in a connected undirected graph $G$ is called a bridge (or a cut edge) if the subgraph $G - e$ (obtained by removing e from $G$ ) is disconnected. \n(a) Given $G$ and edge e describe a linear-time algorithm that determines whether $e$ is a bridge or not. What is the running time to find all bridges by trying your algorithm for each edge?   \n(b) Let $T$ be an arbitrary spanning tree of $G$ . Prove that every bridges of $G$ is also an edge in $T$ . This claim implies that $G$ has at most $V - 1$ bridges. How does this information improve your algorithm from part (a) to find all bridges?   \n(c) Now suppose we root $T$ at an arbitrary vertex $r$ . For any vertex $nu$ , let $T _ { nu }$ denote the subtree of $T$ rooted at $nu$ ; for example, $T _ { r } = T$ . Let $u nu$ be an arbitrary edge of $T$ , where $u$ is the parent of $nu$ . Prove that $u nu$ is a bridge of $G$ if and only if $u nu$ is the only edge in $G$ with exactly one endpoint in $T _ { nu }$ .   \n(d) Describe a linear-time algorithm to identify every bridge in $G$ . [Hint: Let $T$ be a depth-first spanning tree of G.] \n6. The transitive closure $G ^ { T }$ of a directed graph $G$ is a directed graph with the same vertices as $G$ , that contains any edge $u {  } nu$ if and only if there is a directed path from $u$ to $nu$ in $G$ . A transitive redu\u0001ction of $G$ is a graph with the smallest possible number of edges whose transitive closure is $G ^ { T }$ . The same graph may have several transitive reductions. \n(a) Describe an efficient algorithm to compute the transitive closure of a given directed graph. \n(b) Prove that a directed graph $G$ has a unique transitive reduction if and only if $G$ is acyclic.   \n(c) Describe an efficient algorithm to compute a transitive reduction of a given directed graph. \n7. One of the oldest algorithms for exploring arbitrary connected graphs was proposed by Gaston Tarry in 1895, as a systematic procedure for solving mazes.7 The input to Tarry’s algorithm is an undirected graph $G$ ; however, for ease of presentation, we formally split each undirected edge $u nu$ into two directed edges $u {  } nu$ and $nu {  } u$ . (In an actual implementation, this split is trivial; the algorit\u0001hm simply\u0001uses the given adjacency list for $G$ as though $G$ were directed.) \nWe informally say that Tarry’s algorithm “visits” vertex $nu$ every time it marks $nu$ , and it “traverses” edge $nu {  } w$ when it colors that edge red and recursively calls RecTarry $( w )$ . Unlike\u0001our earlier graph traversal algorithm, Tarry’s algorithm can mark same vertex multiple times. \n(a) Describe how to implement Tarry’s algorithm so that it runs in $O ( V + E )$ time.   \n(b) Prove that no directed edge in $G$ is traversed more than once.   \n(c) When the algorithm visits a vertex $nu$ for the $k$ th time, exactly how many edges into $nu$ are red, and exactly how many edges out of $nu$ are red? [Hint: Consider the starting vertex s separately from the other vertices.]   \n(d) Prove each vertex $nu$ is visited at most $deg ( nu )$ times, except the starting vertex s, which is visited at most $deg ( s ) + 1$ times. This claim immediately implies that Tarry $( G )$ terminates. \n(e) Prove that the last vertex visited by Tarry $( G )$ is the starting vertex s. \n(f) For every vertex $nu$ that Tarry $( G )$ visits, prove that all edges into $nu$ and out of $nu$ are red when Tarry $( G )$ halts. [Hint: Consider the vertices in the order that they are marked for the first time, starting with $s$ , and prove the claim by induction.]   \n(g) Prove that Tarry $( G )$ visits every vertex of $G$ . This claim and the previous claim imply that Tarry $( G )$ traverses every edge of $G$ exactly once. \n8. Consider the following variant of Tarry’s graph-traversal algorithm; this variant traverses green edges without recoloring them red and assigns two numerical labels to every vertex: \nProve or disprove the following claim: When Tarry2 $( G )$ halts, the green edges define a spanning tree and the labels $nu$ .pre and $nu$ .post define a preorder and postorder labeling that are all consistent with a single depth-first search of $G$ . In other words, prove or disprove that Tarry2 produces the same output as depth-first search, even though it visits the edges in a completely different order. \n9. You have a collection of $n$ lock-boxes and $m$ gold keys. Each key unlocks at most one box. However, each box might be unlocked by one key, by multiple keys, or by no keys at all. There are only two ways to open each box once it is locked: Unlock it properly (which requires having one matching key in your hand), or smash it to bits with a hammer. \nYour baby brother, who loves playing with shiny objects, has somehow managed to lock all your keys inside the boxes! Luckily, your home security system recorded everything, so you know exactly which keys (if any) are inside each box. You need to get all the keys back out of the boxes, because they are made of gold. Clearly you have to smash at least one box. \n(a) Your baby brother has found the hammer and is eagerly eyeing one of the boxes. Describe and analyze an algorithm to determine if it is \npossible to retrieve all the keys without smashing any box except the one your brother has chosen. (b) Describe and analyze an algorithm to compute the minimum number of boxes that must be smashed to retrieve all the keys. \n10. Suppose you are teaching an algorithms course. In your second midterm, you give your students a drawing of a graph and ask then to indicate a breadth-first search tree and a depth-first search tree rooted at a particular vertex. Unfortunately, once you start grading the exam, you realize that the graph you gave the students has several such spanning trees—far too many to list. Instead, you need a way to tell whether each student’s submission is correct! \nIn each of the following problems, suppose you are given a connected graph $G$ , a start vertex s, and a spanning tree $T$ of $G$ . \n(a) Suppose $G$ is undirected. Describe and analyze an algorithm to decide whether $T$ is a depth-first spanning tree rooted at $s$ .   \n(b) Suppose $G$ is undirected. Describe and analyze an algorithm to decide whether $T$ is a breadth-first spanning tree rooted at s. [Hint: It’s not enough for $T$ to be an unweighted shortest-path tree. Yes, this is the right chapter for this problem!]   \n(c) Suppose $G$ is directed. Describe and analyze an algorithm to decide whether $T$ is a breadth-first spanning tree rooted at s. [Hint: Solve part $( b )$ first.]   \n(d) Suppose $G$ is directed. Describe and analyze an algorithm to decide whether $T$ is a depth-first spanning tree rooted at $s$ . \n11. Several modern programming languages, including JavaScript, Python, Perl, and Ruby, include a feature called parallel assignment, which allows multiple assignment operations to be encoded in a single line of code. For example, the Python code $mathsf { x } , mathsf { y } = mathsf { 0 } , mathsf { 1 }$ simultaneously sets $x$ to 0 and y to 1. The values of the right-hand side of the assignment are all determined by the old values of the variables. Thus, the Python code a, ${ mathsf b } = { mathsf b }$ ,a swaps the values of a and b, and the following Python code computes the nth Fibonacci number: \ndef fib(n): prev, curr $= ~ 1$ , 0 while $n > 0$ : prev, curr, ${ mathfrak { n } } =$ curr, prev+curr, n-1 return curr \nSuppose the interpreter you are writing needs to convert every parallel assignment into an equivalent sequence of individual assignments. For example, the parallel assignment a, ${ bf b } = boldsymbol { 0 } , 1$ can be serialized in either order— either $a = 0$ ; $b = 1$ or $a = 0$ ; $b = 1$ —but the parallel assignment $mathsf { x } , mathsf { y } = mathsf { x } + 1$ , $x + y$ can only be serialized as $y = x + y$ ; $x = x + 1$ . Serialization may require one or more additional temporary variables; for example, serializing a, $b = 6$ ,a requires one temporary variable, and serializing $x , y = x { + } y$ , $x - y$ requires two temporary variables. \n\n(a) Describe an algorithm to determine whether a given parallel assignment can be serialized without additional temporary variables.   \n(b) Describe an algorithm to determine whether a given parallel assignment can be serialized with exactly one additional temporary variable. \nAssume that the given parallel assignment involves only simple integer variables (no indirection via pointers or arrays); no variable appears on the left side more than once; and expressions on the right side have no side effects. Don’t worry about the details of parsing the assignment statement; just assume (but describe!) an appropriate graph representation. \nDynamic Programming \n12. Suppose we are given a directed acyclic graph $G$ whose nodes represent jobs and whose edges represent precedence constraints; that is. each edge $u {  } nu$ indicates the job $u$ must be completed before job $nu$ begins. Each node $nu$ al\u0001so has a weight $T ( nu )$ indicating the time required to execute job $nu$ . \n(a) Describe an algorithm to determine the shortest interval of time in which all jobs in $G$ can be executed.   \n(b) Suppose the first job starts at time 0. Describe an algorithm to determine, for each vertex $nu$ , the earliest time when job $nu$ can begin.   \n(c) Now describe an algorithm to determine, for each vertex $nu$ , the latest time when job $nu$ can begin without violating the precedence constraints or increasing the overall completion time (computed in part (a)), assuming that every job except $nu$ starts at its earliest start time (computed in part (b)). \n13. Let $G$ be a directed acyclic graph with a unique source s and a unique sink $t$ . \n(a) A Hamiltonian path in $G$ is a directed path in $G$ that contains every vertex in $G$ . Describe an algorithm to determine whether $G$ has a Hamiltonian path.   \n(b) Suppose the vertices of $G$ have weights. Describe an efficient algorithm to find the path from $s$ to $t$ with maximum total weight.   \n(c) Suppose we are also given an integer $ell$ . Describe an efficient algorithm to find the maximum-weight path from $s$ to $t$ that contains at most $ell$ edges. (Assume there is at least one such path.)   \n(d) Suppose some of the vertices of $G$ are marked as important, and we are also given an integer $k$ . Describe an efficient algorithm to find the maximum-weight path from s to $t$ that visits at least $k$ important vertices. (Assume there is at least one such path.)   \n(e) Describe an algorithm to compute the number of paths from s to $t$ in $G$ . (Assume that you can add arbitrarily large integers in $O ( 1 )$ time.) \n\n14. Let $G$ be a directed acyclic graph whose vertices have labels from some fixed alphabet, and let $A [ 1 ldots ell ]$ be a string over the same alphabet. Any directed path in $G$ has a label, which is a string obtained by concatenating the labels of its vertices. \n(a) Describe an algorithm that either finds a path in $G$ whose label is $A$ or correctly reports that there is no such path.   \n(b) Describe an algorithm to find the number of paths in $G$ whose label is $A$ . (Assume that you can add arbitrarily large integers in $O ( 1 )$ time.)   \n(c) Describe an algorithm to find the longest path in $G$ whose label is a subsequence of A.   \n(d) Describe an algorithm to find the shortest path in $G$ whose label is a supersequence of A.   \n(e) Describe an algorithm to find a path in $G$ whose label has minimum edit distance from A. \n15. A polygonal path is a sequence of line segments joined end-to-end; the endpoints of these line segments are called the vertices of the path. The length of a polygonal path is the sum of the lengths of its segments. A polygonal path with vertices $( x _ { 1 } , y _ { 1 } ) , ( x _ { 2 } , y _ { 2 } ) , ldots , ( x _ { k } , y _ { k } )$ is monotonically increasing if $x _ { i } < x _ { i + 1 }$ and $y _ { i } < y _ { i + 1 }$ for every index $i$ —informally, each vertex of the path is above and to the right of its predecessor. \nSuppose you are given a set $s$ of $n$ points in the plane, represented as two arrays $X [ 1 ldots n ]$ and $Y [ 1 ldots n ]$ . Describe and analyze an algorithm to compute the length of the longest monotonically increasing path with vertices in $s$ . Assume you have a subroutine $mathrm { L E N G T H } ( x , y , x ^ { prime } , y ^ { prime } )$ that returns the length of the segment from $( x , y )$ to $( x ^ { prime } , y ^ { prime } )$ . \n16. For any two nodes $u$ and $w$ in a directed acyclic graph $G$ , the interval $G [ u , w ]$ is the union of all directed paths in $G$ from $u$ to $nu$ . Equivalently, $G [ u , w ]$ consists of all vertices $nu$ such that $nu in r e a c h ( u )$ and $w in r e a c h ( x )$ , together with all the edges in $G$ connecting those vertices. \nSuppose we are given a directed acyclic graph $G$ , in which every vertex has a numerical weight, which may be positive, negative, or zero. \n(a) Describe an efficient algorithm to find the maximum-weight interval in $G$ , where the weight of each interval is the sum of the weights of its vertices.   \n(b) Describe an efficient algorithm to find the largest vertex weight in every interval in $G$ . Your algorithm should compute a two-dimensional array $M a x W t [ 1 ldots V , 1 ldots V ]$ where each entry $M a x W t [ u , w ]$ is the maximum weight among all vertices in the interval $G [ u , w ]$ . In particular, if $G [ u , w ]$ is empty, then $M a x W t [ u , w ]$ should be $- infty$ . \n17. Let $G$ be a directed acyclic graph whose vertices have labels from some fixed alphabet. Any directed path in $G$ has a label, which is a string obtained by concatenating the labels of its vertices. Recall that a palindrome is a string that is equal to its reversal. \n(a) Describe and analyze an algorithm to find the length of the longest palindrome that is the label of a path in $G$ . For example, given the graph in Figure 6.23, your algorithm should return the integer 6, which is the length of the palindrome HANNAH. \n(b) Describe an algorithm to find the longest palindrome that is a subsequence of the label of a path in $G$ .   \n(c) Suppose $G$ has a single source s and a single sink t. Describe an algorithm to find the shortest palindrome that is a supersequence of the label of a path in $G$ from $s$ to $t$ . \n18. Suppose you are given two directed acyclic graphs $G$ and $H$ in which every node has a label from some finite alphabet; different nodes may have the same label. The label of a path in either dag is the string obtained by concatenating the labels of its vertices. \n(a) Describe and analyze an algorithm to compute the length of the longest string that is both the label of a path in $G$ and the label of a path in $H$ .   \n(b) Describe and analyze an algorithm to compute the length of the longest string that is both a subsequence of the label of a path in $G$ and a subsequence of the label of a path in $H$ .   \n(c) Describe and analyze an algorithm to compute the length of the shortest string that is both a supersequence of the label of a path in $G$ and a supersequence of the label of a path in $H$ . [Hint: This is easier than it looks.] \n19. Let $G$ be an arbitrary (not necessarily acyclic) directed graph in which every vertex $nu$ has an integer weight $w ( nu )$ . \n(a) Describe an algorithm to find the longest directed path in $G$ whose vertex weights define an increasing sequence.   \n(b) Describe and analyze an algorithm to determine the maximum-weight vertex reachable from each vertex in $G$ . That is, for each vertex $nu$ , your algorithm needs to compute maxreach $( nu ) : = operatorname* { m a x } { w ( x ) mid x in r e a c h ( nu ) }$ . \n20. (a) Suppose you are given a directed acyclic graph $G$ with $n$ vertices and an integer $k leq n$ . Describe an efficient algorithm to find a set of at most $k$ vertex-disjoint paths that visit every vertex in $G$ . (b) Now suppose the edges of the input dag $G$ have weights, which may be positive, negative, or zero. Describe an efficient algorithm to find a set of at most $k$ vertex-disjoint paths with minimum total weight that visit every vertex in $G$ . \nYour algorithms should run in $O ( n ^ { k + c } )$ time for some small constant c. A single vertex is a path with weight zero. (We will see a more efficient algorithm for part (a) in Chapter 11.) \n21. Kris is a professional rock climber who is competing in the U.S. climbing nationals. The competition requires Kris to use as many holds on the climbing wall as possible, using only transitions that have been explicitly allowed by the route-setter. \nThe climbing wall has $n$ holds. Kris is given a list of $m$ pairs $( x , y )$ of holds, each indicating that moving directly from hold $x$ to hold $y$ is allowed; however, moving directly from $y$ to $x$ is not allowed unless the list also includes the pair $( y , x )$ . Kris needs to figure out a sequence of allowed transitions that uses as many holds as possible, since each new hold increases his score by one point. The rules allow Kris to choose the first and last hold in his climbing route. The rules also allow him to use each hold as many times as he likes; however, only the first use of each hold increases Kris’s score. \n(a) Define the natural graph representing the input. Describe and analyze an algorithm to solve Kris’s climbing problem if you are guaranteed that the input graph is a dag.   \n(b) Describe and analyze an algorithm to solve Kris’s climbing problem with no restrictions on the input graph. \nBoth of your algorithms should output the maximum possible score that Kris can earn. \n22. There are $n$ galaxies connected by $m$ intergalactic teleport-ways. Each teleport-way joins two galaxies and can be traversed in both directions. However, the company that runs the teleport-ways has established an extremely lucrative cost structure: Anyone can teleport further from their home galaxy at no cost whatsoever, but teleporting toward their home galaxy is prohibitively expensive. \nJudy has decided to take a sabbatical tour of the universe by visiting as many galaxies as possible, starting at her home galaxy. To save on travel expenses, she wants to teleport away from her home galaxy at every step, except for the very last teleport home. \n(a) Describe and analyze an algorithm to compute the maximum number of galaxies that Judy can visit. Your input consists of an undirected graph $G$ with $n$ vertices and $m$ edges describing the teleport-way network, an integer $1 leq s leq n$ identifying Judy’s home galaxy, and an array $D [ 1 ldots n ]$ containing the distances of each galaxy from s.   \nn(b) Just before embarking on her universal tour, Judy wins the space lottery, giving her just enough money to afford two teleports toward her home galaxy. Describe a new algorithm to compute the maximum number of distinct galaxies Judy can visit. She can visit the same galaxy more than once, but crucially, only the first visit counts toward her total. \n23. The Doctor and River Song decide to play a game on a directed acyclic graph $G$ , which has one source $s$ and one sink $t$ .8 \nEach player has a token on one of the vertices of $G$ . At the start of the game, The Doctor’s token is on the source vertex s, and River’s token is on the sink vertex t. The players alternate turns, with The Doctor moving first. On each of his turns, the Doctor moves his token forward along a directed edge; on each of her turns, River moves her token backward along a directed edge. \nIf the two tokens ever meet on the same vertex, River wins the game. (“Hello, Sweetie!”) If the Doctor’s token reaches t or River’s token reaches s before the two tokens meet, then the Doctor wins the game. \nDescribe and analyze an algorithm to determine who wins this game, assuming both players play perfectly. That is, if the Doctor can win no matter how River moves, then your algorithm should output “Doctor”, and if River can win no matter how the Doctor moves, your algorithm should output “River”. (Why are these the only two possibilities?) The input to your algorithm is the graph $G$ . \n$a v _ { 2 4 }$ . Let $x = x _ { 1 } x _ { 2 } ldots x _ { n }$ be a given $n$ -character string over some finite alphabet $Sigma$ , and let $A$ be a deterministic finite-state machine with m states over the same alphabet. \n(a) Describe and analyze an algorithm to compute the length of the longest subsequence of $x$ that is accepted by $A .$ . For example, if A accepts the language $( mathsf { A R } ) ^ { * }$ and $x =$ ABRACADABRA, your algorithm should output the number 4, which is the length of the subsequence ARAR.   \n(b) Describe and analyze an algorithm to compute the length of the shortest supersequence of $x$ that is accepted by $A$ . For example, if $A$ accepts the language (ABCDR)∗ and $x =$ ABRACADABRA, your algorithm should output the number 25, which is the length of the supersequence ABCDRABCDRABCDRABCDRABCDR. \nAnalyze your algorithms in terms of the length $n$ of the input string, the number m of states in the finite-state machine, and the size of the alphabet $Sigma$ . \n25. Not every dynamic programming algorithm can be modeled as finding an optimal path through a directed acyclic graph, but every dynamic programming algorithm does process some underlying dependency graph in postorder. \n(a) Suppose we are given a directed acyclic graph $G$ where every node stores a numerical search key. Describe and analyze an algorithm to find the largest binary search tree that is a subgraph of $G$ .   \n(b) Suppose we are given a directed acyclic graph $G$ and two vertices $s$ and $t$ . Describe an algorithm to compute the number of directed paths in $G$ from s to $t$ . (Assume that any arithmetic operation requires $O ( 1 )$ time.) \n(c) Let $G$ be a directed acyclic graph with the following features: \n• $G$ has a single source $s$ and several sinks $t _ { 1 } , t _ { 2 } , ldots , t _ { k }$ . • Each edge $nu {  } w$ has an associated weight $p ( nu {  } w )$ between 0 and 1. • For each non\u0001-sink vertex $nu$ , the total weight \u0001of all edges leaving $nu$ is 1; that is, $begin{array} { r } { sum _ { w } p ( nu to w ) = 1 } end{array}$ . \nThe weights $p ( nu {  } w )$ defi\u0001ne a random walk in $G$ from the source $s$ to some sink $t _ { i }$ ; after reac\u0001hing any non-sink vertex $nu$ , the walk follows edge $nu {  } w$ with probability $p ( nu {  } w )$ . All probabilities are mutually independe\u0001nt. Describe and analyze\u0001an algorithm to compute the probability that this random walk reaches sink $t _ { i }$ , for every index i. (Assume that each arithmetic operation takes only $O ( 1 )$ time.) \nWe must all hang together, gentlemen, or else we shall most assuredly hang separately. \n— Benjamin Franklin, at the signing of the Declaration of Independence (July 4, 1776) \nI remember seeking advice from someone—who could it have been?—about whether this work was worth submitting for publication; the reasoning it uses is so very simple....Fortunately he advised me to go ahead, and many years passed before another of my publications became as well-known as this very simple one. \n— Joseph Kruskal, describing his shortest-spanning-subtree algorithm (1997) \nClean ALL the things! \n— Allie Brosh, “This is Why I’ll Never be an Adult”, Hyperbole and a Half, June 17, 2010. \nMinimum Spanning Trees \nSuppose we are given a connected, undirected, weighted graph. This is a graph $G = ( V , E )$ together with a function $boldsymbol { w } colon boldsymbol { E } to mathbb { R }$ that assigns a real weight $w ( e )$ to each edge $e$ , which may be positive, negative, or zero. This chapter describes several algorithms to find the minimum spanning tree of $G$ , that is, the spanning tree $T$ that minimizes the function \nSee Figure 7.1 for an example. \n7.1 Distinct Edge Weights \nAn annoying subtlety in the problem statement is that weighted graphs can have more than one spanning tree with the same minimum weight; in particular, if every edge in $G$ has weight 1, then every spanning tree of $G$ is a minimum spanning tree, with weight $V - 1$ . This ambiguity complicates the development of our algorithms; everything would be much simpler if we could simply assume that minimum spanning trees are unique.",
        "chapter": "Depth-First Search",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 88
      }
    },
    {
      "node_id": "tb1_node86",
      "content": "Kosaraju and Sharir’s Algorithm \nAt first glance, finding a vertex in a sink component quickly seems quite difficult. However, it’s actually quite easy to find a vertex in a source component—a strong component of $G$ that corresponds to a source in $s c c ( G ) .$ —using depth-first search. \nLemma 6.3. The last vertex in any postordering of G lies in a source component of $G$ . \nProof: Fix a depth-first traversal of $G$ , and let $nu$ be the last vertex in the resulting postordering. Then $mathrm { D F S } ( nu )$ must be the last direct call to DFS made by the wrapper algorithm DFSAll. Moreover, $nu$ is the root of one of the trees in the depth-first forest, so any node $x$ with $x . p o s t > nu .$ .pre is a descendant of $nu$ . Finally, $nu$ is the root of its strong component $C$ . \nFor the sake of argument, suppose there is an edge $x {  } y$ such that $x not in C$ and $y in C$ . Then $x$ can reach $y$ , and y can reach $nu$ , so $x$ can\u0001reach $nu$ . Because $nu$ is the root of $C$ , vertex $y$ is a descendant of $nu$ , and thus $nu . p r e < y . p r e$ . The edge $x {  } y$ guarantees that $y . p r e < x . p o s t$ and therefore $nu . p r e < x . p o s t$ . It follows th\u0001at $x$ is a descendant of $nu$ . But then $nu$ can reach $x$ (through tree edges), contradicting our assumption that $x not in C$ . □ \nIt is easy to check (hint, hint) that $r e nu ( s c c ( G ) ) = s c c ( r e nu ( G ) )$ for any directed graph $G$ . Thus, the last vertex in a postordering of $r e nu ( G )$ lies in a sink component of the original graph $G$ . Thus, if we traverse the graph a second time, where the wrapper function follows a reverse postordering of $r e nu ( G )$ , then each call to DFS visits exactly one strong component of $G$ .4 \nPutting everything together, we obtain the algorithm shown in Figure 6.16, which counts and labels the strong components of any directed graph in $O ( V + E )$ time. This algorithm was discovered (but never published) by Rao Kosaraju in 1978, and later independently rediscovered by Micha Sharir in 1981.5 The Kosaraju-Sharir algorithm has two phases. The first phase performs a depth-first search of $r e nu ( G )$ , pushing each vertex onto a stack when it is finished. In the second phase, we perform a whatever-first traversal of the original graph $G$ , considering vertices in the order they appear on the stack. The algorithm labels each vertex with the root of its strong component (with respect to the second depth-first traversal). \nFigure 6.17 shows the Kosaraju-Sharir algorithm running on our example graph. With only minor modifications to the algorithm, we can also compute the strong component graph $s c c ( G )$ in $O ( V + E )$ time. \nKosarajuSharir(G): $s gets$ new empty stack for all vertices $nu$ PushPostRevDFS(v, S): unmark $nu$ mark v $nu . r o o t gets mathrm { N o N E }$ for each edge $u {  } nu$ Reversed! if $u$ is unma\u0001rked Phase 1: Push in postorder in rev(G) PushPostRevDFS $( u , S )$ for all vertices $nu$ $P { cal U } S H ( nu , S )$ if $nu$ is unmarked PushPostRevDFS $( nu , S )$ LabelOneDFS(v, r): v.root r Phase 2: DFS again in stack order for each edge $nu {  } w$ while $s$ is non-empty if w.root = \u0001None $nu  P o P ( S )$ LabelOneDFS(w, r) if $nu . r o o t = mathrm { N o N E }$ LabelOneDFS(v, v) \nnTarjan’s Algorithm \nAn earlier but considerably more subtle linear-time algorithm to compute strong components was published by Bob Tarjan in 1972.6 Intuitively, Tarjan’s algorithm identifies a source component of $G$ , “deletes” it, and then “recursively” finds the remaining strong components; however, the entire computation happens during a single depth-first search. \nFix an arbitrary depth-first traversal of some directed graph $G$ . For each vertex $nu$ , let $boldsymbol { l o w } ( nu )$ denote the smallest starting time among all vertices reachable from $nu$ by a path of tree edges followed by at most one non-tree edge. Trivially, $l o w ( nu ) leq nu . p r e .$ , because $nu$ can reach itself through zero tree edges followed by zero non-tree edges. Tarjan observed that sink components can be characterized in terms of this low function. \nLemma 6.4. A vertex $nu$ is the root of a sink component of $G$ if and only if $l o w ( nu ) = nu .$ .pre and $l o w ( w ) < w$ .pre for every proper descendant w of $nu$ . \nProof: First, let $nu$ be a vertex such that $l o w ( nu ) = nu .$ pre. Then there is no edge $w {  } x$ where $w$ is a descendant of $nu$ and $x . p r e < nu$ .pre. On the other hand, $nu$ ca\u0001nnot reach any vertex y such that $y . p r e > nu . p o s t$ . It follows that $nu$ can reach only its descendants, and therefore any descendant of $nu$ can reach only descendants of $nu$ . In particular, $nu$ cannot reach its parent (if it has one), so $nu$ is the root of its strong component. \nNow suppose in addition that $l o w ( w ) < w . p r e$ for every descendant $w$ of $nu$ . Then each descendant $w$ can reach another vertex $x$ (which must be another descendant of $nu$ ) such that $x . p r e < w . p r e$ . Thus, by induction, every descendant of $nu$ can reach $nu$ . It follows that the descendants of $nu$ comprise the strong component $C$ whose root is $nu$ . Moreover, $C$ must be a sink component, because $nu$ cannot reach any vertex outside of $C$ . \nOn the other hand, suppose $nu$ is the root of a sink component $C$ . Then $nu$ can reach another vertex $w$ if and only if $w in C$ . But $nu$ can reach all of its descendants, and every vertex in $C$ is a descendant of $nu$ , so v’s descendants comprise C. If $l o w ( w ) = w . p r $ for any other node $w in C$ , then $w$ would be another root of $C$ , which is impossible. □ \nComputing $l o w ( nu )$ for every vertex $nu$ via depth-first search is straightforward; see Figure 6.18. \nLemma 6.4 implies that after running FindLow, we can identify the root of every sink component in $O ( V + E )$ time (by a global whatever-first search), and then mark and delete those sink components in $O ( V + E )$ additional time (by calling whatever-first search at each root), and then recurse. Unfortunately, the resulting algorithm might require $V$ iterations, each removing only a single vertex, naively giving us a total running time of $O ( V E )$ .",
      "metadata": {
        "content": "Kosaraju and Sharir’s Algorithm \nAt first glance, finding a vertex in a sink component quickly seems quite difficult. However, it’s actually quite easy to find a vertex in a source component—a strong component of $G$ that corresponds to a source in $s c c ( G ) .$ —using depth-first search. \nLemma 6.3. The last vertex in any postordering of G lies in a source component of $G$ . \nProof: Fix a depth-first traversal of $G$ , and let $nu$ be the last vertex in the resulting postordering. Then $mathrm { D F S } ( nu )$ must be the last direct call to DFS made by the wrapper algorithm DFSAll. Moreover, $nu$ is the root of one of the trees in the depth-first forest, so any node $x$ with $x . p o s t > nu .$ .pre is a descendant of $nu$ . Finally, $nu$ is the root of its strong component $C$ . \nFor the sake of argument, suppose there is an edge $x {  } y$ such that $x not in C$ and $y in C$ . Then $x$ can reach $y$ , and y can reach $nu$ , so $x$ can\u0001reach $nu$ . Because $nu$ is the root of $C$ , vertex $y$ is a descendant of $nu$ , and thus $nu . p r e < y . p r e$ . The edge $x {  } y$ guarantees that $y . p r e < x . p o s t$ and therefore $nu . p r e < x . p o s t$ . It follows th\u0001at $x$ is a descendant of $nu$ . But then $nu$ can reach $x$ (through tree edges), contradicting our assumption that $x not in C$ . □ \nIt is easy to check (hint, hint) that $r e nu ( s c c ( G ) ) = s c c ( r e nu ( G ) )$ for any directed graph $G$ . Thus, the last vertex in a postordering of $r e nu ( G )$ lies in a sink component of the original graph $G$ . Thus, if we traverse the graph a second time, where the wrapper function follows a reverse postordering of $r e nu ( G )$ , then each call to DFS visits exactly one strong component of $G$ .4 \nPutting everything together, we obtain the algorithm shown in Figure 6.16, which counts and labels the strong components of any directed graph in $O ( V + E )$ time. This algorithm was discovered (but never published) by Rao Kosaraju in 1978, and later independently rediscovered by Micha Sharir in 1981.5 The Kosaraju-Sharir algorithm has two phases. The first phase performs a depth-first search of $r e nu ( G )$ , pushing each vertex onto a stack when it is finished. In the second phase, we perform a whatever-first traversal of the original graph $G$ , considering vertices in the order they appear on the stack. The algorithm labels each vertex with the root of its strong component (with respect to the second depth-first traversal). \nFigure 6.17 shows the Kosaraju-Sharir algorithm running on our example graph. With only minor modifications to the algorithm, we can also compute the strong component graph $s c c ( G )$ in $O ( V + E )$ time. \nKosarajuSharir(G): $s gets$ new empty stack for all vertices $nu$ PushPostRevDFS(v, S): unmark $nu$ mark v $nu . r o o t gets mathrm { N o N E }$ for each edge $u {  } nu$ Reversed! if $u$ is unma\u0001rked Phase 1: Push in postorder in rev(G) PushPostRevDFS $( u , S )$ for all vertices $nu$ $P { cal U } S H ( nu , S )$ if $nu$ is unmarked PushPostRevDFS $( nu , S )$ LabelOneDFS(v, r): v.root r Phase 2: DFS again in stack order for each edge $nu {  } w$ while $s$ is non-empty if w.root = \u0001None $nu  P o P ( S )$ LabelOneDFS(w, r) if $nu . r o o t = mathrm { N o N E }$ LabelOneDFS(v, v) \nnTarjan’s Algorithm \nAn earlier but considerably more subtle linear-time algorithm to compute strong components was published by Bob Tarjan in 1972.6 Intuitively, Tarjan’s algorithm identifies a source component of $G$ , “deletes” it, and then “recursively” finds the remaining strong components; however, the entire computation happens during a single depth-first search. \nFix an arbitrary depth-first traversal of some directed graph $G$ . For each vertex $nu$ , let $boldsymbol { l o w } ( nu )$ denote the smallest starting time among all vertices reachable from $nu$ by a path of tree edges followed by at most one non-tree edge. Trivially, $l o w ( nu ) leq nu . p r e .$ , because $nu$ can reach itself through zero tree edges followed by zero non-tree edges. Tarjan observed that sink components can be characterized in terms of this low function. \nLemma 6.4. A vertex $nu$ is the root of a sink component of $G$ if and only if $l o w ( nu ) = nu .$ .pre and $l o w ( w ) < w$ .pre for every proper descendant w of $nu$ . \nProof: First, let $nu$ be a vertex such that $l o w ( nu ) = nu .$ pre. Then there is no edge $w {  } x$ where $w$ is a descendant of $nu$ and $x . p r e < nu$ .pre. On the other hand, $nu$ ca\u0001nnot reach any vertex y such that $y . p r e > nu . p o s t$ . It follows that $nu$ can reach only its descendants, and therefore any descendant of $nu$ can reach only descendants of $nu$ . In particular, $nu$ cannot reach its parent (if it has one), so $nu$ is the root of its strong component. \nNow suppose in addition that $l o w ( w ) < w . p r e$ for every descendant $w$ of $nu$ . Then each descendant $w$ can reach another vertex $x$ (which must be another descendant of $nu$ ) such that $x . p r e < w . p r e$ . Thus, by induction, every descendant of $nu$ can reach $nu$ . It follows that the descendants of $nu$ comprise the strong component $C$ whose root is $nu$ . Moreover, $C$ must be a sink component, because $nu$ cannot reach any vertex outside of $C$ . \nOn the other hand, suppose $nu$ is the root of a sink component $C$ . Then $nu$ can reach another vertex $w$ if and only if $w in C$ . But $nu$ can reach all of its descendants, and every vertex in $C$ is a descendant of $nu$ , so v’s descendants comprise C. If $l o w ( w ) = w . p r $ for any other node $w in C$ , then $w$ would be another root of $C$ , which is impossible. □ \nComputing $l o w ( nu )$ for every vertex $nu$ via depth-first search is straightforward; see Figure 6.18. \nLemma 6.4 implies that after running FindLow, we can identify the root of every sink component in $O ( V + E )$ time (by a global whatever-first search), and then mark and delete those sink components in $O ( V + E )$ additional time (by calling whatever-first search at each root), and then recurse. Unfortunately, the resulting algorithm might require $V$ iterations, each removing only a single vertex, naively giving us a total running time of $O ( V E )$ .",
        "chapter": "Depth-First Search",
        "section": "Strong Components in Linear Time",
        "subsection": "Kosaraju and Sharir’s Algorithm",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 86
      }
    },
    {
      "node_id": "tb1_node89",
      "content": "We must all hang together, gentlemen, or else we shall most assuredly hang separately. \n— Benjamin Franklin, at the signing of the Declaration of Independence (July 4, 1776) \nI remember seeking advice from someone—who could it have been?—about whether this work was worth submitting for publication; the reasoning it uses is so very simple....Fortunately he advised me to go ahead, and many years passed before another of my publications became as well-known as this very simple one. \n— Joseph Kruskal, describing his shortest-spanning-subtree algorithm (1997) \nClean ALL the things! \n— Allie Brosh, “This is Why I’ll Never be an Adult”, Hyperbole and a Half, June 17, 2010. \nMinimum Spanning Trees \nSuppose we are given a connected, undirected, weighted graph. This is a graph $G = ( V , E )$ together with a function $boldsymbol { w } colon boldsymbol { E } to mathbb { R }$ that assigns a real weight $w ( e )$ to each edge $e$ , which may be positive, negative, or zero. This chapter describes several algorithms to find the minimum spanning tree of $G$ , that is, the spanning tree $T$ that minimizes the function \nSee Figure 7.1 for an example. \n7.1 Distinct Edge Weights \nAn annoying subtlety in the problem statement is that weighted graphs can have more than one spanning tree with the same minimum weight; in particular, if every edge in $G$ has weight 1, then every spanning tree of $G$ is a minimum spanning tree, with weight $V - 1$ . This ambiguity complicates the development of our algorithms; everything would be much simpler if we could simply assume that minimum spanning trees are unique. \n\nFortunately, there is an easy condition that implies the uniqueness we want. \nLemma 7.1. If all edge weights in a connected graph $G$ are distinct, then $G$ has a unique minimum spanning tree. \nProof: Let $G$ be an arbitrary connected graph with two minimum spanning trees $T$ and $T ^ { prime }$ ; we need to prove that some pair of edges in $G$ have the same weight. The proof is essentially a greedy exchange argument. \nEach of our spanning trees must contain an edge that the other tree omits. Let $e$ be a minimum-weight edge in $T setminus T ^ { prime }$ , and let $e ^ { prime }$ be a minimum-weight edge in $T ^ { prime } backslash T$ (breaking ties arbitrarily). Without loss of generality, suppose $w ( e ) leq w ( e ^ { prime } )$ . \nThe subgraph $T ^ { prime } cup { e }$ contains exactly one cycle $C$ , which passes through the edge e. Let $e ^ { prime prime }$ be any edge of this cycle that is not in $T$ . At least one such edge must exist, because $T$ is a tree. (We may or may not have $e ^ { prime prime } = e ^ { prime }$ .) Because $e in T$ , we immediately have $e ^ { prime prime } ne e$ and therefore $e ^ { prime prime } in T ^ { prime } setminus T$ . It follows that $w ( e ^ { prime prime } ) geq w ( e ^ { prime } ) geq w ( e )$ . \nNow consider the spanning tree $T ^ { prime prime } { = } T ^ { prime } { + } e - e ^ { prime prime }$ . (This new tree $T ^ { prime prime }$ might be equal to $T$ .) We immediately have $w ( T ^ { prime prime } ) = w ( T ^ { prime } ) + w ( e ) - w ( e ^ { prime prime } ) leq w ( T ^ { prime } )$ . But $T ^ { prime }$ is a minimum spanning tree, so we must have $w ( T ^ { prime prime } ) = w ( T ^ { prime } )$ ; in other words, $T ^ { prime prime }$ is also a minimum spanning tree. We conclude that $w ( e ) = w ( e ^ { prime prime } )$ , which completes the proof. □ \nIf we already have an algorithm that assumes distinct edge weights, we can still run it on graphs where some edges have equal weights, as long as we have a consistent method for breaking ties. One such method uses the following algorithm in place of simple weight comparisons. ShorterEdge takes as input four integers $i , j , k , l$ , representing four (not necessarily distinct) vertices, and decides which of the two edges $( i , j )$ and $( k , l )$ has “smaller” weight. (Because the input graph undirected, the pairs $( i , j )$ and $( j , i )$ represent the same edge.) \n\nIn light of Lemma 7.1 and this tie-breaking rule, we will safely assume for the rest of this chapter that edge weights are always distinct, and therefore minimum spanning trees are always unique. In particular, we can freely discuss the minimum spanning tree with no confusion. \n7.2 The Only Minimum Spanning Tree Algorithm \nThere are many algorithms to compute minimum spanning trees, but almost all of them are instances of the following generic strategy. The situation is similar to graph traversal, where several different algorithms are all variants of the generic traversal algorithm whatever-first search. \nThe generic minimum spanning tree algorithm maintains an acyclic subgraph $F$ of the input graph $G$ , which we will call the intermediate spanning forest. At all times, $F$ satisfies the following invariant: \n$F$ is a subgraph of the minimum spanning tree of G. \nInitially, $F$ consists of $V$ one-vertex trees. The generic algorithm connects trees in $F$ by adding certain edges between them. When the algorithm halts, $F$ consists of a single spanning tree; our invariant implies that this must be the minimum spanning tree of $G$ . Obviously, we have to be careful about which edges we add to the evolving forest, because not every edge is in the minimum spanning tree. \nAt any stage of its evolution, the intermediate spanning forest $F$ induces two special types of edges in the rest of the graph. \n• An edge is useless if it is not an edge of $F$ , but both its endpoints are in the same component of $F$ .   \n• An edge is safe if it is the minimum-weight edge with exactly one endpoint in some component of $F$ .",
      "metadata": {
        "content": "We must all hang together, gentlemen, or else we shall most assuredly hang separately. \n— Benjamin Franklin, at the signing of the Declaration of Independence (July 4, 1776) \nI remember seeking advice from someone—who could it have been?—about whether this work was worth submitting for publication; the reasoning it uses is so very simple....Fortunately he advised me to go ahead, and many years passed before another of my publications became as well-known as this very simple one. \n— Joseph Kruskal, describing his shortest-spanning-subtree algorithm (1997) \nClean ALL the things! \n— Allie Brosh, “This is Why I’ll Never be an Adult”, Hyperbole and a Half, June 17, 2010. \nMinimum Spanning Trees \nSuppose we are given a connected, undirected, weighted graph. This is a graph $G = ( V , E )$ together with a function $boldsymbol { w } colon boldsymbol { E } to mathbb { R }$ that assigns a real weight $w ( e )$ to each edge $e$ , which may be positive, negative, or zero. This chapter describes several algorithms to find the minimum spanning tree of $G$ , that is, the spanning tree $T$ that minimizes the function \nSee Figure 7.1 for an example. \n7.1 Distinct Edge Weights \nAn annoying subtlety in the problem statement is that weighted graphs can have more than one spanning tree with the same minimum weight; in particular, if every edge in $G$ has weight 1, then every spanning tree of $G$ is a minimum spanning tree, with weight $V - 1$ . This ambiguity complicates the development of our algorithms; everything would be much simpler if we could simply assume that minimum spanning trees are unique. \n\nFortunately, there is an easy condition that implies the uniqueness we want. \nLemma 7.1. If all edge weights in a connected graph $G$ are distinct, then $G$ has a unique minimum spanning tree. \nProof: Let $G$ be an arbitrary connected graph with two minimum spanning trees $T$ and $T ^ { prime }$ ; we need to prove that some pair of edges in $G$ have the same weight. The proof is essentially a greedy exchange argument. \nEach of our spanning trees must contain an edge that the other tree omits. Let $e$ be a minimum-weight edge in $T setminus T ^ { prime }$ , and let $e ^ { prime }$ be a minimum-weight edge in $T ^ { prime } backslash T$ (breaking ties arbitrarily). Without loss of generality, suppose $w ( e ) leq w ( e ^ { prime } )$ . \nThe subgraph $T ^ { prime } cup { e }$ contains exactly one cycle $C$ , which passes through the edge e. Let $e ^ { prime prime }$ be any edge of this cycle that is not in $T$ . At least one such edge must exist, because $T$ is a tree. (We may or may not have $e ^ { prime prime } = e ^ { prime }$ .) Because $e in T$ , we immediately have $e ^ { prime prime } ne e$ and therefore $e ^ { prime prime } in T ^ { prime } setminus T$ . It follows that $w ( e ^ { prime prime } ) geq w ( e ^ { prime } ) geq w ( e )$ . \nNow consider the spanning tree $T ^ { prime prime } { = } T ^ { prime } { + } e - e ^ { prime prime }$ . (This new tree $T ^ { prime prime }$ might be equal to $T$ .) We immediately have $w ( T ^ { prime prime } ) = w ( T ^ { prime } ) + w ( e ) - w ( e ^ { prime prime } ) leq w ( T ^ { prime } )$ . But $T ^ { prime }$ is a minimum spanning tree, so we must have $w ( T ^ { prime prime } ) = w ( T ^ { prime } )$ ; in other words, $T ^ { prime prime }$ is also a minimum spanning tree. We conclude that $w ( e ) = w ( e ^ { prime prime } )$ , which completes the proof. □ \nIf we already have an algorithm that assumes distinct edge weights, we can still run it on graphs where some edges have equal weights, as long as we have a consistent method for breaking ties. One such method uses the following algorithm in place of simple weight comparisons. ShorterEdge takes as input four integers $i , j , k , l$ , representing four (not necessarily distinct) vertices, and decides which of the two edges $( i , j )$ and $( k , l )$ has “smaller” weight. (Because the input graph undirected, the pairs $( i , j )$ and $( j , i )$ represent the same edge.) \n\nIn light of Lemma 7.1 and this tie-breaking rule, we will safely assume for the rest of this chapter that edge weights are always distinct, and therefore minimum spanning trees are always unique. In particular, we can freely discuss the minimum spanning tree with no confusion. \n7.2 The Only Minimum Spanning Tree Algorithm \nThere are many algorithms to compute minimum spanning trees, but almost all of them are instances of the following generic strategy. The situation is similar to graph traversal, where several different algorithms are all variants of the generic traversal algorithm whatever-first search. \nThe generic minimum spanning tree algorithm maintains an acyclic subgraph $F$ of the input graph $G$ , which we will call the intermediate spanning forest. At all times, $F$ satisfies the following invariant: \n$F$ is a subgraph of the minimum spanning tree of G. \nInitially, $F$ consists of $V$ one-vertex trees. The generic algorithm connects trees in $F$ by adding certain edges between them. When the algorithm halts, $F$ consists of a single spanning tree; our invariant implies that this must be the minimum spanning tree of $G$ . Obviously, we have to be careful about which edges we add to the evolving forest, because not every edge is in the minimum spanning tree. \nAt any stage of its evolution, the intermediate spanning forest $F$ induces two special types of edges in the rest of the graph. \n• An edge is useless if it is not an edge of $F$ , but both its endpoints are in the same component of $F$ .   \n• An edge is safe if it is the minimum-weight edge with exactly one endpoint in some component of $F$ .",
        "chapter": "Minimum Spanning Trees",
        "section": "Distinct Edge Weights",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 89
      }
    }
  ],
  "output": {
    "question": "In the context of graph algorithms, how does the concept of a 'safe edge' in Jarník's algorithm for minimum spanning trees relate to the concept of 'source components' in Tarjan's algorithm for strongly connected components? Specifically, explain how the selection criteria in both algorithms ensure progress toward their respective goals.",
    "correct_answer": "In Jarník's algorithm, a 'safe edge' is the minimum-weight edge connecting the current partial spanning tree T to a vertex not yet in T, ensuring the algorithm always adds an edge that must be part of the unique minimum spanning tree (assuming distinct weights). In Tarjan's algorithm, a 'source component' is a strongly connected component with no incoming edges from other components in the condensation graph SCC(G). Identifying and processing source components first ensures the algorithm can recursively decompose the graph. Both concepts involve selecting elements (edges or components) that guarantee correctness: safe edges maintain the invariant that T is always a subtree of the MST, while processing source components ensures all vertices in a component are identified before moving to components that depend on them.",
    "distractors": [
      "Both algorithms use priority queues to select safe edges and source components based on edge weights.",
      "Safe edges are used to connect different strongly connected components, similar to how source components are identified in Tarjan's algorithm.",
      "Jarník's algorithm finds safe edges by performing depth-first search, just as Tarjan's algorithm uses DFS to find source components.",
      "The concept of a safe edge ensures the graph remains acyclic, while source components are used to break cycles in the graph."
    ],
    "rationale": "Step 1: Identify relevant snippets. Snippet [3] describes Jarník's algorithm and the concept of 'safe edges' for minimum spanning trees. Snippet [2] describes Tarjan's algorithm and mentions 'source components' for strongly connected components. Step 2: Synthesize concepts. Both algorithms involve graph traversal: Jarník's builds a spanning tree by iteratively adding safe edges, while Tarjan's decomposes a graph into strongly connected components by identifying source components. Step 3: Formulate a multi-hop question. The question connects the selection mechanism in Jarník's algorithm (safe edges) to the structural concept in Tarjan's algorithm (source components), requiring understanding of how each ensures algorithmic progress and correctness. Step 4: Ensure the question is not a simple definition question by asking for a relational explanation between the two concepts across different algorithms."
  },
  "score": {
    "relevance": 1.0,
    "faithfulness": 0.5,
    "integration": 1.0,
    "complexity": 1.0,
    "overall": 0.9000000000000001
  }
}
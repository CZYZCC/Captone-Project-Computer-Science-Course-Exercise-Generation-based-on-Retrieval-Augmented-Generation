{
  "context": [
    {
      "node_id": "tb1_node70",
      "content": "Finally, sometimes one graph can be used to implicitly represent other larger graphs. A good example of this implicit representation is the subset construction, which is normally used to convert NFAs into DFAs, but can be applied to arbitrary directed graphs as follows. Given any directed graph $G = left( V , E right)$ , we can define a new directed graph $G ^ { prime } = ( 2 ^ { V } , E ^ { prime } )$ whose vertices are all subsets of vertices in $V$ , and whose edges $E ^ { prime }$ are defined as follows: \nWe can mechanically translate this definition into an algorithm to construct $G ^ { prime }$ from $G$ , but strictly speaking, this construction is unnecessary, because $G$ is already an implicit representation of $G ^ { prime }$ . \nIt’s important not to confuse any of these examples/representations with the actual formal definition: A graph is a pair of sets $( V , E )$ , where $V$ is an arbitrary non-empty finite set, and $E$ is a set of pairs (either ordered or unordered) of elements of $V$ . In short: A graph is a set of pairs of things. \n5.4 Data Structures \nIn practice, graphs are usually represented by one of two standard data structures: adjacency lists and adjacency matrices. At a high level, both data structures are arrays indexed by vertices; this requires that each vertex has a unique integer identifier between 1 and $V$ . In a formal sense, these integers are the vertices. \nAdjacency Lists \nBy far the most common data structure for storing graphs is the adjacency list. An adjacency list is an array of lists, each containing the neighbors of one of the vertices (or the out-neighbors if the graph is directed).6 For undirected graphs, each edge $u nu$ is stored twice, once in u’s neighbor list and once in v’s neighbor list; for directed graphs, each edge $u {  } nu$ is stored only once, in the neighbor list of the tail $u$ . For both types of gr\u0001aphs, the overall space required for an adjacency list is $O ( V + E )$ . \nThere are several different ways to represent these neighbor lists, but the standard implementation uses a simple singly-linked list. The resulting data structure allows us to list the (out-)neighbors of a node $nu$ in $O ( 1 + deg ( nu ) )$ time; just scan v’s neighbor list. Similarly, we can determine whether $u {  } nu$ is an edge in $O ( 1 + deg ( u ) )$ time by scanning the neighbor list of $u$ . For undir\u0001ected graphs, we can improve the time to $O ( 1 + operatorname* { m i n } { deg ( u ) , deg ( nu ) } )$ by simultaneously scanning the neighbor lists of both $u$ and $nu$ , stopping either when we locate the edge or when we fall of the end of a list. \nOf course, linked lists are not the only data structure we could use; any other structure that supports searching, listing, insertion, and deletion will do. For example, we can reduce the time to determine whether uv is an edge to $O ( 1 + log ( deg ( u ) ) )$ by using a balanced binary search tree to store the neighbors of $u$ , or even to $O ( 1 )$ time by using an appropriately constructed hash table.7 \nOne common implementation of adjacency lists is the adjacency array, which uses a single array to store all edge records, with the records of edges incident to each vertex in a contiguous interval, and with a separate array storing the index of the first edge incident to each vertex. Moreover, it is useful to keep the intervals for each vertex in sorted order, as shown in Figure 5.10, so that we can check in $O ( log mathrm { d e g } ( u ) )$ time whether two vertices $u$ and $nu$ are adjacent. \nAdjacency Matrices \nThe other standard data structure for graphs is the adjacency matrix,8 first proposed by Georges Brunel in $1 8 9 4$ . The adjacency matrix of a graph $G$ is a $V times V$ matrix of 0s and 1s, normally represented by a two-dimensional array $A [ 1 ldots V , 1 ldots V ]$ , where each entry indicates whether a particular edge is present in $G$ . Specifically, for all vertices $u$ and $nu$ : \n• if the graph is undirected, then $A [ u , nu ] : = 1$ if and only if $u nu in E$ , and • if the graph is directed, then $A [ u , nu ] : = 1$ if and only if $u {  } nu in E$ .",
      "metadata": {
        "content": "Finally, sometimes one graph can be used to implicitly represent other larger graphs. A good example of this implicit representation is the subset construction, which is normally used to convert NFAs into DFAs, but can be applied to arbitrary directed graphs as follows. Given any directed graph $G = left( V , E right)$ , we can define a new directed graph $G ^ { prime } = ( 2 ^ { V } , E ^ { prime } )$ whose vertices are all subsets of vertices in $V$ , and whose edges $E ^ { prime }$ are defined as follows: \nWe can mechanically translate this definition into an algorithm to construct $G ^ { prime }$ from $G$ , but strictly speaking, this construction is unnecessary, because $G$ is already an implicit representation of $G ^ { prime }$ . \nIt’s important not to confuse any of these examples/representations with the actual formal definition: A graph is a pair of sets $( V , E )$ , where $V$ is an arbitrary non-empty finite set, and $E$ is a set of pairs (either ordered or unordered) of elements of $V$ . In short: A graph is a set of pairs of things. \n5.4 Data Structures \nIn practice, graphs are usually represented by one of two standard data structures: adjacency lists and adjacency matrices. At a high level, both data structures are arrays indexed by vertices; this requires that each vertex has a unique integer identifier between 1 and $V$ . In a formal sense, these integers are the vertices. \nAdjacency Lists \nBy far the most common data structure for storing graphs is the adjacency list. An adjacency list is an array of lists, each containing the neighbors of one of the vertices (or the out-neighbors if the graph is directed).6 For undirected graphs, each edge $u nu$ is stored twice, once in u’s neighbor list and once in v’s neighbor list; for directed graphs, each edge $u {  } nu$ is stored only once, in the neighbor list of the tail $u$ . For both types of gr\u0001aphs, the overall space required for an adjacency list is $O ( V + E )$ . \nThere are several different ways to represent these neighbor lists, but the standard implementation uses a simple singly-linked list. The resulting data structure allows us to list the (out-)neighbors of a node $nu$ in $O ( 1 + deg ( nu ) )$ time; just scan v’s neighbor list. Similarly, we can determine whether $u {  } nu$ is an edge in $O ( 1 + deg ( u ) )$ time by scanning the neighbor list of $u$ . For undir\u0001ected graphs, we can improve the time to $O ( 1 + operatorname* { m i n } { deg ( u ) , deg ( nu ) } )$ by simultaneously scanning the neighbor lists of both $u$ and $nu$ , stopping either when we locate the edge or when we fall of the end of a list. \nOf course, linked lists are not the only data structure we could use; any other structure that supports searching, listing, insertion, and deletion will do. For example, we can reduce the time to determine whether uv is an edge to $O ( 1 + log ( deg ( u ) ) )$ by using a balanced binary search tree to store the neighbors of $u$ , or even to $O ( 1 )$ time by using an appropriately constructed hash table.7 \nOne common implementation of adjacency lists is the adjacency array, which uses a single array to store all edge records, with the records of edges incident to each vertex in a contiguous interval, and with a separate array storing the index of the first edge incident to each vertex. Moreover, it is useful to keep the intervals for each vertex in sorted order, as shown in Figure 5.10, so that we can check in $O ( log mathrm { d e g } ( u ) )$ time whether two vertices $u$ and $nu$ are adjacent. \nAdjacency Matrices \nThe other standard data structure for graphs is the adjacency matrix,8 first proposed by Georges Brunel in $1 8 9 4$ . The adjacency matrix of a graph $G$ is a $V times V$ matrix of 0s and 1s, normally represented by a two-dimensional array $A [ 1 ldots V , 1 ldots V ]$ , where each entry indicates whether a particular edge is present in $G$ . Specifically, for all vertices $u$ and $nu$ : \n• if the graph is undirected, then $A [ u , nu ] : = 1$ if and only if $u nu in E$ , and • if the graph is directed, then $A [ u , nu ] : = 1$ if and only if $u {  } nu in E$ .",
        "chapter": "Basic Graph Algorithms",
        "section": "Data Structures",
        "subsection": "Adjacency Lists",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 70
      }
    },
    {
      "node_id": "tb1_node71",
      "content": "There are several different ways to represent these neighbor lists, but the standard implementation uses a simple singly-linked list. The resulting data structure allows us to list the (out-)neighbors of a node $nu$ in $O ( 1 + deg ( nu ) )$ time; just scan v’s neighbor list. Similarly, we can determine whether $u {  } nu$ is an edge in $O ( 1 + deg ( u ) )$ time by scanning the neighbor list of $u$ . For undir\u0001ected graphs, we can improve the time to $O ( 1 + operatorname* { m i n } { deg ( u ) , deg ( nu ) } )$ by simultaneously scanning the neighbor lists of both $u$ and $nu$ , stopping either when we locate the edge or when we fall of the end of a list. \nOf course, linked lists are not the only data structure we could use; any other structure that supports searching, listing, insertion, and deletion will do. For example, we can reduce the time to determine whether uv is an edge to $O ( 1 + log ( deg ( u ) ) )$ by using a balanced binary search tree to store the neighbors of $u$ , or even to $O ( 1 )$ time by using an appropriately constructed hash table.7 \nOne common implementation of adjacency lists is the adjacency array, which uses a single array to store all edge records, with the records of edges incident to each vertex in a contiguous interval, and with a separate array storing the index of the first edge incident to each vertex. Moreover, it is useful to keep the intervals for each vertex in sorted order, as shown in Figure 5.10, so that we can check in $O ( log mathrm { d e g } ( u ) )$ time whether two vertices $u$ and $nu$ are adjacent. \nAdjacency Matrices \nThe other standard data structure for graphs is the adjacency matrix,8 first proposed by Georges Brunel in $1 8 9 4$ . The adjacency matrix of a graph $G$ is a $V times V$ matrix of 0s and 1s, normally represented by a two-dimensional array $A [ 1 ldots V , 1 ldots V ]$ , where each entry indicates whether a particular edge is present in $G$ . Specifically, for all vertices $u$ and $nu$ : \n• if the graph is undirected, then $A [ u , nu ] : = 1$ if and only if $u nu in E$ , and • if the graph is directed, then $A [ u , nu ] : = 1$ if and only if $u {  } nu in E$ . \n\nFor undirected graphs, the adjacency matrix is always symmetric, meaning $A [ u , nu ] = A [ nu , u ]$ for all vertices $u$ and $nu$ , because $u nu$ and $nu u$ are just different names for the same edge, and the diagonal entries $A [ u , u ]$ are all zeros. For directed graphs, the adjacency matrix may or may not be symmetric, and the diagonal entries may or may not be zero. \nGiven an adjacency matrix, we can decide in $Theta ( 1 )$ time whether two vertices are connected by an edge just by looking in the appropriate slot in the matrix. We can also list all the neighbors of a vertex in $Theta ( V )$ time by scanning the corresponding row (or column). This running time is optimal in the worst case, but even if a vertex has few neighbors, we still have to scan the entire row to find them all. Similarly, adjacency matrices require $Theta ( V ^ { 2 } )$ space, regardless of how many edges the graph actually has, so they are only space-efficient for very dense graphs. \nComparison \nTable 5.1 summarizes the performance of the various standard graph data structures. Stars∗ indicate expected amortized time bounds for maintaining dynamic hash tables.9 \nIn light of this comparison, one might reasonably wonder why anyone would ever use an adjacency matrix; after all, adjacency lists with hash tables support the same operations in the same time, using less space. The main reason is that for sufficiently dense graphs, adjacency matrices are simpler and more efficient in practice, because they avoid the overhead of chasing pointers and computing hash functions; they’re just contiguous blocks of memory. \nSimilarly, why would anyone use linked lists in an adjacency list structure to store neighbors, instead of balanced binary search trees or hash tables? Although the primary reason in practice is almost surely tradition—If they were good enough for Donald Knuth’s code, they should be good enough for yours!—there are more principled arguments. One is that standard adjacency lists are in fact good enough for most applications. Most standard graph algorithms never (or rarely) actually ask whether an arbitrary edge is present or absent, or attempt to insert or delete edges, and so optimizing the data structures to support those operations is unnecessary. \nBut in my opinion, the most compelling reason for both standard data structures is that many graphs are implicitly represented by adjacency matrices and standard adjacency lists. For example: \n• Intersection graphs are usually represented as a list of the underlying geometric objects. As long as we can test whether two objects intersect in constant time, we can apply any graph algorithm to an intersection graph by pretending that the input graph is stored explicitly as an adjacency matrix. • Any data structure composed from records with pointers between them can be seen as a directed graph. Graph algorithms can be applied to these data structures by pretending that the graph is stored in a standard adjacency list.",
      "metadata": {
        "content": "There are several different ways to represent these neighbor lists, but the standard implementation uses a simple singly-linked list. The resulting data structure allows us to list the (out-)neighbors of a node $nu$ in $O ( 1 + deg ( nu ) )$ time; just scan v’s neighbor list. Similarly, we can determine whether $u {  } nu$ is an edge in $O ( 1 + deg ( u ) )$ time by scanning the neighbor list of $u$ . For undir\u0001ected graphs, we can improve the time to $O ( 1 + operatorname* { m i n } { deg ( u ) , deg ( nu ) } )$ by simultaneously scanning the neighbor lists of both $u$ and $nu$ , stopping either when we locate the edge or when we fall of the end of a list. \nOf course, linked lists are not the only data structure we could use; any other structure that supports searching, listing, insertion, and deletion will do. For example, we can reduce the time to determine whether uv is an edge to $O ( 1 + log ( deg ( u ) ) )$ by using a balanced binary search tree to store the neighbors of $u$ , or even to $O ( 1 )$ time by using an appropriately constructed hash table.7 \nOne common implementation of adjacency lists is the adjacency array, which uses a single array to store all edge records, with the records of edges incident to each vertex in a contiguous interval, and with a separate array storing the index of the first edge incident to each vertex. Moreover, it is useful to keep the intervals for each vertex in sorted order, as shown in Figure 5.10, so that we can check in $O ( log mathrm { d e g } ( u ) )$ time whether two vertices $u$ and $nu$ are adjacent. \nAdjacency Matrices \nThe other standard data structure for graphs is the adjacency matrix,8 first proposed by Georges Brunel in $1 8 9 4$ . The adjacency matrix of a graph $G$ is a $V times V$ matrix of 0s and 1s, normally represented by a two-dimensional array $A [ 1 ldots V , 1 ldots V ]$ , where each entry indicates whether a particular edge is present in $G$ . Specifically, for all vertices $u$ and $nu$ : \n• if the graph is undirected, then $A [ u , nu ] : = 1$ if and only if $u nu in E$ , and • if the graph is directed, then $A [ u , nu ] : = 1$ if and only if $u {  } nu in E$ . \n\nFor undirected graphs, the adjacency matrix is always symmetric, meaning $A [ u , nu ] = A [ nu , u ]$ for all vertices $u$ and $nu$ , because $u nu$ and $nu u$ are just different names for the same edge, and the diagonal entries $A [ u , u ]$ are all zeros. For directed graphs, the adjacency matrix may or may not be symmetric, and the diagonal entries may or may not be zero. \nGiven an adjacency matrix, we can decide in $Theta ( 1 )$ time whether two vertices are connected by an edge just by looking in the appropriate slot in the matrix. We can also list all the neighbors of a vertex in $Theta ( V )$ time by scanning the corresponding row (or column). This running time is optimal in the worst case, but even if a vertex has few neighbors, we still have to scan the entire row to find them all. Similarly, adjacency matrices require $Theta ( V ^ { 2 } )$ space, regardless of how many edges the graph actually has, so they are only space-efficient for very dense graphs. \nComparison \nTable 5.1 summarizes the performance of the various standard graph data structures. Stars∗ indicate expected amortized time bounds for maintaining dynamic hash tables.9 \nIn light of this comparison, one might reasonably wonder why anyone would ever use an adjacency matrix; after all, adjacency lists with hash tables support the same operations in the same time, using less space. The main reason is that for sufficiently dense graphs, adjacency matrices are simpler and more efficient in practice, because they avoid the overhead of chasing pointers and computing hash functions; they’re just contiguous blocks of memory. \nSimilarly, why would anyone use linked lists in an adjacency list structure to store neighbors, instead of balanced binary search trees or hash tables? Although the primary reason in practice is almost surely tradition—If they were good enough for Donald Knuth’s code, they should be good enough for yours!—there are more principled arguments. One is that standard adjacency lists are in fact good enough for most applications. Most standard graph algorithms never (or rarely) actually ask whether an arbitrary edge is present or absent, or attempt to insert or delete edges, and so optimizing the data structures to support those operations is unnecessary. \nBut in my opinion, the most compelling reason for both standard data structures is that many graphs are implicitly represented by adjacency matrices and standard adjacency lists. For example: \n• Intersection graphs are usually represented as a list of the underlying geometric objects. As long as we can test whether two objects intersect in constant time, we can apply any graph algorithm to an intersection graph by pretending that the input graph is stored explicitly as an adjacency matrix. • Any data structure composed from records with pointers between them can be seen as a directed graph. Graph algorithms can be applied to these data structures by pretending that the graph is stored in a standard adjacency list.",
        "chapter": "Basic Graph Algorithms",
        "section": "Data Structures",
        "subsection": "Adjacency Matrices",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 71
      }
    },
    {
      "node_id": "tb1_node72",
      "content": "Comparison \nTable 5.1 summarizes the performance of the various standard graph data structures. Stars∗ indicate expected amortized time bounds for maintaining dynamic hash tables.9 \nIn light of this comparison, one might reasonably wonder why anyone would ever use an adjacency matrix; after all, adjacency lists with hash tables support the same operations in the same time, using less space. The main reason is that for sufficiently dense graphs, adjacency matrices are simpler and more efficient in practice, because they avoid the overhead of chasing pointers and computing hash functions; they’re just contiguous blocks of memory. \nSimilarly, why would anyone use linked lists in an adjacency list structure to store neighbors, instead of balanced binary search trees or hash tables? Although the primary reason in practice is almost surely tradition—If they were good enough for Donald Knuth’s code, they should be good enough for yours!—there are more principled arguments. One is that standard adjacency lists are in fact good enough for most applications. Most standard graph algorithms never (or rarely) actually ask whether an arbitrary edge is present or absent, or attempt to insert or delete edges, and so optimizing the data structures to support those operations is unnecessary. \nBut in my opinion, the most compelling reason for both standard data structures is that many graphs are implicitly represented by adjacency matrices and standard adjacency lists. For example: \n• Intersection graphs are usually represented as a list of the underlying geometric objects. As long as we can test whether two objects intersect in constant time, we can apply any graph algorithm to an intersection graph by pretending that the input graph is stored explicitly as an adjacency matrix. • Any data structure composed from records with pointers between them can be seen as a directed graph. Graph algorithms can be applied to these data structures by pretending that the graph is stored in a standard adjacency list. \n• Similarly, we can apply any graph algorithm to a configuration graph as though it were represented as a standard adjacency list, provided we can enumerate all possible moves from a given configuration in constant time each. \nFor the last two examples, we can enumerate the edges leaving any vertex in time proportional to its degree, but we cannot necessarily determine in constant time if two vertices are adjacent. (Is there a pointer from this record to that record? Can we get from this configuration to that configuration in one move?) Moreover, we usually don’t have the luxury of reorganizing the pointers in each record or the moves out of a given configuration into a more efficient data structure. Thus, a standard adjacency list, with neighbors stored in linked lists, is the appropriate model data structure. \nIn the rest of this book, unless explicitly stated otherwise, all time bounds for graph algorithms assume that the input graph is represented by a standard adjacency list. Similarly, unless explicitly stated otherwise, when an exercise asks you to design and analyze a graph algorithm, you should assume that the input graph is represented in a standard adjacency list. \n5.5 Whatever-First Search \nSo far we have only discussed local operations on graphs; arguably the most fundamental global question we can ask about graphs is reachability. Given a graph $G$ and a vertex s in $G$ , the reachability question asks which vertices are reachable from $s$ ; that is, for which vertices $nu$ is there a path from $s$ to $nu ?$ For now, let’s consider only undirected graphs; I’ll consider directed graphs briefly at the end of this section. For undirected graphs, the vertices reachable from s are precisely the vertices in the same component as $s$ . \nPerhaps the most natural reachability algorithm—at least for people like us who are used to thinking recursively—is depth-first search. This algorithm can be written either recursively or iteratively. It’s exactly the same algorithm either way; the only difference is that we can actually see the “recursion” stack in the non-recursive version.",
      "metadata": {
        "content": "Comparison \nTable 5.1 summarizes the performance of the various standard graph data structures. Stars∗ indicate expected amortized time bounds for maintaining dynamic hash tables.9 \nIn light of this comparison, one might reasonably wonder why anyone would ever use an adjacency matrix; after all, adjacency lists with hash tables support the same operations in the same time, using less space. The main reason is that for sufficiently dense graphs, adjacency matrices are simpler and more efficient in practice, because they avoid the overhead of chasing pointers and computing hash functions; they’re just contiguous blocks of memory. \nSimilarly, why would anyone use linked lists in an adjacency list structure to store neighbors, instead of balanced binary search trees or hash tables? Although the primary reason in practice is almost surely tradition—If they were good enough for Donald Knuth’s code, they should be good enough for yours!—there are more principled arguments. One is that standard adjacency lists are in fact good enough for most applications. Most standard graph algorithms never (or rarely) actually ask whether an arbitrary edge is present or absent, or attempt to insert or delete edges, and so optimizing the data structures to support those operations is unnecessary. \nBut in my opinion, the most compelling reason for both standard data structures is that many graphs are implicitly represented by adjacency matrices and standard adjacency lists. For example: \n• Intersection graphs are usually represented as a list of the underlying geometric objects. As long as we can test whether two objects intersect in constant time, we can apply any graph algorithm to an intersection graph by pretending that the input graph is stored explicitly as an adjacency matrix. • Any data structure composed from records with pointers between them can be seen as a directed graph. Graph algorithms can be applied to these data structures by pretending that the graph is stored in a standard adjacency list. \n• Similarly, we can apply any graph algorithm to a configuration graph as though it were represented as a standard adjacency list, provided we can enumerate all possible moves from a given configuration in constant time each. \nFor the last two examples, we can enumerate the edges leaving any vertex in time proportional to its degree, but we cannot necessarily determine in constant time if two vertices are adjacent. (Is there a pointer from this record to that record? Can we get from this configuration to that configuration in one move?) Moreover, we usually don’t have the luxury of reorganizing the pointers in each record or the moves out of a given configuration into a more efficient data structure. Thus, a standard adjacency list, with neighbors stored in linked lists, is the appropriate model data structure. \nIn the rest of this book, unless explicitly stated otherwise, all time bounds for graph algorithms assume that the input graph is represented by a standard adjacency list. Similarly, unless explicitly stated otherwise, when an exercise asks you to design and analyze a graph algorithm, you should assume that the input graph is represented in a standard adjacency list. \n5.5 Whatever-First Search \nSo far we have only discussed local operations on graphs; arguably the most fundamental global question we can ask about graphs is reachability. Given a graph $G$ and a vertex s in $G$ , the reachability question asks which vertices are reachable from $s$ ; that is, for which vertices $nu$ is there a path from $s$ to $nu ?$ For now, let’s consider only undirected graphs; I’ll consider directed graphs briefly at the end of this section. For undirected graphs, the vertices reachable from s are precisely the vertices in the same component as $s$ . \nPerhaps the most natural reachability algorithm—at least for people like us who are used to thinking recursively—is depth-first search. This algorithm can be written either recursively or iteratively. It’s exactly the same algorithm either way; the only difference is that we can actually see the “recursion” stack in the non-recursive version.",
        "chapter": "Basic Graph Algorithms",
        "section": "Data Structures",
        "subsection": "Comparison",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 72
      }
    },
    {
      "node_id": "tb2_node12",
      "content": "PROTEIN DESIGN \nundoing some of the work already done. Checking a step in the sequence for feasibility is a difficult geometrical search problem closely related to robot navigation. Thus, the generation of legal actions is the expensive part of assembly sequencing. Any practical algorithm must avoid exploring all but a tiny fraction of the state space. Another important assembly problem is protein design, in which the goal is to find a sequence of amino acids that will fold into a three-dimensional protein with the right properties to cure some disease. \n3.3 SEARCHING FOR SOLUTIONS \nSEARCH TREE NODE \nHaving formulated some problems, we now need to solve them. A solution is an action sequence, so search algorithms work by considering various possible action sequences. The possible action sequences starting at the initial state form a search tree with the initial state at the root; the branches are actions and the nodes correspond to states in the state space of the problem. Figure 3.6 shows the first few steps in growing the search tree for finding a route from Arad to Bucharest. The root node of the tree corresponds to the initial state, $I n ( A r a d )$ . The first step is to test whether this is a goal state. (Clearly it is not, but it is important to check so that we can solve trick problems like “starting in Arad, get to Arad.”) Then we need to consider taking various actions. We do this by expanding the current state; that is, applying each legal action to the current state, thereby generating a new set of states. In this case, we add three branches from the parent node In(Arad) leading to three new child nodes: In(Sibiu), In(Timisoara), and In(Zerind). Now we must choose which of these three possibilities to consider further. \nEXPANDING GENERATING PARENT NODE CHILD NODE \nLEAF NODE \nFRONTIER OPEN LIST \nThis is the essence of search—following up one option now and putting the others aside for later, in case the first choice does not lead to a solution. Suppose we choose Sibiu first. We check to see whether it is a goal state (it is not) and then expand it to get $I n ( A r a d )$ In(Fagaras), In(Oradea), and In(RimnicuVilcea). We can then choose any of these four or go back and choose Timisoara or Zerind. Each of these six nodes is a leaf node, that is, a node with no children in the tree. The set of all leaf nodes available for expansion at any given point is called the frontier. (Many authors call it the open list, which is both geographically less evocative and less accurate, because other data structures are better suited than a list.) In Figure 3.6, the frontier of each tree consists of those nodes with bold outlines. \nSEARCH STRATEGY \nThe process of expanding nodes on the frontier continues until either a solution is found or there are no more states to expand. The general TREE-SEARCH algorithm is shown informally in Figure 3.7. Search algorithms all share this basic structure; they vary primarily according to how they choose which state to expand next—the so-called search strategy. \nREPEATED STATE LOOPY PATH \nThe eagle-eyed reader will notice one peculiar thing about the search tree shown in Figure 3.6: it includes the path from Arad to Sibiu and back to Arad again! We say that $I n ( A r a d )$ is a repeated state in the search tree, generated in this case by a loopy path. Considering such loopy paths means that the complete search tree for Romania is infinite because there is no limit to how often one can traverse a loop. On the other hand, the state space—the map shown in Figure 3.2—has only 20 states. As we discuss in Section 3.4, loops can cause certain algorithms to fail, making otherwise solvable problems unsolvable. Fortunately, there is no need to consider loopy paths. We can rely on more than intuition for this: because path costs are additive and step costs are nonnegative, a loopy path to any given state is never better than the same path with the loop removed. \n\nREDUNDANT PATH \nLoopy paths are a special case of the more general concept of redundant paths, which exist whenever there is more than one way to get from one state to another. Consider the paths Arad–Sibiu ( $mathrm { 1 4 0 ~ k m }$ long) and Arad–Zerind–Oradea–Sibiu ( $2 9 7  mathrm { k m }$ long). Obviously, the second path is redundant—it’s just a worse way to get to the same state. If you are concerned about reaching the goal, there’s never any reason to keep more than one path to any given state, because any goal state that is reachable by extending one path is also reachable by extending the other. \nIn some cases, it is possible to define the problem itself so as to eliminate redundant paths. For example, if we formulate the 8-queens problem (page 71) so that a queen can be placed in any column, then each state with $n$ queens can be reached by $n !$ different paths; but if we reformulate the problem so that each new queen is placed in the leftmost empty column, then each state can be reached only through one path. \nfunction TREE-SEARCH(problem) returns a solution, or failure initialize the frontier using the initial state of problem loop do if the frontier is empty then return failure choose a leaf node and remove it from the frontier if the node contains a goal state then return the corresponding solution expand the chosen node, adding the resulting nodes to the frontier   \nfunction GRAPH-SEARCH(problem) returns a solution, or failure initialize the frontier using the initial state of problem initialize the explored set to be empty loop do if the frontier is empty then return failure choose a leaf node and remove it from the frontier if the node contains a goal state then return the corresponding solution add the node to the explored set expand the chosen node, adding the resulting nodes to the frontier only if not in the frontier or explored set \nRECTANGULAR GRID \nIn other cases, redundant paths are unavoidable. This includes all problems where the actions are reversible, such as route-finding problems and sliding-block puzzles. Routefinding on a rectangular grid (like the one used later for Figure 3.9) is a particularly important example in computer games. In such a grid, each state has four successors, so a search tree of depth $d$ that includes repeated states has $4 ^ { d }$ leaves; but there are only about $2 d ^ { 2 }$ distinct states within $d$ steps of any given state. For $d = 2 0$ , this means about a trillion nodes but only about 800 distinct states. Thus, following redundant paths can cause a tractable problem to become intractable. This is true even for algorithms that know how to avoid infinite loops. \nEXPLORED SET CLOSED LIST \nAs the saying goes, algorithms that forget their history are doomed to repeat it. The way to avoid exploring redundant paths is to remember where one has been. To do this, we augment the TREE-SEARCH algorithm with a data structure called the explored set (also known as the closed list), which remembers every expanded node. Newly generated nodes that match previously generated nodes—ones in the explored set or the frontier—can be discarded instead of being added to the frontier. The new algorithm, called GRAPH-SEARCH, is shown informally in Figure 3.7. The specific algorithms in this chapter draw on this general design. \nSEPARATOR \nClearly, the search tree constructed by the GRAPH-SEARCH algorithm contains at most one copy of each state, so we can think of it as growing a tree directly on the state-space graph, as shown in Figure 3.8. The algorithm has another nice property: the frontier separates the state-space graph into the explored region and the unexplored region, so that every path from the initial state to an unexplored state has to pass through a state in the frontier. (If this seems completely obvious, try Exercise 3.13 now.) This property is illustrated in Figure 3.9. As every step moves a state from the frontier into the explored region while moving some states from the unexplored region into the frontier, we see that the algorithm is systematically examining the states in the state space, one by one, until it finds a solution. \n\n3.3.1 Infrastructure for search algorithms \nSearch algorithms require a data structure to keep track of the search tree that is being constructed. For each node $n$ of the tree, we have a structure that contains four components: \n• $n$ .STATE: the state in the state space to which the node corresponds;   \n• $n$ .PARENT: the node in the search tree that generated this node;   \n• $n$ .ACTION: the action that was applied to the parent to generate the node;   \n• $n$ .PATH-COST: the cost, traditionally denoted by $g ( n )$ , of the path from the initial state to the node, as indicated by the parent pointers. \nGiven the components for a parent node, it is easy to see how to compute the necessary components for a child node. The function CHILD-NODE takes a parent node and an action and returns the resulting child node: \nfunction CHILD-NODE(problem, parent, action) returns a node return a node with STATE $mathbf { tau } = mathbf { tau }$ problem.RESULT(parent.STATE, action), PARENT $ c =$ parent, ACTION ${ mathrm { J } } = a c t i$ on, PATH-COST $mathbf { tau } = mathbf { tau }$ parent.PATH-COST $^ +$ problem.STEP-COST(parent.STATE, action) \nThe node data structure is depicted in Figure 3.10. Notice how the PARENT pointers string the nodes together into a tree structure. These pointers also allow the solution path to be extracted when a goal node is found; we use the SOLUTION function to return the sequence of actions obtained by following parent pointers back to the root. \nUp to now, we have not been very careful to distinguish between nodes and states, but in writing detailed algorithms it’s important to make that distinction. A node is a bookkeeping data structure used to represent the search tree. A state corresponds to a configuration of the world. Thus, nodes are on particular paths, as defined by PARENT pointers, whereas states are not. Furthermore, two different nodes can contain the same world state if that state is generated via two different search paths. \nNow that we have nodes, we need somewhere to put them. The frontier needs to be stored in such a way that the search algorithm can easily choose the next node to expand according to its preferred strategy. The appropriate data structure for this is a queue. The operations on a queue are as follows: \n• EMPTY?(queue) returns true only if there are no more elements in the queue.   \n• POP(queue) removes the first element of the queue and returns it.   \n• INSERT(element, queue) inserts an element and returns the resulting queue. \nFIFO QUEUE LIFO QUEUE PRIORITY QUEUE \nQueues are characterized by the order in which they store the inserted nodes. Three common variants are the first-in, first-out or FIFO queue, which pops the oldest element of the queue; the last-in, first-out or LIFO queue (also known as a stack), which pops the newest element of the queue; and the priority queue, which pops the element of the queue with the highest priority according to some ordering function. \nCANONICAL FORM \nThe explored set can be implemented with a hash table to allow efficient checking for repeated states. With a good implementation, insertion and lookup can be done in roughly constant time no matter how many states are stored. One must take care to implement the hash table with the right notion of equality between states. For example, in the traveling salesperson problem (page 74), the hash table needs to know that the set of visited cities {Bucharest,Urziceni,Vaslui} is the same as {Urziceni,Vaslui,Bucharest}. Sometimes this can be achieved most easily by insisting that the data structures for states be in some canonical form; that is, logically equivalent states should map to the same data structure. In the case of states described by sets, for example, a bit-vector representation or a sorted list without repetition would be canonical, whereas an unsorted list would not. \n3.3.2 Measuring problem-solving performance \nBefore we get into the design of specific search algorithms, we need to consider the criteria that might be used to choose among them. We can evaluate an algorithm’s performance in four ways: \nCOMPLETENESSOPTIMALITYTIME COMPLEXITYSPACE COMPLEXITY\n• Completeness: Is the algorithm guaranteed to find a solution when there is one? • Optimality: Does the strategy find the optimal solution, as defined on page 68? • Time complexity: How long does it take to find a solution? • Space complexity: How much memory is needed to perform the search? \nBRANCHING FACTOR DEPTH \nTime and space complexity are always considered with respect to some measure of the problem difficulty. In theoretical computer science, the typical measure is the size of the state space graph, $vert V vert + vert E vert$ , where $V$ is the set of vertices (nodes) of the graph and $E$ is the set of edges (links). This is appropriate when the graph is an explicit data structure that is input to the search program. (The map of Romania is an example of this.) In AI, the graph is often represented implicitly by the initial state, actions, and transition model and is frequently infinite. For these reasons, complexity is expressed in terms of three quantities: $b$ , the branching factor or maximum number of successors of any node; $d$ , the depth of the shallowest goal node (i.e., the number of steps along the path from the root); and $m$ , the maximum length of any path in the state space. Time is often measured in terms of the number of nodes generated during the search, and space in terms of the maximum number of nodes stored in memory. For the most part, we describe time and space complexity for search on a tree; for a graph, the answer depends on how “redundant” the paths in the state space are. \nSEARCH COST \nTOTAL COST \nTo assess the effectiveness of a search algorithm, we can consider just the search cost— which typically depends on the time complexity but can also include a term for memory usage—or we can use the total cost, which combines the search cost and the path cost of the solution found. For the problem of finding a route from Arad to Bucharest, the search cost is the amount of time taken by the search and the solution cost is the total length of the path in kilometers. Thus, to compute the total cost, we have to add milliseconds and kilometers. There is no “official exchange rate” between the two, but it might be reasonable in this case to convert kilometers into milliseconds by using an estimate of the car’s average speed (because time is what the agent cares about). This enables the agent to find an optimal tradeoff point at which further computation to find a shorter path becomes counterproductive. The more general problem of tradeoffs between different goods is taken up in Chapter 16. \n\n3.4 UNINFORMED SEARCH STRATEGIES \nUNINFORMED SEARCH BLIND SEARCH \nINFORMED SEARCH HEURISTIC SEARCH \nBREADTH-FIRST SEARCH \nThis section covers several search strategies that come under the heading of uninformed search (also called blind search). The term means that the strategies have no additional information about states beyond that provided in the problem definition. All they can do is generate successors and distinguish a goal state from a non-goal state. All search strategies are distinguished by the order in which nodes are expanded. Strategies that know whether one non-goal state is “more promising” than another are called informed search or heuristic search strategies; they are covered in Section 3.5. \n3.4.1 Breadth-first search \nBreadth-first search is a simple strategy in which the root node is expanded first, then all the successors of the root node are expanded next, then their successors, and so on. In general, all the nodes are expanded at a given depth in the search tree before any nodes at the next level are expanded. \nBreadth-first search is an instance of the general graph-search algorithm (Figure 3.7) in which the shallowest unexpanded node is chosen for expansion. This is achieved very simply by using a FIFO queue for the frontier. Thus, new nodes (which are always deeper than their parents) go to the back of the queue, and old nodes, which are shallower than the new nodes, get expanded first. There is one slight tweak on the general graph-search algorithm, which is that the goal test is applied to each node when it is generated rather than when it is selected for expansion. This decision is explained below, where we discuss time complexity. Note also that the algorithm, following the general template for graph search, discards any new path to a state already in the frontier or explored set; it is easy to see that any such path must be at least as deep as the one already found. Thus, breadth-first search always has the shallowest path to every node on the frontier. \nPseudocode is given in Figure 3.11. Figure 3.12 shows the progress of the search on a simple binary tree. \nHow does breadth-first search rate according to the four criteria from the previous section? We can easily see that it is complete—if the shallowest goal node is at some finite depth $d$ , breadth-first search will eventually find it after generating all shallower nodes (provided the branching factor $b$ is finite). Note that as soon as a goal node is generated, we know it is the shallowest goal node because all shallower nodes must have been generated already and failed the goal test. Now, the shallowest goal node is not necessarily the optimal one;",
      "metadata": {
        "content": "PROTEIN DESIGN \nundoing some of the work already done. Checking a step in the sequence for feasibility is a difficult geometrical search problem closely related to robot navigation. Thus, the generation of legal actions is the expensive part of assembly sequencing. Any practical algorithm must avoid exploring all but a tiny fraction of the state space. Another important assembly problem is protein design, in which the goal is to find a sequence of amino acids that will fold into a three-dimensional protein with the right properties to cure some disease. \n3.3 SEARCHING FOR SOLUTIONS \nSEARCH TREE NODE \nHaving formulated some problems, we now need to solve them. A solution is an action sequence, so search algorithms work by considering various possible action sequences. The possible action sequences starting at the initial state form a search tree with the initial state at the root; the branches are actions and the nodes correspond to states in the state space of the problem. Figure 3.6 shows the first few steps in growing the search tree for finding a route from Arad to Bucharest. The root node of the tree corresponds to the initial state, $I n ( A r a d )$ . The first step is to test whether this is a goal state. (Clearly it is not, but it is important to check so that we can solve trick problems like “starting in Arad, get to Arad.”) Then we need to consider taking various actions. We do this by expanding the current state; that is, applying each legal action to the current state, thereby generating a new set of states. In this case, we add three branches from the parent node In(Arad) leading to three new child nodes: In(Sibiu), In(Timisoara), and In(Zerind). Now we must choose which of these three possibilities to consider further. \nEXPANDING GENERATING PARENT NODE CHILD NODE \nLEAF NODE \nFRONTIER OPEN LIST \nThis is the essence of search—following up one option now and putting the others aside for later, in case the first choice does not lead to a solution. Suppose we choose Sibiu first. We check to see whether it is a goal state (it is not) and then expand it to get $I n ( A r a d )$ In(Fagaras), In(Oradea), and In(RimnicuVilcea). We can then choose any of these four or go back and choose Timisoara or Zerind. Each of these six nodes is a leaf node, that is, a node with no children in the tree. The set of all leaf nodes available for expansion at any given point is called the frontier. (Many authors call it the open list, which is both geographically less evocative and less accurate, because other data structures are better suited than a list.) In Figure 3.6, the frontier of each tree consists of those nodes with bold outlines. \nSEARCH STRATEGY \nThe process of expanding nodes on the frontier continues until either a solution is found or there are no more states to expand. The general TREE-SEARCH algorithm is shown informally in Figure 3.7. Search algorithms all share this basic structure; they vary primarily according to how they choose which state to expand next—the so-called search strategy. \nREPEATED STATE LOOPY PATH \nThe eagle-eyed reader will notice one peculiar thing about the search tree shown in Figure 3.6: it includes the path from Arad to Sibiu and back to Arad again! We say that $I n ( A r a d )$ is a repeated state in the search tree, generated in this case by a loopy path. Considering such loopy paths means that the complete search tree for Romania is infinite because there is no limit to how often one can traverse a loop. On the other hand, the state space—the map shown in Figure 3.2—has only 20 states. As we discuss in Section 3.4, loops can cause certain algorithms to fail, making otherwise solvable problems unsolvable. Fortunately, there is no need to consider loopy paths. We can rely on more than intuition for this: because path costs are additive and step costs are nonnegative, a loopy path to any given state is never better than the same path with the loop removed. \n\nREDUNDANT PATH \nLoopy paths are a special case of the more general concept of redundant paths, which exist whenever there is more than one way to get from one state to another. Consider the paths Arad–Sibiu ( $mathrm { 1 4 0 ~ k m }$ long) and Arad–Zerind–Oradea–Sibiu ( $2 9 7  mathrm { k m }$ long). Obviously, the second path is redundant—it’s just a worse way to get to the same state. If you are concerned about reaching the goal, there’s never any reason to keep more than one path to any given state, because any goal state that is reachable by extending one path is also reachable by extending the other. \nIn some cases, it is possible to define the problem itself so as to eliminate redundant paths. For example, if we formulate the 8-queens problem (page 71) so that a queen can be placed in any column, then each state with $n$ queens can be reached by $n !$ different paths; but if we reformulate the problem so that each new queen is placed in the leftmost empty column, then each state can be reached only through one path. \nfunction TREE-SEARCH(problem) returns a solution, or failure initialize the frontier using the initial state of problem loop do if the frontier is empty then return failure choose a leaf node and remove it from the frontier if the node contains a goal state then return the corresponding solution expand the chosen node, adding the resulting nodes to the frontier   \nfunction GRAPH-SEARCH(problem) returns a solution, or failure initialize the frontier using the initial state of problem initialize the explored set to be empty loop do if the frontier is empty then return failure choose a leaf node and remove it from the frontier if the node contains a goal state then return the corresponding solution add the node to the explored set expand the chosen node, adding the resulting nodes to the frontier only if not in the frontier or explored set \nRECTANGULAR GRID \nIn other cases, redundant paths are unavoidable. This includes all problems where the actions are reversible, such as route-finding problems and sliding-block puzzles. Routefinding on a rectangular grid (like the one used later for Figure 3.9) is a particularly important example in computer games. In such a grid, each state has four successors, so a search tree of depth $d$ that includes repeated states has $4 ^ { d }$ leaves; but there are only about $2 d ^ { 2 }$ distinct states within $d$ steps of any given state. For $d = 2 0$ , this means about a trillion nodes but only about 800 distinct states. Thus, following redundant paths can cause a tractable problem to become intractable. This is true even for algorithms that know how to avoid infinite loops. \nEXPLORED SET CLOSED LIST \nAs the saying goes, algorithms that forget their history are doomed to repeat it. The way to avoid exploring redundant paths is to remember where one has been. To do this, we augment the TREE-SEARCH algorithm with a data structure called the explored set (also known as the closed list), which remembers every expanded node. Newly generated nodes that match previously generated nodes—ones in the explored set or the frontier—can be discarded instead of being added to the frontier. The new algorithm, called GRAPH-SEARCH, is shown informally in Figure 3.7. The specific algorithms in this chapter draw on this general design. \nSEPARATOR \nClearly, the search tree constructed by the GRAPH-SEARCH algorithm contains at most one copy of each state, so we can think of it as growing a tree directly on the state-space graph, as shown in Figure 3.8. The algorithm has another nice property: the frontier separates the state-space graph into the explored region and the unexplored region, so that every path from the initial state to an unexplored state has to pass through a state in the frontier. (If this seems completely obvious, try Exercise 3.13 now.) This property is illustrated in Figure 3.9. As every step moves a state from the frontier into the explored region while moving some states from the unexplored region into the frontier, we see that the algorithm is systematically examining the states in the state space, one by one, until it finds a solution. \n\n3.3.1 Infrastructure for search algorithms \nSearch algorithms require a data structure to keep track of the search tree that is being constructed. For each node $n$ of the tree, we have a structure that contains four components: \n• $n$ .STATE: the state in the state space to which the node corresponds;   \n• $n$ .PARENT: the node in the search tree that generated this node;   \n• $n$ .ACTION: the action that was applied to the parent to generate the node;   \n• $n$ .PATH-COST: the cost, traditionally denoted by $g ( n )$ , of the path from the initial state to the node, as indicated by the parent pointers. \nGiven the components for a parent node, it is easy to see how to compute the necessary components for a child node. The function CHILD-NODE takes a parent node and an action and returns the resulting child node: \nfunction CHILD-NODE(problem, parent, action) returns a node return a node with STATE $mathbf { tau } = mathbf { tau }$ problem.RESULT(parent.STATE, action), PARENT $ c =$ parent, ACTION ${ mathrm { J } } = a c t i$ on, PATH-COST $mathbf { tau } = mathbf { tau }$ parent.PATH-COST $^ +$ problem.STEP-COST(parent.STATE, action) \nThe node data structure is depicted in Figure 3.10. Notice how the PARENT pointers string the nodes together into a tree structure. These pointers also allow the solution path to be extracted when a goal node is found; we use the SOLUTION function to return the sequence of actions obtained by following parent pointers back to the root. \nUp to now, we have not been very careful to distinguish between nodes and states, but in writing detailed algorithms it’s important to make that distinction. A node is a bookkeeping data structure used to represent the search tree. A state corresponds to a configuration of the world. Thus, nodes are on particular paths, as defined by PARENT pointers, whereas states are not. Furthermore, two different nodes can contain the same world state if that state is generated via two different search paths. \nNow that we have nodes, we need somewhere to put them. The frontier needs to be stored in such a way that the search algorithm can easily choose the next node to expand according to its preferred strategy. The appropriate data structure for this is a queue. The operations on a queue are as follows: \n• EMPTY?(queue) returns true only if there are no more elements in the queue.   \n• POP(queue) removes the first element of the queue and returns it.   \n• INSERT(element, queue) inserts an element and returns the resulting queue. \nFIFO QUEUE LIFO QUEUE PRIORITY QUEUE \nQueues are characterized by the order in which they store the inserted nodes. Three common variants are the first-in, first-out or FIFO queue, which pops the oldest element of the queue; the last-in, first-out or LIFO queue (also known as a stack), which pops the newest element of the queue; and the priority queue, which pops the element of the queue with the highest priority according to some ordering function. \nCANONICAL FORM \nThe explored set can be implemented with a hash table to allow efficient checking for repeated states. With a good implementation, insertion and lookup can be done in roughly constant time no matter how many states are stored. One must take care to implement the hash table with the right notion of equality between states. For example, in the traveling salesperson problem (page 74), the hash table needs to know that the set of visited cities {Bucharest,Urziceni,Vaslui} is the same as {Urziceni,Vaslui,Bucharest}. Sometimes this can be achieved most easily by insisting that the data structures for states be in some canonical form; that is, logically equivalent states should map to the same data structure. In the case of states described by sets, for example, a bit-vector representation or a sorted list without repetition would be canonical, whereas an unsorted list would not. \n3.3.2 Measuring problem-solving performance \nBefore we get into the design of specific search algorithms, we need to consider the criteria that might be used to choose among them. We can evaluate an algorithm’s performance in four ways: \nCOMPLETENESSOPTIMALITYTIME COMPLEXITYSPACE COMPLEXITY\n• Completeness: Is the algorithm guaranteed to find a solution when there is one? • Optimality: Does the strategy find the optimal solution, as defined on page 68? • Time complexity: How long does it take to find a solution? • Space complexity: How much memory is needed to perform the search? \nBRANCHING FACTOR DEPTH \nTime and space complexity are always considered with respect to some measure of the problem difficulty. In theoretical computer science, the typical measure is the size of the state space graph, $vert V vert + vert E vert$ , where $V$ is the set of vertices (nodes) of the graph and $E$ is the set of edges (links). This is appropriate when the graph is an explicit data structure that is input to the search program. (The map of Romania is an example of this.) In AI, the graph is often represented implicitly by the initial state, actions, and transition model and is frequently infinite. For these reasons, complexity is expressed in terms of three quantities: $b$ , the branching factor or maximum number of successors of any node; $d$ , the depth of the shallowest goal node (i.e., the number of steps along the path from the root); and $m$ , the maximum length of any path in the state space. Time is often measured in terms of the number of nodes generated during the search, and space in terms of the maximum number of nodes stored in memory. For the most part, we describe time and space complexity for search on a tree; for a graph, the answer depends on how “redundant” the paths in the state space are. \nSEARCH COST \nTOTAL COST \nTo assess the effectiveness of a search algorithm, we can consider just the search cost— which typically depends on the time complexity but can also include a term for memory usage—or we can use the total cost, which combines the search cost and the path cost of the solution found. For the problem of finding a route from Arad to Bucharest, the search cost is the amount of time taken by the search and the solution cost is the total length of the path in kilometers. Thus, to compute the total cost, we have to add milliseconds and kilometers. There is no “official exchange rate” between the two, but it might be reasonable in this case to convert kilometers into milliseconds by using an estimate of the car’s average speed (because time is what the agent cares about). This enables the agent to find an optimal tradeoff point at which further computation to find a shorter path becomes counterproductive. The more general problem of tradeoffs between different goods is taken up in Chapter 16. \n\n3.4 UNINFORMED SEARCH STRATEGIES \nUNINFORMED SEARCH BLIND SEARCH \nINFORMED SEARCH HEURISTIC SEARCH \nBREADTH-FIRST SEARCH \nThis section covers several search strategies that come under the heading of uninformed search (also called blind search). The term means that the strategies have no additional information about states beyond that provided in the problem definition. All they can do is generate successors and distinguish a goal state from a non-goal state. All search strategies are distinguished by the order in which nodes are expanded. Strategies that know whether one non-goal state is “more promising” than another are called informed search or heuristic search strategies; they are covered in Section 3.5. \n3.4.1 Breadth-first search \nBreadth-first search is a simple strategy in which the root node is expanded first, then all the successors of the root node are expanded next, then their successors, and so on. In general, all the nodes are expanded at a given depth in the search tree before any nodes at the next level are expanded. \nBreadth-first search is an instance of the general graph-search algorithm (Figure 3.7) in which the shallowest unexpanded node is chosen for expansion. This is achieved very simply by using a FIFO queue for the frontier. Thus, new nodes (which are always deeper than their parents) go to the back of the queue, and old nodes, which are shallower than the new nodes, get expanded first. There is one slight tweak on the general graph-search algorithm, which is that the goal test is applied to each node when it is generated rather than when it is selected for expansion. This decision is explained below, where we discuss time complexity. Note also that the algorithm, following the general template for graph search, discards any new path to a state already in the frontier or explored set; it is easy to see that any such path must be at least as deep as the one already found. Thus, breadth-first search always has the shallowest path to every node on the frontier. \nPseudocode is given in Figure 3.11. Figure 3.12 shows the progress of the search on a simple binary tree. \nHow does breadth-first search rate according to the four criteria from the previous section? We can easily see that it is complete—if the shallowest goal node is at some finite depth $d$ , breadth-first search will eventually find it after generating all shallower nodes (provided the branching factor $b$ is finite). Note that as soon as a goal node is generated, we know it is the shallowest goal node because all shallower nodes must have been generated already and failed the goal test. Now, the shallowest goal node is not necessarily the optimal one;",
        "chapter": "II: Problem-solving",
        "section": "3 Solving Problems by Searching",
        "subsection": "3.3 Searching for Solutions",
        "subsubsection": "N/A",
        "textbook_id": 2,
        "node_index": 12
      }
    },
    {
      "node_id": "tb2_node13",
      "content": "3.4 UNINFORMED SEARCH STRATEGIES \nUNINFORMED SEARCH BLIND SEARCH \nINFORMED SEARCH HEURISTIC SEARCH \nBREADTH-FIRST SEARCH \nThis section covers several search strategies that come under the heading of uninformed search (also called blind search). The term means that the strategies have no additional information about states beyond that provided in the problem definition. All they can do is generate successors and distinguish a goal state from a non-goal state. All search strategies are distinguished by the order in which nodes are expanded. Strategies that know whether one non-goal state is “more promising” than another are called informed search or heuristic search strategies; they are covered in Section 3.5. \n3.4.1 Breadth-first search \nBreadth-first search is a simple strategy in which the root node is expanded first, then all the successors of the root node are expanded next, then their successors, and so on. In general, all the nodes are expanded at a given depth in the search tree before any nodes at the next level are expanded. \nBreadth-first search is an instance of the general graph-search algorithm (Figure 3.7) in which the shallowest unexpanded node is chosen for expansion. This is achieved very simply by using a FIFO queue for the frontier. Thus, new nodes (which are always deeper than their parents) go to the back of the queue, and old nodes, which are shallower than the new nodes, get expanded first. There is one slight tweak on the general graph-search algorithm, which is that the goal test is applied to each node when it is generated rather than when it is selected for expansion. This decision is explained below, where we discuss time complexity. Note also that the algorithm, following the general template for graph search, discards any new path to a state already in the frontier or explored set; it is easy to see that any such path must be at least as deep as the one already found. Thus, breadth-first search always has the shallowest path to every node on the frontier. \nPseudocode is given in Figure 3.11. Figure 3.12 shows the progress of the search on a simple binary tree. \nHow does breadth-first search rate according to the four criteria from the previous section? We can easily see that it is complete—if the shallowest goal node is at some finite depth $d$ , breadth-first search will eventually find it after generating all shallower nodes (provided the branching factor $b$ is finite). Note that as soon as a goal node is generated, we know it is the shallowest goal node because all shallower nodes must have been generated already and failed the goal test. Now, the shallowest goal node is not necessarily the optimal one; \nfunction BREADTH-FIRST-SEARCH(problem) returns a solution, or failure $n o d e gets mathbf { a }$ node with STATE $mathbf { tau } = mathbf { tau }$ problem.INITIAL-STATE, PATH- $mathrm { C O S T } = 0$ if problem.GOAL-TEST(node.STATE) then return SOLUTION(node) frontier $ mathbf { a }$ FIFO queue with node as the only element explored $$ an empty set loop do if EMPTY?( frontier) then return failure node $$ POP( frontier ) /* chooses the shallowest node in frontier */ add node.STATE to explored for each action in problem.ACTIONS(node.STATE) do child $$ CHILD-NODE(problem, node, action) if child .STATE is not in explored or frontier then if problem.GOAL-TEST(child.STATE) then return SOLUTION(child) frontier $$ INSERT(child , frontier ) \ntechnically, breadth-first search is optimal if the path cost is a nondecreasing function of the depth of the node. The most common such scenario is that all actions have the same cost. \nSo far, the news about breadth-first search has been good. The news about time and space is not so good. Imagine searching a uniform tree where every state has $b$ successors. The root of the search tree generates $b$ nodes at the first level, each of which generates $b$ more nodes, for a total of $b ^ { 2 }$ at the second level. Each of these generates $b$ more nodes, yielding $b ^ { 3 }$ nodes at the third level, and so on. Now suppose that the solution is at depth $d .$ In the worst case, it is the last node generated at that level. Then the total number of nodes generated is \n(If the algorithm were to apply the goal test to nodes when selected for expansion, rather than when generated, the whole layer of nodes at depth $d$ would be expanded before the goal was detected and the time complexity would be $O ( b ^ { d + 1 } )$ ) \nAs for space complexity: for any kind of graph search, which stores every expanded node in the explored set, the space complexity is always within a factor of $b$ of the time complexity. For breadth-first graph search in particular, every node generated remains in memory. There will be $O ( b ^ { d - 1 } )$ nodes in the explored set and $O ( b ^ { d } )$ nodes in the frontier, so the space complexity is $O ( b ^ { d } )$ , i.e., it is dominated by the size of the frontier. Switching to a tree search would not save much space, and in a state space with many redundant paths, switching could cost a great deal of time. \n\nAn exponential complexity bound such as $O ( b ^ { d } )$ is scary. Figure 3.13 shows why. It lists, for various values of the solution depth $d$ the time and memory required for a breadthfirst search with branching factor $b = 1 0$ . The table assumes that 1 million nodes can be generated per second and that a node requires 1000 bytes of storage. Many search problems fit roughly within these assumptions (give or take a factor of 100) when run on a modern personal computer. \nDepth Nodes Time Memory   \n2 110 .11 milliseconds 107 kilobytes   \n4 11,110 11 milliseconds 10.6 megabytes   \n6 106 1.1 seconds 1 gigabyte   \n8 108 2 minutes 103 gigabytes   \n10 10 3 hours 10 terabytes   \n12 1012 13 days 1 petabyte   \n14 1014 3.5 years 99 petabytes   \n16 1016 350 years 10 exabytes \nTwo lessons can be learned from Figure 3.13. First, the memory requirements are a bigger problem for breadth-first search than is the execution time. One might wait 13 days for the solution to an important problem with search depth 12, but no personal computer has the petabyte of memory it would take. Fortunately, other strategies require less memory. \n珍 哈 UNIFORM-COST SEARCH \nThe second lesson is that time is still a major factor. If your problem has a solution at depth 16, then (given our assumptions) it will take about 350 years for breadth-first search (or indeed any uninformed search) to find it. In general, exponential-complexity search problems cannot be solved by uninformed methods for any but the smallest instances. \n3.4.2 Uniform-cost search \nWhen all step costs are equal, breadth-first search is optimal because it always expands the shallowest unexpanded node. By a simple extension, we can find an algorithm that is optimal with any step-cost function. Instead of expanding the shallowest node, uniform-cost search expands the node $n$ with the lowest path cost $g ( n )$ . This is done by storing the frontier as a priority queue ordered by $g$ . The algorithm is shown in Figure 3.14. \nIn addition to the ordering of the queue by path cost, there are two other significant differences from breadth-first search. The first is that the goal test is applied to a node when it is selected for expansion (as in the generic graph-search algorithm shown in Figure 3.7) rather than when it is first generated. The reason is that the first goal node that is generated function UNIFORM-COST-SEARCH(problem) returns a solution, or failure $n o d e gets mathbf { a }$ node with STATE $mathbf { tau } = mathbf { tau }$ problem.INITIAL-STATE, PATH- $mathrm { C O S T } = 0$ frontier $ mathbf { a }$ priority queue ordered by PATH-COST, with node as the only element explored $$ an empty set loop do if EMPTY?( frontier) then return failure node ← POP( frontier ) /* chooses the lowest-cost node in frontier */ if problem.GOAL-TEST(node.STATE) then return SOLUTION(node) add node.STATE to explored for each action in problem.ACTIONS(node.STATE) do child $$ CHILD-NODE(problem, node, action) if child .STATE is not in explored or frontier then frontier $$ INSERT(child , frontier ) else if child .STATE is in frontier with higher PATH-COST then replace that frontier node with child \n\nmay be on a suboptimal path. The second difference is that a test is added in case a better path is found to a node currently on the frontier. \nBoth of these modifications come into play in the example shown in Figure 3.15, where the problem is to get from Sibiu to Bucharest. The successors of Sibiu are Rimnicu Vilcea and Fagaras, with costs 80 and 99, respectively. The least-cost node, Rimnicu Vilcea, is expanded next, adding Pitesti with cost $8 0 + 9 7 = 1 7 7$ . The least-cost node is now Fagaras, so it is expanded, adding Bucharest with cost $9 9 + 2 1 1 = 3 1 0 .$ Now a goal node has been generated, but uniform-cost search keeps going, choosing Pitesti for expansion and adding a second path to Bucharest with cost $8 0 + 9 7 + 1 0 1 = 2 7 8 .$ Now the algorithm checks to see if this new path is better than the old one; it is, so the old one is discarded. Bucharest, now with $g$ -cost 278, is selected for expansion and the solution is returned. \n\nIt is easy to see that uniform-cost search is optimal in general. First, we observe that whenever uniform-cost search selects a node $n$ for expansion, the optimal path to that node has been found. (Were this not the case, there would have to be another frontier node $n ^ { prime }$ on the optimal path from the start node to $n$ , by the graph separation property of Figure 3.9; by definition, $n ^ { prime }$ would have lower $g$ -cost than $n$ and would have been selected first.) Then, because step costs are nonnegative, paths never get shorter as nodes are added. These two facts together imply that uniform-cost search expands nodes in order of their optimal path cost. Hence, the first goal node selected for expansion must be the optimal solution. \nUniform-cost search does not care about the number of steps a path has, but only about their total cost. Therefore, it will get stuck in an infinite loop if there is a path with an infinite sequence of zero-cost actions—for example, a sequence of $N o O p$ actions.6 Completeness is guaranteed provided the cost of every step exceeds some small positive constant $epsilon$ . \nUniform-cost search is guided by path costs rather than depths, so its complexity is not easily characterized in terms of $b$ and $d$ . Instead, let $C ^ { * }$ be the cost of the optimal solution,7 and assume that every action costs at least $epsilon$ . Then the algorithm’s worst-case time and space complexity is $O ( b ^ { 1 + } dot { lfloor cal C ^ { * } / epsilon rfloor } )$ , which can be much greater than $b ^ { d }$ . This is because uniformcost search can explore large trees of small steps before exploring paths involving large and perhaps useful steps. When all step costs are equal, $b ^ { 1 + lfloor bar { C } ^ { * } / epsilon rfloor }$ is just $b ^ { d + 1 }$ . When all step costs are the same, uniform-cost search is similar to breadth-first search, except that the latter stops as soon as it generates a goal, whereas uniform-cost search examines all the nodes at the goal’s depth to see if one has a lower cost; thus uniform-cost search does strictly more work by expanding nodes at depth $d$ unnecessarily. \n3.4.3 Depth-first search \nDepth-first search always expands the deepest node in the current frontier of the search tree. The progress of the search is illustrated in Figure 3.16. The search proceeds immediately to the deepest level of the search tree, where the nodes have no successors. As those nodes are expanded, they are dropped from the frontier, so then the search “backs up” to the next deepest node that still has unexplored successors. \nThe depth-first search algorithm is an instance of the graph-search algorithm in Figure 3.7; whereas breadth-first-search uses a FIFO queue, depth-first search uses a LIFO queue. A LIFO queue means that the most recently generated node is chosen for expansion. This must be the deepest unexpanded node because it is one deeper than its parent—which, in turn, was the deepest unexpanded node when it was selected. \nAs an alternative to the GRAPH-SEARCH-style implementation, it is common to implement depth-first search with a recursive function that calls itself on each of its children in turn. (A recursive depth-first algorithm incorporating a depth limit is shown in Figure 3.17.) \nThe properties of depth-first search depend strongly on whether the graph-search or tree-search version is used. The graph-search version, which avoids repeated states and redundant paths, is complete in finite state spaces because it will eventually expand every node. The tree-search version, on the other hand, is not complete—for example, in Figure 3.6 the algorithm will follow the Arad–Sibiu–Arad–Sibiu loop forever. Depth-first tree search can be modified at no extra memory cost so that it checks new states against those on the path from the root to the current node; this avoids infinite loops in finite state spaces but does not avoid the proliferation of redundant paths. In infinite state spaces, both versions fail if an infinite non-goal path is encountered. For example, in Knuth’s 4 problem, depth-first search would keep applying the factorial operator forever. \nFor similar reasons, both versions are nonoptimal. For example, in Figure 3.16, depthfirst search will explore the entire left subtree even if node $C$ is a goal node. If node $J$ were also a goal node, then depth-first search would return it as a solution instead of $C$ , which would be a better solution; hence, depth-first search is not optimal. \nThe time complexity of depth-first graph search is bounded by the size of the state space (which may be infinite, of course). A depth-first tree search, on the other hand, may generate all of the $O ( b ^ { m } )$ nodes in the search tree, where $m$ is the maximum depth of any node; this can be much greater than the size of the state space. Note that $m$ itself can be much larger than $d$ (the depth of the shallowest solution) and is infinite if the tree is unbounded. \nSo far, depth-first search seems to have no clear advantage over breadth-first search, so why do we include it? The reason is the space complexity. For a graph search, there is no advantage, but a depth-first tree search needs to store only a single path from the root to a leaf node, along with the remaining unexpanded sibling nodes for each node on the path. Once a node has been expanded, it can be removed from memory as soon as all its descendants have been fully explored. (See Figure 3.16.) For a state space with branching factor $b$ and maximum depth $m$ , depth-first search requires storage of only $O ( b m )$ nodes. Using the same assumptions as for Figure 3.13 and assuming that nodes at the same depth as the goal node have no successors, we find that depth-first search would require 156 kilobytes instead of 10 exabytes at depth $d = 1 6$ , a factor of 7 trillion times less space. This has led to the adoption of depth-first tree search as the basic workhorse of many areas of AI, including constraint satisfaction (Chapter 6), propositional satisfiability (Chapter 7), and logic programming (Chapter 9). For the remainder of this section, we focus primarily on the treesearch version of depth-first search. \nA variant of depth-first search called backtracking search uses still less memory. (See Chapter 6 for more details.) In backtracking, only one successor is generated at a time rather than all successors; each partially expanded node remembers which successor to generate next. In this way, only $O ( m )$ memory is needed rather than $O ( b m )$ . Backtracking search facilitates yet another memory-saving (and time-saving) trick: the idea of generating a successor by modifying the current state description directly rather than copying it first. This reduces the memory requirements to just one state description and $O ( m )$ actions. For this to work, we must be able to undo each modification when we go back to generate the next successor. For problems with large state descriptions, such as robotic assembly, these techniques are critical to success. \n3.4.4 Depth-limited search \nThe embarrassing failure of depth-first search in infinite state spaces can be alleviated by supplying depth-first search with a predetermined depth limit $ell$ . That is, nodes at depth $ell$ are treated as if they have no successors. This approach is called depth-limited search. The depth limit solves the infinite-path problem. Unfortunately, it also introduces an additional source of incompleteness if we choose $ell < d$ , that is, the shallowest goal is beyond the depth limit. (This is likely when $d$ is unknown.) Depth-limited search will also be nonoptimal if we choose $ell > d$ . Its time complexity is $O ( b ^ { ell } )$ and its space complexity is $O ( b ell )$ . Depth-first search can be viewed as a special case of depth-limited search with $ell = infty$ . \nSometimes, depth limits can be based on knowledge of the problem. For example, on the map of Romania there are 20 cities. Therefore, we know that if there is a solution, it must be of length 19 at the longest, so $ell = 1 9$ is a possible choice. But in fact if we studied the \nfunction DEPTH-LIMITED-SEARCH(problem, limit ) returns a solution, or failure/cutoff return RECURSIVE-DLS(MAKE-NODE(problem.INITIAL-STATE), problem, limit)   \nfunction RECURSIVE-DLS(node, problem, limit ) returns a solution, or failure/cutoff if problem.GOAL-TEST(node.STATE) then return SOLUTION(node) else if limit $= 0$ then return cutoff else cutoff occurred? $$ false for each action in problem.ACTIONS(node.STATE) do child $$ CHILD-NODE(problem, node, action) result $$ RECURSIVE-DLS(child, problem, limit − 1) if result $mathbf { tau } = mathbf { tau }$ cutoff then cutoff occurred? $$ true else if result $neq$ failure then return result if cutoff occurred? then return cutoff else return failure \nDIAMETER \nmap carefully, we would discover that any city can be reached from any other city in at most 9 steps. This number, known as the diameter of the state space, gives us a better depth limit, which leads to a more efficient depth-limited search. For most problems, however, we will not know a good depth limit until we have solved the problem. \nDepth-limited search can be implemented as a simple modification to the general treeor graph-search algorithm. Alternatively, it can be implemented as a simple recursive algorithm as shown in Figure 3.17. Notice that depth-limited search can terminate with two kinds of failure: the standard failure value indicates no solution; the cutoff value indicates no solution within the depth limit. \nITERATIVE DEEPENING SEARCH \n3.4.5 Iterative deepening depth-first search \nIterative deepening search (or iterative deepening depth-first search) is a general strategy, often used in combination with depth-first tree search, that finds the best depth limit. It does this by gradually increasing the limit—first 0, then 1, then 2, and so on—until a goal is found. This will occur when the depth limit reaches $d$ , the depth of the shallowest goal node. The algorithm is shown in Figure 3.18. Iterative deepening combines the benefits of depth-first and breadth-first search. Like depth-first search, its memory requirements are modest: $O ( b d )$ to be precise. Like breadth-first search, it is complete when the branching factor is finite and optimal when the path cost is a nondecreasing function of the depth of the node. Figure 3.19 shows four iterations of ITERATIVE-DEEPENING-SEARCH on a binary search tree, where the solution is found on the fourth iteration. \nIterative deepening search may seem wasteful because states are generated multiple times. It turns out this is not too costly. The reason is that in a search tree with the same (or nearly the same) branching factor at each level, most of the nodes are in the bottom level, so it does not matter much that the upper levels are generated multiple times. In an iterative deepening search, the nodes on the bottom level (depth $d$ ) are generated once, those on the function ITERATIVE-DEEPENING-SEARCH(problem) returns a solution, or failure for depth $= 0$ to $infty$ do result $$ DEPTH-LIMITED-SEARCH(problem, depth) if result $neq$ cutoff then return result Figure 3.18 The iterative deepening search algorithm, which repeatedly applies depthlimited search with increasing limits. It terminates when a solution is found or if the depthlimited search returns failure, meaning that no solution exists. \n\nnext-to-bottom level are generated twice, and so on, up to the children of the root, which are generated $d$ times. So the total number of nodes generated in the worst case is \nwhich gives a time complexity of $O ( b ^ { d } ) .$ —asymptotically the same as breadth-first search. There is some extra cost for generating the upper levels multiple times, but it is not large. For example, if $b = 1 0$ and $d = 5$ , the numbers are \nIf you are really concerned about repeating the repetition, you can use a hybrid approach that runs breadth-first search until almost all the available memory is consumed, and then runs iterative deepening from all the nodes in the frontier. In general, iterative deepening is the preferred uninformed search method when the search space is large and the depth of the solution is not known. \nIterative deepening search is analogous to breadth-first search in that it explores a complete layer of new nodes at each iteration before going on to the next layer. It would seem worthwhile to develop an iterative analog to uniform-cost search, inheriting the latter algorithm’s optimality guarantees while avoiding its memory requirements. The idea is to use increasing path-cost limits instead of increasing depth limits. The resulting algorithm, called iterative lengthening search, is explored in Exercise 3.17. It turns out, unfortunately, that iterative lengthening incurs substantial overhead compared to uniform-cost search. \n3.4.6 Bidirectional search \nThe idea behind bidirectional search is to run two simultaneous searches—one forward from the initial state and the other backward from the goal—hoping that the two searches meet in the middle (Figure 3.20). The motivation is that $b ^ { d / 2 } + bar { b ^ { d / 2 } }$ is much less than $b ^ { d }$ , or in the figure, the area of the two small circles is less than the area of one big circle centered on the start and reaching to the goal. \nBidirectional search is implemented by replacing the goal test with a check to see whether the frontiers of the two searches intersect; if they do, a solution has been found. (It is important to realize that the first such solution found may not be optimal, even if the two searches are both breadth-first; some additional search is required to make sure there isn’t another short-cut across the gap.) The check can be done when each node is generated or selected for expansion and, with a hash table, will take constant time. For example, if a problem has solution depth $d = 6$ , and each direction runs breadth-first search one node at a time, then in the worst case the two searches meet when they have generated all of the nodes at depth 3. For $b = 1 0$ , this means a total of 2,220 node generations, compared with 1,111,110 for a standard breadth-first search. Thus, the time complexity of bidirectional search using breadth-first searches in both directions is $O ( b ^ { d / 2 } )$ . The space complexity is also $O ( b ^ { d / 2 } )$ . We can reduce this by roughly half if one of the two searches is done by iterative deepening, but at least one of the frontiers must be kept in memory so that the intersection check can be done. This space requirement is the most significant weakness of bidirectional search. \nThe reduction in time complexity makes bidirectional search attractive, but how do we search backward? This is not as easy as it sounds. Let the predecessors of a state $x$ be all those states that have $x$ as a successor. Bidirectional search requires a method for computing predecessors. When all the actions in the state space are reversible, the predecessors of $x$ are just its successors. Other cases may require substantial ingenuity. \nConsider the question of what we mean by “the goal” in searching “backward from the goal.” For the 8-puzzle and for finding a route in Romania, there is just one goal state, so the backward search is very much like the forward search. If there are several explicitly listed goal states—for example, the two dirt-free goal states in Figure 3.3—then we can construct a new dummy goal state whose immediate predecessors are all the actual goal states. But if the goal is an abstract description, such as the goal that “no queen attacks another queen” in the $n$ -queens problem, then bidirectional search is difficult to use. \n3.4.7 Comparing uninformed search strategies \nFigure 3.21 compares search strategies in terms of the four evaluation criteria set forth in Section 3.3.2. This comparison is for tree-search versions. For graph searches, the main differences are that depth-first search is complete for finite state spaces and that the space and time complexities are bounded by the size of the state space. \n3.5 INFORMED (HEURISTIC) SEARCH STRATEGIES \nINFORMED SEARCH \nThis section shows how an informed search strategy—one that uses problem-specific knowledge beyond the definition of the problem itself—can find solutions more efficiently than can an uninformed strategy. \nBEST-FIRST SEARCH \nEVALUATION FUNCTION \nThe general approach we consider is called best-first search. Best-first search is an instance of the general TREE-SEARCH or GRAPH-SEARCH algorithm in which a node is selected for expansion based on an evaluation function, $f ( n )$ . The evaluation function is construed as a cost estimate, so the node with the lowest evaluation is expanded first. The implementation of best-first graph search is identical to that for uniform-cost search (Figure 3.14), except for the use of $f$ instead of $g$ to order the priority queue. \nHEURISTIC FUNCTION \nThe choice of $f$ determines the search strategy. (For example, as Exercise 3.21 shows, best-first tree search includes depth-first search as a special case.) Most best-first algorithms include as a component of $f$ a heuristic function, denoted $h ( n )$ : \n$h ( n ) =$ estimated cost of the cheapest path from the state at node $n$ to a goal state. (Notice that $h ( n )$ takes a node as input, but, unlike $g ( n )$ , it depends only on the state at that node.) For example, in Romania, one might estimate the cost of the cheapest path from Arad to Bucharest via the straight-line distance from Arad to Bucharest. \nHeuristic functions are the most common form in which additional knowledge of the problem is imparted to the search algorithm. We study heuristics in more depth in Section 3.6. For now, we consider them to be arbitrary, nonnegative, problem-specific functions, with one constraint: if $n$ is a goal node, then $h ( n ) = 0$ . The remainder of this section covers two ways to use heuristic information to guide search. \n3.5.1 Greedy best-first search \nGREEDY BEST-FIRST SEARCH \nGreedy best-first search8 tries to expand the node that is closest to the goal, on the grounds that this is likely to lead to a solution quickly. Thus, it evaluates nodes by using just the heuristic function; that is, $f ( n ) = h ( n )$ . \nSTRAIGHT-LINEDISTANCE\nLet us see how this works for route-finding problems in Romania; we use the straightline distance heuristic, which we will call $h _ { S L D }$ . If the goal is Bucharest, we need to know the straight-line distances to Bucharest, which are shown in Figure 3.22. For example, $h _ { S L D } ( I n ( A r a d ) ) = 3 6 6$ . Notice that the values of $h _ { S L D }$ cannot be computed from the problem description itself. Moreover, it takes a certain amount of experience to know that $h _ { S L D }$ is correlated with actual road distances and is, therefore, a useful heuristic. \nFigure 3.23 shows the progress of a greedy best-first search using $h _ { S L D }$ to find a path from Arad to Bucharest. The first node to be expanded from Arad will be Sibiu because it is closer to Bucharest than either Zerind or Timisoara. The next node to be expanded will be Fagaras because it is closest. Fagaras in turn generates Bucharest, which is the goal. For this particular problem, greedy best-first search using $h _ { S L D }$ finds a solution without ever",
      "metadata": {
        "content": "3.4 UNINFORMED SEARCH STRATEGIES \nUNINFORMED SEARCH BLIND SEARCH \nINFORMED SEARCH HEURISTIC SEARCH \nBREADTH-FIRST SEARCH \nThis section covers several search strategies that come under the heading of uninformed search (also called blind search). The term means that the strategies have no additional information about states beyond that provided in the problem definition. All they can do is generate successors and distinguish a goal state from a non-goal state. All search strategies are distinguished by the order in which nodes are expanded. Strategies that know whether one non-goal state is “more promising” than another are called informed search or heuristic search strategies; they are covered in Section 3.5. \n3.4.1 Breadth-first search \nBreadth-first search is a simple strategy in which the root node is expanded first, then all the successors of the root node are expanded next, then their successors, and so on. In general, all the nodes are expanded at a given depth in the search tree before any nodes at the next level are expanded. \nBreadth-first search is an instance of the general graph-search algorithm (Figure 3.7) in which the shallowest unexpanded node is chosen for expansion. This is achieved very simply by using a FIFO queue for the frontier. Thus, new nodes (which are always deeper than their parents) go to the back of the queue, and old nodes, which are shallower than the new nodes, get expanded first. There is one slight tweak on the general graph-search algorithm, which is that the goal test is applied to each node when it is generated rather than when it is selected for expansion. This decision is explained below, where we discuss time complexity. Note also that the algorithm, following the general template for graph search, discards any new path to a state already in the frontier or explored set; it is easy to see that any such path must be at least as deep as the one already found. Thus, breadth-first search always has the shallowest path to every node on the frontier. \nPseudocode is given in Figure 3.11. Figure 3.12 shows the progress of the search on a simple binary tree. \nHow does breadth-first search rate according to the four criteria from the previous section? We can easily see that it is complete—if the shallowest goal node is at some finite depth $d$ , breadth-first search will eventually find it after generating all shallower nodes (provided the branching factor $b$ is finite). Note that as soon as a goal node is generated, we know it is the shallowest goal node because all shallower nodes must have been generated already and failed the goal test. Now, the shallowest goal node is not necessarily the optimal one; \nfunction BREADTH-FIRST-SEARCH(problem) returns a solution, or failure $n o d e gets mathbf { a }$ node with STATE $mathbf { tau } = mathbf { tau }$ problem.INITIAL-STATE, PATH- $mathrm { C O S T } = 0$ if problem.GOAL-TEST(node.STATE) then return SOLUTION(node) frontier $ mathbf { a }$ FIFO queue with node as the only element explored $$ an empty set loop do if EMPTY?( frontier) then return failure node $$ POP( frontier ) /* chooses the shallowest node in frontier */ add node.STATE to explored for each action in problem.ACTIONS(node.STATE) do child $$ CHILD-NODE(problem, node, action) if child .STATE is not in explored or frontier then if problem.GOAL-TEST(child.STATE) then return SOLUTION(child) frontier $$ INSERT(child , frontier ) \ntechnically, breadth-first search is optimal if the path cost is a nondecreasing function of the depth of the node. The most common such scenario is that all actions have the same cost. \nSo far, the news about breadth-first search has been good. The news about time and space is not so good. Imagine searching a uniform tree where every state has $b$ successors. The root of the search tree generates $b$ nodes at the first level, each of which generates $b$ more nodes, for a total of $b ^ { 2 }$ at the second level. Each of these generates $b$ more nodes, yielding $b ^ { 3 }$ nodes at the third level, and so on. Now suppose that the solution is at depth $d .$ In the worst case, it is the last node generated at that level. Then the total number of nodes generated is \n(If the algorithm were to apply the goal test to nodes when selected for expansion, rather than when generated, the whole layer of nodes at depth $d$ would be expanded before the goal was detected and the time complexity would be $O ( b ^ { d + 1 } )$ ) \nAs for space complexity: for any kind of graph search, which stores every expanded node in the explored set, the space complexity is always within a factor of $b$ of the time complexity. For breadth-first graph search in particular, every node generated remains in memory. There will be $O ( b ^ { d - 1 } )$ nodes in the explored set and $O ( b ^ { d } )$ nodes in the frontier, so the space complexity is $O ( b ^ { d } )$ , i.e., it is dominated by the size of the frontier. Switching to a tree search would not save much space, and in a state space with many redundant paths, switching could cost a great deal of time. \n\nAn exponential complexity bound such as $O ( b ^ { d } )$ is scary. Figure 3.13 shows why. It lists, for various values of the solution depth $d$ the time and memory required for a breadthfirst search with branching factor $b = 1 0$ . The table assumes that 1 million nodes can be generated per second and that a node requires 1000 bytes of storage. Many search problems fit roughly within these assumptions (give or take a factor of 100) when run on a modern personal computer. \nDepth Nodes Time Memory   \n2 110 .11 milliseconds 107 kilobytes   \n4 11,110 11 milliseconds 10.6 megabytes   \n6 106 1.1 seconds 1 gigabyte   \n8 108 2 minutes 103 gigabytes   \n10 10 3 hours 10 terabytes   \n12 1012 13 days 1 petabyte   \n14 1014 3.5 years 99 petabytes   \n16 1016 350 years 10 exabytes \nTwo lessons can be learned from Figure 3.13. First, the memory requirements are a bigger problem for breadth-first search than is the execution time. One might wait 13 days for the solution to an important problem with search depth 12, but no personal computer has the petabyte of memory it would take. Fortunately, other strategies require less memory. \n珍 哈 UNIFORM-COST SEARCH \nThe second lesson is that time is still a major factor. If your problem has a solution at depth 16, then (given our assumptions) it will take about 350 years for breadth-first search (or indeed any uninformed search) to find it. In general, exponential-complexity search problems cannot be solved by uninformed methods for any but the smallest instances. \n3.4.2 Uniform-cost search \nWhen all step costs are equal, breadth-first search is optimal because it always expands the shallowest unexpanded node. By a simple extension, we can find an algorithm that is optimal with any step-cost function. Instead of expanding the shallowest node, uniform-cost search expands the node $n$ with the lowest path cost $g ( n )$ . This is done by storing the frontier as a priority queue ordered by $g$ . The algorithm is shown in Figure 3.14. \nIn addition to the ordering of the queue by path cost, there are two other significant differences from breadth-first search. The first is that the goal test is applied to a node when it is selected for expansion (as in the generic graph-search algorithm shown in Figure 3.7) rather than when it is first generated. The reason is that the first goal node that is generated function UNIFORM-COST-SEARCH(problem) returns a solution, or failure $n o d e gets mathbf { a }$ node with STATE $mathbf { tau } = mathbf { tau }$ problem.INITIAL-STATE, PATH- $mathrm { C O S T } = 0$ frontier $ mathbf { a }$ priority queue ordered by PATH-COST, with node as the only element explored $$ an empty set loop do if EMPTY?( frontier) then return failure node ← POP( frontier ) /* chooses the lowest-cost node in frontier */ if problem.GOAL-TEST(node.STATE) then return SOLUTION(node) add node.STATE to explored for each action in problem.ACTIONS(node.STATE) do child $$ CHILD-NODE(problem, node, action) if child .STATE is not in explored or frontier then frontier $$ INSERT(child , frontier ) else if child .STATE is in frontier with higher PATH-COST then replace that frontier node with child \n\nmay be on a suboptimal path. The second difference is that a test is added in case a better path is found to a node currently on the frontier. \nBoth of these modifications come into play in the example shown in Figure 3.15, where the problem is to get from Sibiu to Bucharest. The successors of Sibiu are Rimnicu Vilcea and Fagaras, with costs 80 and 99, respectively. The least-cost node, Rimnicu Vilcea, is expanded next, adding Pitesti with cost $8 0 + 9 7 = 1 7 7$ . The least-cost node is now Fagaras, so it is expanded, adding Bucharest with cost $9 9 + 2 1 1 = 3 1 0 .$ Now a goal node has been generated, but uniform-cost search keeps going, choosing Pitesti for expansion and adding a second path to Bucharest with cost $8 0 + 9 7 + 1 0 1 = 2 7 8 .$ Now the algorithm checks to see if this new path is better than the old one; it is, so the old one is discarded. Bucharest, now with $g$ -cost 278, is selected for expansion and the solution is returned. \n\nIt is easy to see that uniform-cost search is optimal in general. First, we observe that whenever uniform-cost search selects a node $n$ for expansion, the optimal path to that node has been found. (Were this not the case, there would have to be another frontier node $n ^ { prime }$ on the optimal path from the start node to $n$ , by the graph separation property of Figure 3.9; by definition, $n ^ { prime }$ would have lower $g$ -cost than $n$ and would have been selected first.) Then, because step costs are nonnegative, paths never get shorter as nodes are added. These two facts together imply that uniform-cost search expands nodes in order of their optimal path cost. Hence, the first goal node selected for expansion must be the optimal solution. \nUniform-cost search does not care about the number of steps a path has, but only about their total cost. Therefore, it will get stuck in an infinite loop if there is a path with an infinite sequence of zero-cost actions—for example, a sequence of $N o O p$ actions.6 Completeness is guaranteed provided the cost of every step exceeds some small positive constant $epsilon$ . \nUniform-cost search is guided by path costs rather than depths, so its complexity is not easily characterized in terms of $b$ and $d$ . Instead, let $C ^ { * }$ be the cost of the optimal solution,7 and assume that every action costs at least $epsilon$ . Then the algorithm’s worst-case time and space complexity is $O ( b ^ { 1 + } dot { lfloor cal C ^ { * } / epsilon rfloor } )$ , which can be much greater than $b ^ { d }$ . This is because uniformcost search can explore large trees of small steps before exploring paths involving large and perhaps useful steps. When all step costs are equal, $b ^ { 1 + lfloor bar { C } ^ { * } / epsilon rfloor }$ is just $b ^ { d + 1 }$ . When all step costs are the same, uniform-cost search is similar to breadth-first search, except that the latter stops as soon as it generates a goal, whereas uniform-cost search examines all the nodes at the goal’s depth to see if one has a lower cost; thus uniform-cost search does strictly more work by expanding nodes at depth $d$ unnecessarily. \n3.4.3 Depth-first search \nDepth-first search always expands the deepest node in the current frontier of the search tree. The progress of the search is illustrated in Figure 3.16. The search proceeds immediately to the deepest level of the search tree, where the nodes have no successors. As those nodes are expanded, they are dropped from the frontier, so then the search “backs up” to the next deepest node that still has unexplored successors. \nThe depth-first search algorithm is an instance of the graph-search algorithm in Figure 3.7; whereas breadth-first-search uses a FIFO queue, depth-first search uses a LIFO queue. A LIFO queue means that the most recently generated node is chosen for expansion. This must be the deepest unexpanded node because it is one deeper than its parent—which, in turn, was the deepest unexpanded node when it was selected. \nAs an alternative to the GRAPH-SEARCH-style implementation, it is common to implement depth-first search with a recursive function that calls itself on each of its children in turn. (A recursive depth-first algorithm incorporating a depth limit is shown in Figure 3.17.) \nThe properties of depth-first search depend strongly on whether the graph-search or tree-search version is used. The graph-search version, which avoids repeated states and redundant paths, is complete in finite state spaces because it will eventually expand every node. The tree-search version, on the other hand, is not complete—for example, in Figure 3.6 the algorithm will follow the Arad–Sibiu–Arad–Sibiu loop forever. Depth-first tree search can be modified at no extra memory cost so that it checks new states against those on the path from the root to the current node; this avoids infinite loops in finite state spaces but does not avoid the proliferation of redundant paths. In infinite state spaces, both versions fail if an infinite non-goal path is encountered. For example, in Knuth’s 4 problem, depth-first search would keep applying the factorial operator forever. \nFor similar reasons, both versions are nonoptimal. For example, in Figure 3.16, depthfirst search will explore the entire left subtree even if node $C$ is a goal node. If node $J$ were also a goal node, then depth-first search would return it as a solution instead of $C$ , which would be a better solution; hence, depth-first search is not optimal. \nThe time complexity of depth-first graph search is bounded by the size of the state space (which may be infinite, of course). A depth-first tree search, on the other hand, may generate all of the $O ( b ^ { m } )$ nodes in the search tree, where $m$ is the maximum depth of any node; this can be much greater than the size of the state space. Note that $m$ itself can be much larger than $d$ (the depth of the shallowest solution) and is infinite if the tree is unbounded. \nSo far, depth-first search seems to have no clear advantage over breadth-first search, so why do we include it? The reason is the space complexity. For a graph search, there is no advantage, but a depth-first tree search needs to store only a single path from the root to a leaf node, along with the remaining unexpanded sibling nodes for each node on the path. Once a node has been expanded, it can be removed from memory as soon as all its descendants have been fully explored. (See Figure 3.16.) For a state space with branching factor $b$ and maximum depth $m$ , depth-first search requires storage of only $O ( b m )$ nodes. Using the same assumptions as for Figure 3.13 and assuming that nodes at the same depth as the goal node have no successors, we find that depth-first search would require 156 kilobytes instead of 10 exabytes at depth $d = 1 6$ , a factor of 7 trillion times less space. This has led to the adoption of depth-first tree search as the basic workhorse of many areas of AI, including constraint satisfaction (Chapter 6), propositional satisfiability (Chapter 7), and logic programming (Chapter 9). For the remainder of this section, we focus primarily on the treesearch version of depth-first search. \nA variant of depth-first search called backtracking search uses still less memory. (See Chapter 6 for more details.) In backtracking, only one successor is generated at a time rather than all successors; each partially expanded node remembers which successor to generate next. In this way, only $O ( m )$ memory is needed rather than $O ( b m )$ . Backtracking search facilitates yet another memory-saving (and time-saving) trick: the idea of generating a successor by modifying the current state description directly rather than copying it first. This reduces the memory requirements to just one state description and $O ( m )$ actions. For this to work, we must be able to undo each modification when we go back to generate the next successor. For problems with large state descriptions, such as robotic assembly, these techniques are critical to success. \n3.4.4 Depth-limited search \nThe embarrassing failure of depth-first search in infinite state spaces can be alleviated by supplying depth-first search with a predetermined depth limit $ell$ . That is, nodes at depth $ell$ are treated as if they have no successors. This approach is called depth-limited search. The depth limit solves the infinite-path problem. Unfortunately, it also introduces an additional source of incompleteness if we choose $ell < d$ , that is, the shallowest goal is beyond the depth limit. (This is likely when $d$ is unknown.) Depth-limited search will also be nonoptimal if we choose $ell > d$ . Its time complexity is $O ( b ^ { ell } )$ and its space complexity is $O ( b ell )$ . Depth-first search can be viewed as a special case of depth-limited search with $ell = infty$ . \nSometimes, depth limits can be based on knowledge of the problem. For example, on the map of Romania there are 20 cities. Therefore, we know that if there is a solution, it must be of length 19 at the longest, so $ell = 1 9$ is a possible choice. But in fact if we studied the \nfunction DEPTH-LIMITED-SEARCH(problem, limit ) returns a solution, or failure/cutoff return RECURSIVE-DLS(MAKE-NODE(problem.INITIAL-STATE), problem, limit)   \nfunction RECURSIVE-DLS(node, problem, limit ) returns a solution, or failure/cutoff if problem.GOAL-TEST(node.STATE) then return SOLUTION(node) else if limit $= 0$ then return cutoff else cutoff occurred? $$ false for each action in problem.ACTIONS(node.STATE) do child $$ CHILD-NODE(problem, node, action) result $$ RECURSIVE-DLS(child, problem, limit − 1) if result $mathbf { tau } = mathbf { tau }$ cutoff then cutoff occurred? $$ true else if result $neq$ failure then return result if cutoff occurred? then return cutoff else return failure \nDIAMETER \nmap carefully, we would discover that any city can be reached from any other city in at most 9 steps. This number, known as the diameter of the state space, gives us a better depth limit, which leads to a more efficient depth-limited search. For most problems, however, we will not know a good depth limit until we have solved the problem. \nDepth-limited search can be implemented as a simple modification to the general treeor graph-search algorithm. Alternatively, it can be implemented as a simple recursive algorithm as shown in Figure 3.17. Notice that depth-limited search can terminate with two kinds of failure: the standard failure value indicates no solution; the cutoff value indicates no solution within the depth limit. \nITERATIVE DEEPENING SEARCH \n3.4.5 Iterative deepening depth-first search \nIterative deepening search (or iterative deepening depth-first search) is a general strategy, often used in combination with depth-first tree search, that finds the best depth limit. It does this by gradually increasing the limit—first 0, then 1, then 2, and so on—until a goal is found. This will occur when the depth limit reaches $d$ , the depth of the shallowest goal node. The algorithm is shown in Figure 3.18. Iterative deepening combines the benefits of depth-first and breadth-first search. Like depth-first search, its memory requirements are modest: $O ( b d )$ to be precise. Like breadth-first search, it is complete when the branching factor is finite and optimal when the path cost is a nondecreasing function of the depth of the node. Figure 3.19 shows four iterations of ITERATIVE-DEEPENING-SEARCH on a binary search tree, where the solution is found on the fourth iteration. \nIterative deepening search may seem wasteful because states are generated multiple times. It turns out this is not too costly. The reason is that in a search tree with the same (or nearly the same) branching factor at each level, most of the nodes are in the bottom level, so it does not matter much that the upper levels are generated multiple times. In an iterative deepening search, the nodes on the bottom level (depth $d$ ) are generated once, those on the function ITERATIVE-DEEPENING-SEARCH(problem) returns a solution, or failure for depth $= 0$ to $infty$ do result $$ DEPTH-LIMITED-SEARCH(problem, depth) if result $neq$ cutoff then return result Figure 3.18 The iterative deepening search algorithm, which repeatedly applies depthlimited search with increasing limits. It terminates when a solution is found or if the depthlimited search returns failure, meaning that no solution exists. \n\nnext-to-bottom level are generated twice, and so on, up to the children of the root, which are generated $d$ times. So the total number of nodes generated in the worst case is \nwhich gives a time complexity of $O ( b ^ { d } ) .$ —asymptotically the same as breadth-first search. There is some extra cost for generating the upper levels multiple times, but it is not large. For example, if $b = 1 0$ and $d = 5$ , the numbers are \nIf you are really concerned about repeating the repetition, you can use a hybrid approach that runs breadth-first search until almost all the available memory is consumed, and then runs iterative deepening from all the nodes in the frontier. In general, iterative deepening is the preferred uninformed search method when the search space is large and the depth of the solution is not known. \nIterative deepening search is analogous to breadth-first search in that it explores a complete layer of new nodes at each iteration before going on to the next layer. It would seem worthwhile to develop an iterative analog to uniform-cost search, inheriting the latter algorithm’s optimality guarantees while avoiding its memory requirements. The idea is to use increasing path-cost limits instead of increasing depth limits. The resulting algorithm, called iterative lengthening search, is explored in Exercise 3.17. It turns out, unfortunately, that iterative lengthening incurs substantial overhead compared to uniform-cost search. \n3.4.6 Bidirectional search \nThe idea behind bidirectional search is to run two simultaneous searches—one forward from the initial state and the other backward from the goal—hoping that the two searches meet in the middle (Figure 3.20). The motivation is that $b ^ { d / 2 } + bar { b ^ { d / 2 } }$ is much less than $b ^ { d }$ , or in the figure, the area of the two small circles is less than the area of one big circle centered on the start and reaching to the goal. \nBidirectional search is implemented by replacing the goal test with a check to see whether the frontiers of the two searches intersect; if they do, a solution has been found. (It is important to realize that the first such solution found may not be optimal, even if the two searches are both breadth-first; some additional search is required to make sure there isn’t another short-cut across the gap.) The check can be done when each node is generated or selected for expansion and, with a hash table, will take constant time. For example, if a problem has solution depth $d = 6$ , and each direction runs breadth-first search one node at a time, then in the worst case the two searches meet when they have generated all of the nodes at depth 3. For $b = 1 0$ , this means a total of 2,220 node generations, compared with 1,111,110 for a standard breadth-first search. Thus, the time complexity of bidirectional search using breadth-first searches in both directions is $O ( b ^ { d / 2 } )$ . The space complexity is also $O ( b ^ { d / 2 } )$ . We can reduce this by roughly half if one of the two searches is done by iterative deepening, but at least one of the frontiers must be kept in memory so that the intersection check can be done. This space requirement is the most significant weakness of bidirectional search. \nThe reduction in time complexity makes bidirectional search attractive, but how do we search backward? This is not as easy as it sounds. Let the predecessors of a state $x$ be all those states that have $x$ as a successor. Bidirectional search requires a method for computing predecessors. When all the actions in the state space are reversible, the predecessors of $x$ are just its successors. Other cases may require substantial ingenuity. \nConsider the question of what we mean by “the goal” in searching “backward from the goal.” For the 8-puzzle and for finding a route in Romania, there is just one goal state, so the backward search is very much like the forward search. If there are several explicitly listed goal states—for example, the two dirt-free goal states in Figure 3.3—then we can construct a new dummy goal state whose immediate predecessors are all the actual goal states. But if the goal is an abstract description, such as the goal that “no queen attacks another queen” in the $n$ -queens problem, then bidirectional search is difficult to use. \n3.4.7 Comparing uninformed search strategies \nFigure 3.21 compares search strategies in terms of the four evaluation criteria set forth in Section 3.3.2. This comparison is for tree-search versions. For graph searches, the main differences are that depth-first search is complete for finite state spaces and that the space and time complexities are bounded by the size of the state space. \n3.5 INFORMED (HEURISTIC) SEARCH STRATEGIES \nINFORMED SEARCH \nThis section shows how an informed search strategy—one that uses problem-specific knowledge beyond the definition of the problem itself—can find solutions more efficiently than can an uninformed strategy. \nBEST-FIRST SEARCH \nEVALUATION FUNCTION \nThe general approach we consider is called best-first search. Best-first search is an instance of the general TREE-SEARCH or GRAPH-SEARCH algorithm in which a node is selected for expansion based on an evaluation function, $f ( n )$ . The evaluation function is construed as a cost estimate, so the node with the lowest evaluation is expanded first. The implementation of best-first graph search is identical to that for uniform-cost search (Figure 3.14), except for the use of $f$ instead of $g$ to order the priority queue. \nHEURISTIC FUNCTION \nThe choice of $f$ determines the search strategy. (For example, as Exercise 3.21 shows, best-first tree search includes depth-first search as a special case.) Most best-first algorithms include as a component of $f$ a heuristic function, denoted $h ( n )$ : \n$h ( n ) =$ estimated cost of the cheapest path from the state at node $n$ to a goal state. (Notice that $h ( n )$ takes a node as input, but, unlike $g ( n )$ , it depends only on the state at that node.) For example, in Romania, one might estimate the cost of the cheapest path from Arad to Bucharest via the straight-line distance from Arad to Bucharest. \nHeuristic functions are the most common form in which additional knowledge of the problem is imparted to the search algorithm. We study heuristics in more depth in Section 3.6. For now, we consider them to be arbitrary, nonnegative, problem-specific functions, with one constraint: if $n$ is a goal node, then $h ( n ) = 0$ . The remainder of this section covers two ways to use heuristic information to guide search. \n3.5.1 Greedy best-first search \nGREEDY BEST-FIRST SEARCH \nGreedy best-first search8 tries to expand the node that is closest to the goal, on the grounds that this is likely to lead to a solution quickly. Thus, it evaluates nodes by using just the heuristic function; that is, $f ( n ) = h ( n )$ . \nSTRAIGHT-LINEDISTANCE\nLet us see how this works for route-finding problems in Romania; we use the straightline distance heuristic, which we will call $h _ { S L D }$ . If the goal is Bucharest, we need to know the straight-line distances to Bucharest, which are shown in Figure 3.22. For example, $h _ { S L D } ( I n ( A r a d ) ) = 3 6 6$ . Notice that the values of $h _ { S L D }$ cannot be computed from the problem description itself. Moreover, it takes a certain amount of experience to know that $h _ { S L D }$ is correlated with actual road distances and is, therefore, a useful heuristic. \nFigure 3.23 shows the progress of a greedy best-first search using $h _ { S L D }$ to find a path from Arad to Bucharest. The first node to be expanded from Arad will be Sibiu because it is closer to Bucharest than either Zerind or Timisoara. The next node to be expanded will be Fagaras because it is closest. Fagaras in turn generates Bucharest, which is the goal. For this particular problem, greedy best-first search using $h _ { S L D }$ finds a solution without ever",
        "chapter": "II: Problem-solving",
        "section": "3 Solving Problems by Searching",
        "subsection": "3.4 Uninformed Search Strategies",
        "subsubsection": "N/A",
        "textbook_id": 2,
        "node_index": 13
      }
    }
  ],
  "output": {
    "question": "According to the comparison of graph data structures in the context, what does a star (*) indicate in the performance table?",
    "correct_answer": "Expected amortized time bounds for maintaining dynamic hash tables",
    "distractors": [
      "Worst-case time complexity for adjacency matrices",
      "Average-case performance for linked lists",
      "Best-case scenario for uninformed search algorithms"
    ],
    "rationale": "Step 1: Identify relevant snippet - Snippet [3] discusses 'Comparison' and 'Table 5.1' about graph data structures. Step 2: Extract key information - The snippet explicitly states: 'Stars∗ indicate expected amortized time bounds for maintaining dynamic hash tables.' Step 3: Formulate question - Create a multiple-choice question focusing on what the star symbol represents in the performance comparison table. Step 4: Create distractors - Use other concepts from the context (adjacency matrices from snippet [3], linked lists from snippet [2], and uninformed search from snippet [5]) to create plausible but incorrect alternatives. Step 5: Verify alignment - Ensure the question, correct answer, and rationale directly reference the specific information in snippet [3]."
  },
  "score": {
    "relevance": 1.0,
    "faithfulness": 1.0,
    "integration": 0.0,
    "complexity": 0.0,
    "overall": 0.30000000000000004
  }
}
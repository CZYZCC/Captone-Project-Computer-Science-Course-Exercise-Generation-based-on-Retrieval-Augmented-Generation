{
  "context": [
    {
      "node_id": "tb1_node15",
      "content": "Proof: Let $A [ 1 ldots n ]$ be any array and $m$ any integer such that the subarrays $A [ 1 ldots m ]$ and $A [ m + 1 ldots n ]$ are sorted. We prove that for all $k$ from 0 to $n$ , the last $n - k - 1$ iterations of the main loop correctly merge $A [ i ldots m ]$ and $A [ j ldots n ]$ into $B [ k . . n ]$ . The proof proceeds by induction on $n - k + 1$ , the number of elements remaining to be merged. \nIf $k > n$ , the algorithm correctly merges the two empty subarrays by doing absolutely nothing. (This is the base case of the inductive proof.) Otherwise, there are four cases to consider for the kth iteration of the main loop. \n• If $j > n$ , then subarray $A [ j ldots n ]$ is empty, so $operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ i ]$ .   \n• If $i > m$ , then subarray $A [ i ldots m ]$ is empty, $operatorname { s o } operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ j ]$ .   \n• Otherwise, if $A [ i ] < A [ j ]$ , then $operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ i ]$ .   \n• Otherwise, we must have $A [ i ] geq A [ j ]$ , and $operatorname* { m i n } { big ( } A [ i ldots m ] cup A [ j ldots n ] { big ) } = A [ j ]$ . \nIn all four cases, $B [ k ]$ is correctly assigned the smallest element of $A [ i ldots m ] cup$ $A [ j ldots n ]$ . In the two cases with the assignment $B [ k ]  A [ i ]$ , the Recursion Fairy correctly merges—sorry, I mean the Induction Hypothesis implies that the last $n - k$ iterations of the main loop correctly merge $A [ i + 1 ldots m ]$ and $A [ j ldots n ]$ into $B [ k + 1 . . n ]$ . Similarly, in the other two cases, the Recursion Fairy also correctly merges the rest of the subarrays. □ \nTheorem 1.2. MergeSort correctly sorts any input array $A [ 1 ldots n ]$ . \nProof: We prove the theorem by induction on $n$ . If $n leq 1$ , the algorithm correctly does nothing. Otherwise, the Recursion Fairy correctly sorts—sorry, I mean the induction hypothesis implies that our algorithm correctly sorts the two smaller subarrays $A [ 1 ldots m ]$ and $A [ m + 1 ldots n ]$ , after which they are correctly Merged into a single sorted array (by Lemma 1.1). □ \nAnalysis \nBecause the MergeSort algorithm is recursive, its running time is naturally expressed as a recurrence. Merge clearly takes $O ( n )$ time, because it’s a simple for-loop with constant work per iteration. We immediately obtain the following recurrence for MergeSort: \nAs in most divide-and-conquer recurrences, we can safely strip out the floors and ceilings (using a technique called domain transformations described later in this chapter), giving us the simpler recurrence $T ( n ) = 2 T ( n / 2 ) + O ( n )$ . The “all levels equal” case of the recursion tree method (also described later in this chapter) immediately implies the closed-form solution $T ( n ) = O ( n log n )$ . Even if you are not (yet) familiar with recursion trees, you can verify the solution $T ( n ) = O ( n log n )$ by induction. \n1.5 Quicksort \nQuicksort is another recursive sorting algorithm, discovered by Tony Hoare in 1959 and first published in 1961. In this algorithm, the hard work is splitting the array into smaller subarrays before recursion, so that merging the sorted subarrays is trivial. \n1. Choose a pivot element from the array.   \n2. Partition the array into three subarrays containing the elements smaller than the pivot, the pivot element itself, and the elements larger than the pivot.   \n3. Recursively quicksort the first and last subarrays. Input: S O R T I N G E X A M P L   \nChoose a pivot: S O R T I N G E X A M P L Partition: A G O E I N L M P T X S R Recurse Left: A E G I L M N O P T X S R   \nRecurse Right: A E G I L M N O P R S T X \n\nMore detailed pseudocode is given in Figure 1.8. In the Partition subroutine, the input parameter $p$ is the index of the pivot element in the unsorted array; the subroutine partitions the array and returns the new index of the pivot element. There are many different efficient partitioning algorithms; the one I’m presenting here is attributed to Nico Lomuto.6 The variable $ell$ counts the number of items in the array that are ℓess than the pivot element. \nPartition(A[1 .. n], p): swap A[p] A[n] QuickSort(A[1 .. n]): ℓ 0 #items < pivot if $( n > 1 )$ 0 for $i gets 1$ to $n - 1$ Choose a pivot element A[p] if A[i] < A[n] r Partition(A, p) ℓ ℓ + 1 QuickSort(A[1 .. r 1]) Recurse! QuickSort(A[r + 1 .. n]) Recurse! swap $A [ ell ] longleftrightarrow A [ i ]$ $s mathrm { w a p } A [ n ] longleftrightarrow A [ ell + 1 ]$ return ℓ + 1 \nCorrectness \nJust like mergesort, proving that QuickSort is correct requires two separate induction proofs: one to prove that Partition correctly partitions the array, and the other to prove that QuickSort correctly sorts assuming Partition is correct. To prove Partition is correct, we need to prove the following loop invariant: At the end of each iteration of the main loop, everything in the subarray $A [ 1 ldots ell ]$ is ℓess than $A [ n ]$ , and nothing in the subarray $A [ ell + 1 ldots i ]$ is less than $boldsymbol { A } [ n ]$ . I’ll leave the remaining straightforward but tedious details as exercises for the reader.",
      "metadata": {
        "content": "Proof: Let $A [ 1 ldots n ]$ be any array and $m$ any integer such that the subarrays $A [ 1 ldots m ]$ and $A [ m + 1 ldots n ]$ are sorted. We prove that for all $k$ from 0 to $n$ , the last $n - k - 1$ iterations of the main loop correctly merge $A [ i ldots m ]$ and $A [ j ldots n ]$ into $B [ k . . n ]$ . The proof proceeds by induction on $n - k + 1$ , the number of elements remaining to be merged. \nIf $k > n$ , the algorithm correctly merges the two empty subarrays by doing absolutely nothing. (This is the base case of the inductive proof.) Otherwise, there are four cases to consider for the kth iteration of the main loop. \n• If $j > n$ , then subarray $A [ j ldots n ]$ is empty, so $operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ i ]$ .   \n• If $i > m$ , then subarray $A [ i ldots m ]$ is empty, $operatorname { s o } operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ j ]$ .   \n• Otherwise, if $A [ i ] < A [ j ]$ , then $operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ i ]$ .   \n• Otherwise, we must have $A [ i ] geq A [ j ]$ , and $operatorname* { m i n } { big ( } A [ i ldots m ] cup A [ j ldots n ] { big ) } = A [ j ]$ . \nIn all four cases, $B [ k ]$ is correctly assigned the smallest element of $A [ i ldots m ] cup$ $A [ j ldots n ]$ . In the two cases with the assignment $B [ k ]  A [ i ]$ , the Recursion Fairy correctly merges—sorry, I mean the Induction Hypothesis implies that the last $n - k$ iterations of the main loop correctly merge $A [ i + 1 ldots m ]$ and $A [ j ldots n ]$ into $B [ k + 1 . . n ]$ . Similarly, in the other two cases, the Recursion Fairy also correctly merges the rest of the subarrays. □ \nTheorem 1.2. MergeSort correctly sorts any input array $A [ 1 ldots n ]$ . \nProof: We prove the theorem by induction on $n$ . If $n leq 1$ , the algorithm correctly does nothing. Otherwise, the Recursion Fairy correctly sorts—sorry, I mean the induction hypothesis implies that our algorithm correctly sorts the two smaller subarrays $A [ 1 ldots m ]$ and $A [ m + 1 ldots n ]$ , after which they are correctly Merged into a single sorted array (by Lemma 1.1). □ \nAnalysis \nBecause the MergeSort algorithm is recursive, its running time is naturally expressed as a recurrence. Merge clearly takes $O ( n )$ time, because it’s a simple for-loop with constant work per iteration. We immediately obtain the following recurrence for MergeSort: \nAs in most divide-and-conquer recurrences, we can safely strip out the floors and ceilings (using a technique called domain transformations described later in this chapter), giving us the simpler recurrence $T ( n ) = 2 T ( n / 2 ) + O ( n )$ . The “all levels equal” case of the recursion tree method (also described later in this chapter) immediately implies the closed-form solution $T ( n ) = O ( n log n )$ . Even if you are not (yet) familiar with recursion trees, you can verify the solution $T ( n ) = O ( n log n )$ by induction. \n1.5 Quicksort \nQuicksort is another recursive sorting algorithm, discovered by Tony Hoare in 1959 and first published in 1961. In this algorithm, the hard work is splitting the array into smaller subarrays before recursion, so that merging the sorted subarrays is trivial. \n1. Choose a pivot element from the array.   \n2. Partition the array into three subarrays containing the elements smaller than the pivot, the pivot element itself, and the elements larger than the pivot.   \n3. Recursively quicksort the first and last subarrays. Input: S O R T I N G E X A M P L   \nChoose a pivot: S O R T I N G E X A M P L Partition: A G O E I N L M P T X S R Recurse Left: A E G I L M N O P T X S R   \nRecurse Right: A E G I L M N O P R S T X \n\nMore detailed pseudocode is given in Figure 1.8. In the Partition subroutine, the input parameter $p$ is the index of the pivot element in the unsorted array; the subroutine partitions the array and returns the new index of the pivot element. There are many different efficient partitioning algorithms; the one I’m presenting here is attributed to Nico Lomuto.6 The variable $ell$ counts the number of items in the array that are ℓess than the pivot element. \nPartition(A[1 .. n], p): swap A[p] A[n] QuickSort(A[1 .. n]): ℓ 0 #items < pivot if $( n > 1 )$ 0 for $i gets 1$ to $n - 1$ Choose a pivot element A[p] if A[i] < A[n] r Partition(A, p) ℓ ℓ + 1 QuickSort(A[1 .. r 1]) Recurse! QuickSort(A[r + 1 .. n]) Recurse! swap $A [ ell ] longleftrightarrow A [ i ]$ $s mathrm { w a p } A [ n ] longleftrightarrow A [ ell + 1 ]$ return ℓ + 1 \nCorrectness \nJust like mergesort, proving that QuickSort is correct requires two separate induction proofs: one to prove that Partition correctly partitions the array, and the other to prove that QuickSort correctly sorts assuming Partition is correct. To prove Partition is correct, we need to prove the following loop invariant: At the end of each iteration of the main loop, everything in the subarray $A [ 1 ldots ell ]$ is ℓess than $A [ n ]$ , and nothing in the subarray $A [ ell + 1 ldots i ]$ is less than $boldsymbol { A } [ n ]$ . I’ll leave the remaining straightforward but tedious details as exercises for the reader.",
        "chapter": "Recursion",
        "section": "Mergesort",
        "subsection": "Analysis",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 15
      }
    },
    {
      "node_id": "tb1_node16",
      "content": "1.5 Quicksort \nQuicksort is another recursive sorting algorithm, discovered by Tony Hoare in 1959 and first published in 1961. In this algorithm, the hard work is splitting the array into smaller subarrays before recursion, so that merging the sorted subarrays is trivial. \n1. Choose a pivot element from the array.   \n2. Partition the array into three subarrays containing the elements smaller than the pivot, the pivot element itself, and the elements larger than the pivot.   \n3. Recursively quicksort the first and last subarrays. Input: S O R T I N G E X A M P L   \nChoose a pivot: S O R T I N G E X A M P L Partition: A G O E I N L M P T X S R Recurse Left: A E G I L M N O P T X S R   \nRecurse Right: A E G I L M N O P R S T X \n\nMore detailed pseudocode is given in Figure 1.8. In the Partition subroutine, the input parameter $p$ is the index of the pivot element in the unsorted array; the subroutine partitions the array and returns the new index of the pivot element. There are many different efficient partitioning algorithms; the one I’m presenting here is attributed to Nico Lomuto.6 The variable $ell$ counts the number of items in the array that are ℓess than the pivot element. \nPartition(A[1 .. n], p): swap A[p] A[n] QuickSort(A[1 .. n]): ℓ 0 #items < pivot if $( n > 1 )$ 0 for $i gets 1$ to $n - 1$ Choose a pivot element A[p] if A[i] < A[n] r Partition(A, p) ℓ ℓ + 1 QuickSort(A[1 .. r 1]) Recurse! QuickSort(A[r + 1 .. n]) Recurse! swap $A [ ell ] longleftrightarrow A [ i ]$ $s mathrm { w a p } A [ n ] longleftrightarrow A [ ell + 1 ]$ return ℓ + 1 \nCorrectness \nJust like mergesort, proving that QuickSort is correct requires two separate induction proofs: one to prove that Partition correctly partitions the array, and the other to prove that QuickSort correctly sorts assuming Partition is correct. To prove Partition is correct, we need to prove the following loop invariant: At the end of each iteration of the main loop, everything in the subarray $A [ 1 ldots ell ]$ is ℓess than $A [ n ]$ , and nothing in the subarray $A [ ell + 1 ldots i ]$ is less than $boldsymbol { A } [ n ]$ . I’ll leave the remaining straightforward but tedious details as exercises for the reader. \n\nAnalysis \nThe analysis of quicksort is also similar to that of mergesort. Partition clearly runs in $O ( n )$ time, because it’s a simple for-loop with constant work per iteration. For QuickSort, we get a recurrence that depends on $r$ , the rank of the chosen pivot element: \nIf we could somehow always magically choose the pivot to be the median element of the array $A$ , we would have $r = lceil n / 2 rceil$ , the two subproblems would be as close to the same size as possible, the recurrence would become \nand we’d have $T ( n ) = O ( n log n )$ using either the recursion tree method or the even simpler “Oh yeah, we already solved that recurrence for mergesort” method. \nIn fact, as we will see later in this chapter, we can actually locate the median element in an unsorted array in linear time, but the algorithm is fairly complicated, and the hidden constant in the $O ( cdot )$ notation is large enough to make the resulting sorting algorithm impractical. In practice, most programmers settle for something simple, like choosing the first or last element of the array. In this case, $r$ can take any value between 1 and $n$ , so we have \nIn the worst case, the two subproblems are completely unbalanced—either $r = 1$ or $r = n$ —and the recurrence becomes $T ( n ) leq T ( n - 1 ) + O ( n )$ . The solution is $T ( n ) = O ( n ^ { 2 } )$ . \nAnother common heuristic is called “median of three”—choose three elements (usually at the beginning, middle, and end of the array), and take the median of those three elements as the pivot. Although this heuristic is somewhat more efficient in practice than just choosing one element, especially when the array is already (nearly) sorted, we can still have $r = 2$ or $r = n - 1$ in the worst case. With the median-of-three heuristic, the recurrence becomes $T ( n ) leq T ( 1 ) + T ( n - 2 ) + O ( n )$ , whose solution is still $T ( n ) = O ( n ^ { 2 } )$ .",
      "metadata": {
        "content": "1.5 Quicksort \nQuicksort is another recursive sorting algorithm, discovered by Tony Hoare in 1959 and first published in 1961. In this algorithm, the hard work is splitting the array into smaller subarrays before recursion, so that merging the sorted subarrays is trivial. \n1. Choose a pivot element from the array.   \n2. Partition the array into three subarrays containing the elements smaller than the pivot, the pivot element itself, and the elements larger than the pivot.   \n3. Recursively quicksort the first and last subarrays. Input: S O R T I N G E X A M P L   \nChoose a pivot: S O R T I N G E X A M P L Partition: A G O E I N L M P T X S R Recurse Left: A E G I L M N O P T X S R   \nRecurse Right: A E G I L M N O P R S T X \n\nMore detailed pseudocode is given in Figure 1.8. In the Partition subroutine, the input parameter $p$ is the index of the pivot element in the unsorted array; the subroutine partitions the array and returns the new index of the pivot element. There are many different efficient partitioning algorithms; the one I’m presenting here is attributed to Nico Lomuto.6 The variable $ell$ counts the number of items in the array that are ℓess than the pivot element. \nPartition(A[1 .. n], p): swap A[p] A[n] QuickSort(A[1 .. n]): ℓ 0 #items < pivot if $( n > 1 )$ 0 for $i gets 1$ to $n - 1$ Choose a pivot element A[p] if A[i] < A[n] r Partition(A, p) ℓ ℓ + 1 QuickSort(A[1 .. r 1]) Recurse! QuickSort(A[r + 1 .. n]) Recurse! swap $A [ ell ] longleftrightarrow A [ i ]$ $s mathrm { w a p } A [ n ] longleftrightarrow A [ ell + 1 ]$ return ℓ + 1 \nCorrectness \nJust like mergesort, proving that QuickSort is correct requires two separate induction proofs: one to prove that Partition correctly partitions the array, and the other to prove that QuickSort correctly sorts assuming Partition is correct. To prove Partition is correct, we need to prove the following loop invariant: At the end of each iteration of the main loop, everything in the subarray $A [ 1 ldots ell ]$ is ℓess than $A [ n ]$ , and nothing in the subarray $A [ ell + 1 ldots i ]$ is less than $boldsymbol { A } [ n ]$ . I’ll leave the remaining straightforward but tedious details as exercises for the reader. \n\nAnalysis \nThe analysis of quicksort is also similar to that of mergesort. Partition clearly runs in $O ( n )$ time, because it’s a simple for-loop with constant work per iteration. For QuickSort, we get a recurrence that depends on $r$ , the rank of the chosen pivot element: \nIf we could somehow always magically choose the pivot to be the median element of the array $A$ , we would have $r = lceil n / 2 rceil$ , the two subproblems would be as close to the same size as possible, the recurrence would become \nand we’d have $T ( n ) = O ( n log n )$ using either the recursion tree method or the even simpler “Oh yeah, we already solved that recurrence for mergesort” method. \nIn fact, as we will see later in this chapter, we can actually locate the median element in an unsorted array in linear time, but the algorithm is fairly complicated, and the hidden constant in the $O ( cdot )$ notation is large enough to make the resulting sorting algorithm impractical. In practice, most programmers settle for something simple, like choosing the first or last element of the array. In this case, $r$ can take any value between 1 and $n$ , so we have \nIn the worst case, the two subproblems are completely unbalanced—either $r = 1$ or $r = n$ —and the recurrence becomes $T ( n ) leq T ( n - 1 ) + O ( n )$ . The solution is $T ( n ) = O ( n ^ { 2 } )$ . \nAnother common heuristic is called “median of three”—choose three elements (usually at the beginning, middle, and end of the array), and take the median of those three elements as the pivot. Although this heuristic is somewhat more efficient in practice than just choosing one element, especially when the array is already (nearly) sorted, we can still have $r = 2$ or $r = n - 1$ in the worst case. With the median-of-three heuristic, the recurrence becomes $T ( n ) leq T ( 1 ) + T ( n - 2 ) + O ( n )$ , whose solution is still $T ( n ) = O ( n ^ { 2 } )$ .",
        "chapter": "Recursion",
        "section": "Quicksort",
        "subsection": "Correctness",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 16
      }
    },
    {
      "node_id": "tb1_node17",
      "content": "Analysis \nThe analysis of quicksort is also similar to that of mergesort. Partition clearly runs in $O ( n )$ time, because it’s a simple for-loop with constant work per iteration. For QuickSort, we get a recurrence that depends on $r$ , the rank of the chosen pivot element: \nIf we could somehow always magically choose the pivot to be the median element of the array $A$ , we would have $r = lceil n / 2 rceil$ , the two subproblems would be as close to the same size as possible, the recurrence would become \nand we’d have $T ( n ) = O ( n log n )$ using either the recursion tree method or the even simpler “Oh yeah, we already solved that recurrence for mergesort” method. \nIn fact, as we will see later in this chapter, we can actually locate the median element in an unsorted array in linear time, but the algorithm is fairly complicated, and the hidden constant in the $O ( cdot )$ notation is large enough to make the resulting sorting algorithm impractical. In practice, most programmers settle for something simple, like choosing the first or last element of the array. In this case, $r$ can take any value between 1 and $n$ , so we have \nIn the worst case, the two subproblems are completely unbalanced—either $r = 1$ or $r = n$ —and the recurrence becomes $T ( n ) leq T ( n - 1 ) + O ( n )$ . The solution is $T ( n ) = O ( n ^ { 2 } )$ . \nAnother common heuristic is called “median of three”—choose three elements (usually at the beginning, middle, and end of the array), and take the median of those three elements as the pivot. Although this heuristic is somewhat more efficient in practice than just choosing one element, especially when the array is already (nearly) sorted, we can still have $r = 2$ or $r = n - 1$ in the worst case. With the median-of-three heuristic, the recurrence becomes $T ( n ) leq T ( 1 ) + T ( n - 2 ) + O ( n )$ , whose solution is still $T ( n ) = O ( n ^ { 2 } )$ . \nIntuitively, the pivot element should “usually” fall somewhere in the middle of the array, say with rank between $n / 1 0$ and $9 n / 1 0$ . This observation suggests that the “average-case” running time should be $O ( n log { n } )$ . Although this intuition can be formalized, the most common formalization makes the completely unrealistic assumption that all permutations of the input array are equally likely. Real world data may be random, but it is not random in any way that we can predict in advance, and it is certainly not uniform!7 \nOccasionally people also consider “best case” running time for some reason. We won’t. \n1.6 The Pattern \nBoth mergesort and quicksort follow a general three-step pattern called divide and conquer: \n1. Divide the given instance of the problem into several independent smaller instances of exactly the same problem.   \n2. Delegate each smaller instance to the Recursion Fairy.   \n3. Combine the solutions for the smaller instances into the final solution for the given instance. \nIf the size of any instance falls below some constant threshold, we abandon recursion and solve the problem directly, by brute force, in constant time. \nProving a divide-and-conquer algorithm correct almost always requires induction. Analyzing the running time requires setting up and solving a recurrence, which usually (but unfortunately not always!) can be solved using recursion trees. \n1.7 Recursion Trees \nSo what are these “recursion trees” I keep talking about? Recursion trees are a simple, general, pictorial tool for solving divide-and-conquer recurrences. A recursion tree is a rooted tree with one node for each recursive subproblem. The value of each node is the amount of time spent on the corresponding subproblem excluding recursive calls. Thus, the overall running time of the algorithm is the sum of the values of all nodes in the tree. \nTo make this idea more concrete, imagine a divide-and-conquer algorithm that spends $O ( f ( n ) )$ time on non-recursive work, and then makes $r$ recursive calls, each on a problem of size $n / c$ . Up to constant factors (which we can hide in the $O ( )$ notation), the running time of this algorithm is governed by the recurrence",
      "metadata": {
        "content": "Analysis \nThe analysis of quicksort is also similar to that of mergesort. Partition clearly runs in $O ( n )$ time, because it’s a simple for-loop with constant work per iteration. For QuickSort, we get a recurrence that depends on $r$ , the rank of the chosen pivot element: \nIf we could somehow always magically choose the pivot to be the median element of the array $A$ , we would have $r = lceil n / 2 rceil$ , the two subproblems would be as close to the same size as possible, the recurrence would become \nand we’d have $T ( n ) = O ( n log n )$ using either the recursion tree method or the even simpler “Oh yeah, we already solved that recurrence for mergesort” method. \nIn fact, as we will see later in this chapter, we can actually locate the median element in an unsorted array in linear time, but the algorithm is fairly complicated, and the hidden constant in the $O ( cdot )$ notation is large enough to make the resulting sorting algorithm impractical. In practice, most programmers settle for something simple, like choosing the first or last element of the array. In this case, $r$ can take any value between 1 and $n$ , so we have \nIn the worst case, the two subproblems are completely unbalanced—either $r = 1$ or $r = n$ —and the recurrence becomes $T ( n ) leq T ( n - 1 ) + O ( n )$ . The solution is $T ( n ) = O ( n ^ { 2 } )$ . \nAnother common heuristic is called “median of three”—choose three elements (usually at the beginning, middle, and end of the array), and take the median of those three elements as the pivot. Although this heuristic is somewhat more efficient in practice than just choosing one element, especially when the array is already (nearly) sorted, we can still have $r = 2$ or $r = n - 1$ in the worst case. With the median-of-three heuristic, the recurrence becomes $T ( n ) leq T ( 1 ) + T ( n - 2 ) + O ( n )$ , whose solution is still $T ( n ) = O ( n ^ { 2 } )$ . \nIntuitively, the pivot element should “usually” fall somewhere in the middle of the array, say with rank between $n / 1 0$ and $9 n / 1 0$ . This observation suggests that the “average-case” running time should be $O ( n log { n } )$ . Although this intuition can be formalized, the most common formalization makes the completely unrealistic assumption that all permutations of the input array are equally likely. Real world data may be random, but it is not random in any way that we can predict in advance, and it is certainly not uniform!7 \nOccasionally people also consider “best case” running time for some reason. We won’t. \n1.6 The Pattern \nBoth mergesort and quicksort follow a general three-step pattern called divide and conquer: \n1. Divide the given instance of the problem into several independent smaller instances of exactly the same problem.   \n2. Delegate each smaller instance to the Recursion Fairy.   \n3. Combine the solutions for the smaller instances into the final solution for the given instance. \nIf the size of any instance falls below some constant threshold, we abandon recursion and solve the problem directly, by brute force, in constant time. \nProving a divide-and-conquer algorithm correct almost always requires induction. Analyzing the running time requires setting up and solving a recurrence, which usually (but unfortunately not always!) can be solved using recursion trees. \n1.7 Recursion Trees \nSo what are these “recursion trees” I keep talking about? Recursion trees are a simple, general, pictorial tool for solving divide-and-conquer recurrences. A recursion tree is a rooted tree with one node for each recursive subproblem. The value of each node is the amount of time spent on the corresponding subproblem excluding recursive calls. Thus, the overall running time of the algorithm is the sum of the values of all nodes in the tree. \nTo make this idea more concrete, imagine a divide-and-conquer algorithm that spends $O ( f ( n ) )$ time on non-recursive work, and then makes $r$ recursive calls, each on a problem of size $n / c$ . Up to constant factors (which we can hide in the $O ( )$ notation), the running time of this algorithm is governed by the recurrence",
        "chapter": "Recursion",
        "section": "Quicksort",
        "subsection": "Analysis",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 17
      }
    },
    {
      "node_id": "tb1_node26",
      "content": "This algorithm—which might reasonably be called “squaring and mediation”— also performs only $O ( log n )$ multiplications. \nBoth of these algorithms are asymptotically optimal; any algorithm that computes $a ^ { n }$ must perform at least $Omega ( log n )$ multiplications, because each multiplication at most doubles the largest power computed so far. In fact, when $n$ is a power of two, both of these algorithms require exactly $log _ { 2 } n$ multiplications, which is exactly optimal. However, there are slightly faster methods for other values of $n$ . For example, Pin˙galaPower and PeasantPower each compute $a ^ { 1 5 }$ using six multiplications, but in fact only five multiplications are necessary: \n• Pi˙ngala: $a  a ^ { 2 }  a ^ { 3 }  a ^ { 6 }  a ^ { 7 }  a ^ { 1 4 }  a ^ { 1 5 }$ • Peasant: $a  a ^ { 2 }  a ^ { 4 }  a ^ { 8 }  a ^ { 1 2 }  a ^ { 1 4 }  a ^ { 1 5 }$ • Optimal: $a  a ^ { 2 }  a ^ { 3 }  a ^ { 5 }  a ^ { 1 0 }  a ^ { 1 5 }$ \nIt is a long-standing open question whether the absolute minimum number of multiplications for a given exponent $n$ can be computed efficiently. \nExercises \nTower of Hanoi \n1. Prove that the original recursive Tower of Hanoi algorithm performs exactly the same sequence of moves—the same disks, to and from the same pegs, in the same order—as each of the following non-recursive algorithms. The pegs are labeled 0, 1, and 2, and our problem is to move a stack of $n$ disks from peg 0 to peg 2 (as shown on page 24). \n(a) If $n$ is even, swap pegs 1 and 2. At the ith step, make the only legal move that avoids peg i mod 3. If there is no legal move, then all disks are on peg i mod 3, and the puzzle is solved.   \n(b) For the first move, move disk 1 to peg 1 if $n$ is even and to peg 2 if $n$ is odd. Then repeatedly make the only legal move that involves a different disk from the previous move. If no such move exists, the puzzle is solved.   \n(c) Pretend that disks $n + 1 , n + 2 $ , and $n + 3$ are at the bottom of pegs 0, 1, and 2, respectively. Repeatedly make the only legal move that satisfies the following constraints, until no such move is possible. • Do not place an odd disk directly on top of another odd disk.   \n• Do not place an even disk directly on top of another even disk.   \n• Do not undo the previous move. \n\n(d) Let $rho ( n )$ denote the smallest integer $k$ such that $n / 2 ^ { k }$ is not an integer. For example, $rho ( 4 2 ) = 2$ , because $4 2 / 2 ^ { 1 }$ is an integer but $4 2 / 2 ^ { 2 }$ is not. (Equivalently, $rho ( n )$ is one more than the position of the least significant 1 in the binary representation of $n$ .) Because its behavior resembles the marks on a ruler, $rho ( n )$ is sometimes called the ruler function. \nRulerHanoi(n): i 1 while $rho ( i ) leq n$ if $n - i$ is even move disk $rho ( i )$ forward $langle langle 0  1  2  0 rangle rangle$ else move disk $rho ( i )$ backward $langle langle 0 to 2 to 1 to 0 rangle rangle$ $i gets i + 1$ \n2. The Tower of Hanoi is a relatively recent descendant of a much older mechanical puzzle known as the Chinese linked rings, Baguenaudier, Cardan’s Rings, Meleda, Patience, Tiring Irons, Prisoner’s Lock, Spin-Out, and many other names. This puzzle was already well known in both China and Europe by the 16th century. The Italian mathematician Luca Pacioli described the 7-ring puzzle and its solution in his unpublished treatise De Viribus Quantitatis, written between 1498 and 1506;13 only a few years later, the Ming-dynasty poet Yang Shen described the 9-ring puzzle as “a toy for women and children”. The puzzle is apocryphally attributed to a 2nd-century Chinese general, who gave the puzzle to his wife to occupy her time while he was away at war. \nThe Baguenaudier puzzle has many physical forms, but one of the most common consists of a long metal loop and several rings, which are connected to a solid base by movable rods. The loop is initially threaded through the rings as shown in Figure 1.16; the goal of the puzzle is to remove the loop. \nMore abstractly, we can model the puzzle as a sequence of bits, one for each ring, where the ith bit is 1 if the loop passes through the ith ring and 0 otherwise. (Here we index the rings from right to left, as shown in Figure 1.16.) The puzzle allows two legal moves: \n• You can always flip the 1st ( $mathbf { bar { Psi } } = mathbf { Psi }$ rightmost) bit.   \n• If the bit string ends with exactly $z  0 s$ , you can flip the $( z + 2 ) mathrm { t h }$ bit. \nThe goal of the puzzle is to transform a string of $n uparrow s$ into a string of $n  : 0 s$ . For example, the following sequence of 21 moves solves the 5-ring puzzle: \no(a) Call a sequence of moves reduced if no move is the inverse of the previous move. Prove that for any non-negative integer $n$ , there is exactly one reduced sequence of moves that solves the $n$ -ring Baguenaudier puzzle. [Hint: This problem is much easier if you’re already familiar with graphs.]   \n(b) Describe an algorithm to solve the Baguenaudier puzzle. Your input is the number of rings $n$ ; your algorithm should print a reduced sequence of moves that solves the puzzle. For example, given the integer 5 as input, your algorithm should print the sequence $1 , 3 , 1 , 2 , 1 , 5 , 1 , 2 , 1 , 3$ , $1 , 2 , 1 , 4 , 1 , 2 , 1 , 3 , 1 , 2 , 1$ .   \n(c) Exactly how many moves does your algorithm perform, as a function of n? Prove your answer is correct. \n3. A less familiar chapter in the Tower of Hanoi’s history is its brief relocation of the temple from Benares to Pisa in the early 13th century.14 The relocation was organized by the wealthy merchant-mathematician Leonardo Fibonacci, at the request of the Holy Roman Emperor Frederick II, who had heard reports of the temple from soldiers returning from the Crusades. The Towers of Pisa and their attendant monks became famous, helping to establish Pisa as a dominant trading center on the Italian peninsula. \nUnfortunately, almost as soon as the temple was moved, one of the diamond needles began to lean to one side. To avoid the possibility of the leaning tower falling over from too much use, Fibonacci convinced the priests to adopt a more relaxed rule: Any number of disks on the leaning needle can be moved together to another needle in a single move. It was still forbidden to place a larger disk on top of a smaller disk, and disks had to be moved one at a time onto the leaning needle or between the two vertical needles. \nThanks to Fibonacci’s new rule, the priests could bring about the end of the universe somewhat faster from Pisa than they could from Benares. Fortunately, the temple was moved from Pisa back to Benares after the newly crowned Pope Gregory IX excommunicated Frederick II, making the local priests less sympathetic to hosting foreign heretics with strange mathematical habits. Soon afterward, a bell tower was erected on the spot where the temple once stood; it too began to lean almost immediately. \nDescribe an algorithm to transfer a stack of $n$ disks from one vertical needle to the other vertical needle, using the smallest possible number of moves. Exactly how many moves does your algorithm perform? \n4. Consider the following restricted variants of the Tower of Hanoi puzzle In each problem, the pegs are numbered 0, 1, and 2, and your task is to move a stack of $n$ disks from peg 0 to peg 2, exactly as in problem 1. \n(a) Suppose you are forbidden to move any disk directly between peg 1 and peg 2; every move must involve peg 0. Describe an algorithm to solve this version of the puzzle in as few moves as possible. Exactly how many moves does your algorithm make? pn(b) Suppose you are only allowed to move disks from peg 0 to peg 2, from peg 2 to peg 1, or from peg 1 to peg 0. Equivalently, suppose the pegs are arranged in a circle and numbered in clockwise order, and you are only allowed to move disks counterclockwise. Describe an algorithm to solve this version of the puzzle in as few moves as possible. How many moves does your algorithm make? \nn(c) Finally, suppose your only restriction is that you may never move a disk directly from peg 0 to peg 2. Describe an algorithm to solve this version of the puzzle in as few moves as possible. How many moves does your algorithm make? [Hint: Matrices! This variant is considerably harder to analyze than the other two.] \n5. Consider the following more complex variant of the Tower of Hanoi puzzle The puzzle has a row of $k$ pegs, numbered from 1 to $k$ . In a single turn, you are allowed to move the smallest disk on peg $i$ to either peg $i - 1$ or peg $i + 1$ , for any index $i$ ; as usual, you are not allowed to place a bigger disk on a smaller disk. Your mission is to move a stack of $n$ disks from peg 1 to peg $k$ . \n(a) Describe a recursive algorithm for the case $k = 3$ . Exactly how many moves does your algorithm make? (This is exactly the same as problem 4(a).)   \n(b) Describe a recursive algorithm for the case $k = n + 1$ that requires at most $O ( n ^ { 3 } )$ moves. [Hint: Use part (a).]   \nn(c) Describe a recursive algorithm for the case $k = n + 1$ that requires at most $O ( n ^ { 2 } )$ moves. [Hint: Don’t use part (a).]   \nn(d) Describe a recursive algorithm for the case $k = { sqrt { n } }$ that requires at most a polynomial number of moves. (Which polynomial??)   \nn(e) Describe and analyze a recursive algorithm for arbitrary $n$ and $k$ . How small must $k$ be (as a function of $n$ ) so that the number of moves is bounded by a polynomial in $n ?$ \nRecursion Trees \n6. Use recursion trees to solve each of the following recurrences. \n7. Use recursion trees to solve each of the following recurrences. \n$^ { ast } 8$ . Use recursion trees to solve each of the following recurrences. \nSorting \n9. Suppose you are given a stack of $n$ pancakes of different sizes. You want to sort the pancakes so that smaller pancakes are on top of larger pancakes. The only operation you can perform is a flip—insert a spatula under the top $k$ pancakes, for some integer $k$ between 1 and $n$ , and flip them all over. \n(a) Describe an algorithm to sort an arbitrary stack of $n$ pancakes using $O ( n )$ flips. Exactly how many flips does your algorithm perform in the worst case?15 [Hint: This problem has nothing to do with the Tower of Hanoi.] \n(b) For every positive integer $n$ , describe a stack of $n$ pancakes that requires $Omega ( n )$ flips to sort.   \n(c) Now suppose one side of each pancake is burned. Describe an algorithm to sort an arbitrary stack of $n$ pancakes, so that the burned side of every pancake is facing down, using $O ( n )$ flips. Exactly how many flips does your algorithm perform in the worst case? \n10. Recall that the median-of-three heuristic examines the first, last, and middle element of the array, and uses the median of those three elements as a quicksort pivot. Prove that quicksort with the median-of-three heuristic requires $Omega ( n ^ { 2 } )$ time to sort an array of size $n$ in the worst case. Specifically, for any integer $n$ , describe a permutation of the integers 1 through $n$ , such that in every recursive call to median-of-three-quicksort, the pivot is always the second smallest element of the array. Designing this permutation requires intimate knowledge of the Partition subroutine. \n(a) As a warm-up exercise, assume that the Partition subroutine is stable, meaning it preserves the existing order of all elements smaller than the pivot, and it preserves the existing order of all elements smaller than the pivot.   \nn(b) Assume that the Partition subroutine uses the specific algorithm listed on page 29, which is not stable. \n11. (a) Hey, Moe! Hey, Larry! Prove that the following algorithm actually sorts its input! \n(b) Would StoogeSort still sort correctly if we replaced $m = lceil 2 n / 3 rceil$ with $begin{array} { r } { m = lfloor 2 n / 3 rfloor ? } end{array}$ Justify your answer.   \n(c) State a recurrence (including the base case(s)) for the number of comparisons executed by StoogeSort.   \n(d) Solve the recurrence, and prove that your solution is correct. [Hint: Ignore the ceiling.]   \n(e) Prove that the number of swaps executed by StoogeSort is at most $binom { n } { 2 }$ \n12. The following cruel and unusual sorting algorithm was proposed by Gary Miller: \nThe comparisons performed by this algorithm do not depend at all on the values in the input array; such a sorting algorithm is called oblivious. Assume for this problem that the input size $n$ is always a power of 2. \n(a) Prove by induction that Cruel correctly sorts any input array. [Hint: Consider an array that contains $n / 4 1 s , n / 4 2 s , n / 4 3 s ,$ and $n / 4 4 s$ . Why is this special case enough?]   \n(b) Prove that Cruel would not correctly sort if we removed the for-loop from Unusual.   \n(c) Prove that Cruel would not correctly sort if we swapped the last two lines of Unusual.   \n(d) What is the running time of Unusual? Justify your answer.   \n(e) What is the running time of Cruel? Justify your answer. \n13. An inversion in an array $A [ 1 ldots n ]$ is a pair of indices $( i , j )$ such that $i < j$ and $A [ i ] > A [ j ]$ . The number of inversions in an $n$ -element array is between 0 (if the array is sorted) and $binom { n } { 2 }$ (if the array is sorted backward). Describe and analyze an algorithm to count the number of inversions in an $n$ -element array in $O ( n log { n } )$ time. [Hint: Modify mergesort.] \n14. (a) Suppose you are given two sets of $n$ points, one set ${ p _ { 1 } , p _ { 2 } , . . . , p _ { n } }$ on the line $y = 0$ and the other set ${ q _ { 1 } , q _ { 2 } , ldots , q _ { n } }$ on the line $y = 1$ . Create a set of $n$ line segments by connect each point $p _ { i }$ to the corresponding point $q _ { i }$ . Describe and analyze a divide-and-conquer algorithm to determine how many pairs of these line segments intersect, in $O ( n log { n } )$ time. [Hint: See the previous problem.] (b) Now suppose you are given two sets ${ p _ { 1 } , p _ { 2 } , . . . , p _ { n } }$ and ${ q _ { 1 } , q _ { 2 } , ldots , q _ { n } }$ of $n$ points on the unit circle. Connect each point $p _ { i }$ to the corresponding point $q _ { i }$ . Describe and analyze a divide-and-conquer algorithm to determine how many pairs of these line segments intersect in $O ( n log ^ { 2 } n )$ time. [Hint: Use your solution to part (a).] \n\nn(c) Describe an algorithm for part (b) that runs in $O ( n log { n } )$ time. [Hint: Use your solution from part (b)!] \n15. (a) Describe an algorithm that sorts an input array $A [ 1 ldots n ]$ by calling a subroutine $mathsf { S Q R T S o R T } ( k )$ , which sorts the subarray $A { left[ k + 1 ldots k + { sqrt { n } } right] }$ in place, given an arbitrary integer $k$ between 0 and $n - { sqrt { n } }$ as input. (To simplify the problem, assume that $sqrt { n }$ is an integer.) Your algorithm is only allowed to inspect or modify the input array by calling SqrtSort; in particular, your algorithm must not directly compare, move, or copy array elements. How many times does your algorithm call SqrtSort in the worst case? p(b) Prove that your algorithm from part (a) is optimal up to constant factors. In other words, if $f ( n )$ is the number of times your algorithm calls SqrtSort, prove that no algorithm can sort using $o ( f ( n ) )$ calls to SqrtSort. (c) Now suppose SqrtSort is implemented recursively, by calling your sorting algorithm from part (a). For example, at the second level of recursion, the algorithm is sorting arrays roughly of size $n ^ { 1 / 4 }$ . What is the worst-case running time of the resulting sorting algorithm? (To simplify the analysis, assume that the array size $n$ has the form $2 ^ { 2 ^ { k } }$ , so that repeated square roots are always integers.) \nSelection \n16. Suppose we are given a set $s$ of $n$ items, each with a value and a weight. For any element $x in S$ , we define two subsets \n• $S _ { < x }$ is the set of elements of $s$ whose value is less than the value of $x$ . • $S _ { > x }$ is the set of elements of $s$ whose value is more than the value of $x$ \nFor any subset $R subseteq S$ , let $w ( R )$ denote the sum of the weights of elements in $R$ . The weighted median of $R$ is any element $x$ such that $w ( S _ { < x } ) leq w ( S ) / 2$ and $w ( S _ { > x } ) leq w ( S ) / 2$ . \nDescribe and analyze an algorithm to compute the weighted median of a given weighted set in $O ( n )$ time. Your input consists of two unsorted arrays $S [ 1 . . n ]$ and $W [ 1 ldots n ]$ , where for each index i, the ith element has value $s [ i ]$ and weight $W [ i ]$ . You may assume that all values are distinct and all weights are positive. \n17. (a) Describe an algorithm to determine in $O ( n )$ time whether an arbitrary array $A [ 1 ldots n ]$ contains more than $n / 4$ copies of any value. (b) Describe and analyze an algorithm to determine, given an arbitrary array $A [ 1 ldots n ]$ and an integer $k$ , whether $A$ contains more than $k$ copies of any value. Express the running time of your algorithm as a function of both $n$ and $k$ . \nDo not use hashing, or radix sort, or any other method that depends on the precise input values, as opposed to their order. \n18. Describe an algorithm to compute the median of an array $A [ 1 ldots 5 ]$ of distinct numbers using at most 6 comparisons. Instead of writing pseudocode, describe your algorithm using a decision tree: A binary tree where each internal node contains a comparison of the form $begin{array} { r } { { } ^ { * } A [ i ] gtrless A [ j ]  ? { } ^ { * } } end{array}$ and each leaf contains an index into the array. \n19. Consider the generalization of the Blum-Floyd-Pratt-Rivest-Tarjan MomSelect algorithm shown in Figure 1.22, which partitions the input array into $lceil n / b rceil$ blocks of size $b$ , instead of $lceil n / 5 rceil$ blocks of size 5, but is otherwise identical. \n(a) State a recurrence for the running time of $mathbf { M o u } _ { b }$ Select, assuming that $b$ is a constant (so the subroutine MedianOfB runs in $O ( 1 )$ time). In particular, how do the sizes of the recursive subproblems depend on the constant $b ?$ Consider even $b$ and odd $b$ separately. (b) What is the worst-case running time of $mathbf { M o u } _ { 1 }$ Select? [Hint: This is a trick question.] $triangleq left( mathrm { c } right)$ What is the worst-case running time of $mathbf { M o u } _ { 2 }$ Select? [Hint: This is an unfair question!] n(d) What is the worst-case running time of $mathrm { M o M _ { 3 } S E L E C T ? }$ Finding an upper bound on the running time is straightforward; the hard part is showing that this analysis is actually tight. [Hint: See problem 10.] n(e) What is the worst-case running time of $mathbf { M o M _ { 4 } S E L E C T ? }$ Again, the hard part is showing that the analysis cannot be improved.16 (f) For any constants $b geq 5$ , the algorithm $mathbf { M o u } _ { b }$ Select runs in $O ( n )$ time, but different values of $b$ lead to different constant factors. Let $M ( b )$ denote the minimum number of comparisons required to find the median of $b$ numbers. The exact value of $M ( b )$ is known only for $b leq 1 3$ : \nFor each $b$ between 5 and 13, find an upper bound on the running time of $mathrm { M o m } _ { b }$ Select of the form $T ( n ) leq a _ { b } n$ for some explicit constant $boldsymbol { alpha } _ { b }$ . (For example, on page 39 we showed that $alpha _ { 5 } leq 1 6 . $ (g) Which value of $b$ yields the smallest constant $boldsymbol { a _ { b } ? }$ [Hint: This is a trick question!] \n20. Prove that the variant of the Blum-Floyd-Pratt-Rivest-Tarjan Select algorithm shown in Figure 1.23, which uses an extra layer of small medians to choose the main pivot, runs in $O ( n )$ time. \n21. (a) Suppose we are given two sorted arrays $A [ 1 ldots n ]$ and $B [ 1 ldots n ]$ . Describe an algorithm to find the median element in the union of $A$ and $B$ in $Theta ( log n )$ time. You can assume that the arrays contain no duplicate elements. (b) Suppose we are given two sorted arrays $A [ 1 ldots m ]$ and $B [ 1 ldots n ]$ and an integer $k$ . Describe an algorithm to find the $k$ th smallest element in $A cup B$ in $Theta ( log ( m + n ) )$ time. For example, if $k = 1$ , your algorithm should return the smallest element of $A cup B$ .) [Hint: Use your solution to part (a).] n(c) Now suppose we are given three sorted arrays $A [ 1 ldots n ] , B [ 1 ldots n ] .$ and $C [ 1 ldots n ] _ { scriptscriptstyle { cdot } }$ , and an integer $k$ . Describe an algorithm to find the $k$ th smallest element in $A cup B cup C$ in $O ( log n )$ time. \n(d) Finally, suppose we are given a two dimensional array $A [ 1 ldots m , 1 ldots n ]$ in which every row $A [ i , cdot ]$ is sorted, and an integer $k$ . Describe an algorithm to find the kth smallest element in $A$ as quickly as possible. How does the running time of your algorithm depend on m? [Hint: Solve problem 16 first.] \nArithmetic \n22. In 1854, archaeologists discovered Sumerians clay tablets, carved around 2000bce, that list the squares of integers up to 59. This discovery led some scholars to conjecture that ancient Sumerians performed multiplication by reduction to squaring, using an identity like $x cdot y = ( x ^ { 2 } + y ^ { 2 } - ( x - y ) ^ { 2 } ) / 2$ . Unfortunately, those same scholars are silent on how the Sumerians supposedly squared larger numbers. Four thousand years later, we can finally rescue these Sumerian mathematicians from their lives of drudgery through the power of recursion! \n(a) Describe a variant of Karatsuba’s algorithm that squares any $n$ -digit number in $O ( n ^ { lg 3 } )$ time, by reducing to squaring three $lceil n / 2 rceil$ -digit numbers. (Karatsuba actually did this in 1960.)   \n(b) Describe a recursive algorithm that squares any $n$ -digit number in $O ( n ^ { log _ { 3 } 6 } )$ time, by reducing to squaring six $lceil n / 3 rceil$ -digit numbers.   \nn(c) Describe a recursive algorithm that squares any $n$ -digit number in $O ( n ^ { log _ { 3 } 5 } )$ time, by reducing to squaring only five $( n / 3 + O ( 1 ) )$ -digit numbers. [Hint: What is $( a + b + c ) ^ { 2 } + ( a - b + c ) ^ { 2 } ? J$ \n23. (a) Describe and analyze a variant of Karatsuba’s algorithm that multiplies any $m$ -digit number and any $n$ -digit number, for any $n geq m$ , in $O ( n m ^ { lg 3 - 1 } )$ time. (b) Describe an algorithm to compute the decimal representation of $2 ^ { n }$ in $O ( n ^ { lg 3 } )$ time, using the algorithm from part (a) as a subroutine. (The standard algorithm that computes one digit at a time requires $Theta ( n ^ { 2 } )$ time.) (c) Describe a divide-and-conquer algorithm to compute the decimal representation of an arbitrary $n$ -bit binary number in $O ( n ^ { lg 3 } )$ time. [Hint: Watch out for an extra log factor in the running time.] n(d) Suppose we can multiply two $n$ -digit numbers in $O ( M ( n ) )$ time. Describe an algorithm to compute the decimal representation of an arbitrary $n$ -bit binary number in $O ( M ( n ) log n )$ time. [Hint: The analysis is the hard part; use a domain transformation.] \n24. Consider the following classical recursive algorithm for computing the factorial $n !$ of a non-negative integer $n$ : \n(a) How many multiplications does this algorithm perform? \n(b) How many bits are required to write $n !$ in binary? Express your answer in the form $Theta ( f ( n ) )$ , for some familiar function $f ( n )$ . [Hint: $( n / 2 ) ^ { n / 2 } <$ $n ! < n ^ { n } . 7$   \n(c) Your answer to (b) should convince you that the number of multiplications is not a good estimate of the actual running time of Factorial. We can multiply any $k$ -digit number and any l-digit number in $O ( k cdot l )$ time using either the lattice algorithm or duplation and mediation. What is the running time of Factorial if we use this multiplication algorithm as a subroutine? \n(d) The following recursive algorithm also computes the factorial function, but using a different grouping of the multiplications: \nWhat is the running time of $mathrm { F A L L I N G } ( n , n )$ if we use grade-school multiplication? [Hint: As usual, ignore the floors and ceilings.] \n(e) Describe and analyze a variant of Karatsuba’s algorithm that multiplies any $k$ -digit number and any $l$ -digit number, for any $k geq l$ , in $O ( k cdot$ $l ^ { 1 8 3 - 1 } ) = O ( k cdot l ^ { 0 . 5 8 5 } )$ time.   \nn(f) What are the running times of Factorial $( n )$ and $mathrm { F A L L I N G } ( n , n )$ if we use the modified Karatsuba multiplication from part (e)? \n25. The greatest common divisor of two positive integer $x$ and $y$ , denoted $operatorname* { g c d } ( x , y )$ , is the largest integer $d$ such that both $x / d$ and $y / d$ are integers. Euclid’s Elements, written around 300bce, describes the following recursive algorithm to compute $operatorname* { g c d } ( x , y )$ : 17 \n(a) Prove that EuclidGCD correctly computes $operatorname* { g c d } ( x , y )$ .18 Specifically: \ni. Prove that EuclidGCD $( x , y )$ divides both $x$ and $y$ .   \nii. Prove that every divisor of $x$ and $y$ is a divisor of EuclidGCD $( x , y )$ . \n(b) What is the worst-case running time of EuclidGCD $( x , y )$ , as a function of $x$ and $y ?$ (Assume that computing $x - y$ requires $O ( log x + log y )$ time.) \n(c) Prove that the following algorithm also computes $operatorname* { g c d } ( x , y )$ : \n(d) What is the worst-case running time of FastEuclidGCD $( x , y )$ , as a function of $x$ and $y ?$ (Assume that computing $x$ mod $y$ takes $O ( log x cdot$ $log y .$ ) time.) \n(e) Prove that the following algorithm also computes $operatorname* { g c d } ( x , y )$ : \n(f) What is the worst-case running time of BinaryGCD $( x , y )$ , as a function of $x$ and $y ?$ (Assume that computing $x - y$ takes $O ( log x + log y )$ time, and computing $z / 2$ requires $O ( log z )$ time.) \nArrays \n26. Suppose you are given a $2 ^ { n } times 2 ^ { n }$ checkerboard with one (arbitrarily chosen) square removed. Describe and analyze an algorithm to compute a tiling of the board by without gaps or overlaps by L-shaped tiles, each composed of 3 squares. Your input is the integer $n$ and two $n$ -bit integers representing the row and column of the missing square. The output is a list of the positions and orientations of $( 4 ^ { n } - 1 ) / 3$ tiles. Your algorithm should run in $O ( 4 ^ { n } )$ time. [Hint: First prove that such a tiling always exists.] \n27. You are a visitor at a political convention (or perhaps a faculty meeting) with $n$ delegates; each delegate is a member of exactly one political party. It is impossible to tell which political party any delegate belongs to; in particular, you will be summarily ejected from the convention if you ask. However, you can determine whether any pair of delegates belong to the same party by introducing them to each other. Members of the same political party always greet each other with smiles and friendly handshakes; members of different parties always greet each other with angry stares and insults.19 \n(a) Suppose more than half of the delegates belong to the same political party. Describe an efficient algorithm that identifies all members of this majority party.   \n(b) Now suppose there are more than two parties, but one party has a plurality: more people belong to that party than to any other party. Present a practical procedure to precisely pick the people from the plurality political party as parsimoniously as possible, presuming the plurality party is composed of at least $p$ people. Pretty please. \n28. Smullyan Island has three types of inhabitants: knights always speak the truth; knaves always lie; and normals sometimes speak the truth and sometimes don’t. Everyone on the island knows everyone else’s name and type (knight, knave, or normal). You want to learn the type of every inhabitant. \nYou can ask any inhabitant to tell you the type of any other inhabitant. Specifically, if you ask “Hey $X$ , what is Y ’s type?” then $X$ will respond as follows: \n• If $X$ is a knight, then $X$ will respond with Y ’s correct type. • If $X$ is a knave, then $X$ could respond with either of the types that $Y$ is not. • If $X$ is a normal, then $X$ could respond with any of the three types. \nThe inhabitants will ignore any questions not of this precise form; in particular, you may not ask an inhabitant about their own type. Asking the same inhabitant the same question multiple times always yields the same answer, so there’s no point in asking any question more than once. \n(a) Suppose you know that a strict majority of inhabitants are knights. Describe an efficient algorithm to identify the type of every inhabitant.   \n(b) Prove that if at most half the inhabitants are knights, it is impossible to determine the type of every inhabitant. \n29. Most graphics hardware includes support for a low-level operation called blit, or block transfer, which quickly copies a rectangular chunk of a pixel map (a two-dimensional array of pixel values) from one location to another. This is a two-dimensional version of the standard C library function memcpy(). \nSuppose we want to rotate an $n times n$ pixel map $9 0 ^ { circ }$ clockwise. One way to do this, at least when $n$ is a power of two, is to split the pixel map into four $n / 2 times n / 2$ blocks, move each block to its proper position using a sequence of five blits, and then recursively rotate each block. (Why five? For the same reason the Tower of Hanoi puzzle needs a third peg.) Alternately, we could first recursively rotate the blocks and then blit them into place. \n(a) Prove that both versions of the algorithm are correct when $n$ is a power of 2.   \n(b) Exactly how many blits does the algorithm perform when $n$ is a power of 2?   \n(c) Describe how to modify the algorithm so that it works for arbitrary $n$ , not just powers of 2. How many blits does your modified algorithm perform?   \n(d) What is your algorithm’s running time if a $k times k$ blit takes $O ( k ^ { 2 } )$ time?   \n(e) What if a $k times k$ blit takes only $O ( k )$ time? \n30. An array $A [ 0 ldots n - 1 ]$ of $n$ distinct numbers is bitonic if there are unique indices $i$ and $j$ such that $A [ ( i - 1 ) { bmod { n } } ] < A [ i ] > A [ ( i + 1 ) { bmod { n } } ]$ and \n$A [ ( j - 1 ) { bmod { n } } ] > A [ j ] < A [ ( j + 1 ) { bmod { n } } ]$ . In other words, a bitonic sequence either consists of an increasing sequence followed by a decreasing sequence, or can be circularly shifted to become so. For example, \nDescribe and analyze an algorithm to find the smallest element in an $n$ - element bitonic array in $O ( log n )$ time. You may assume that the numbers in the input array are distinct. \n31. Suppose we are given an array $A [ 1 ldots n ]$ of $n$ distinct integers, which could be positive, negative, or zero, sorted in increasing order so that $A [ 1 ] < A [ 2 ] <$ $cdots < A [ n ]$ . \n(a) Describe a fast algorithm that either computes an index $i$ such that $A [ i ] = i$ or correctly reports that no such index exists.   \n(b) Suppose we know in advance that $A [ 1 ] > 0$ . Describe an even faster algorithm that either computes an index i such that $A [ i ] = i$ or correctly reports that no such index exists. [Hint: This is really easy.] \n32. Suppose we are given an array $A [ 1 ldots n ]$ with the special property that $A [ 1 ] geq A [ 2 ]$ and $A [ n - 1 ] leq A [ n ]$ . We say that an element $A [ x ]$ is a local minimum if it is less than or equal to both its neighbors, or more formally, if $A [ x - 1 ] geq A [ x ]$ and $A [ x ] leq A [ x + 1 ]$ . For example, there are six local minima in the following array: \n9 7 7 2 1 3 7 5 4 7 3 3 4 8 6 9 ▲ ▲ ▲ ▲ ▲ ▲ \nWe can obviously find a local minimum in $O ( n )$ time by scanning through the array. Describe and analyze an algorithm that finds a local minimum in $O ( log n )$ time. [Hint: With the given boundary conditions, the array must have at least one local minimum. Why?] \n33. Suppose you are given a sorted array of $n$ distinct numbers that has been rotated $k$ steps, for some unknown integer $k$ between 1 and $n - 1$ . That is, you are given an array $A [ 1 ldots n ]$ such that some prefix $A [ 1 ldots k ]$ is sorted in increasing order, the corresponding suffix $A [ k + 1 ldots n ]$ is sorted in increasing order, and $A [ n ] < A [ 1 ]$ . \nFor example, you might be given the following 16-element array (where $k = 1 0 mathrm {  : }$ ): \n(a) Describe and analyze an algorithm to compute the unknown integer $k$ . (b) Describe and analyze an algorithm to determine if the given array contains a given number $x$ . \n34. At the end of the second act of the action blockbuster Fast and Impossible XIII¾: The Last Guardians of Expendable Justice Reloaded,the villainous Dr. Metaphor hypnotizes the entire Hero League/Force/Squad, arranges them in a long line at the edge of a cliff, and instructs each hero to shoot the closest taller heroes to their left and right, at a prearranged signal. \nSuppose we are given the heights of all $n$ heroes, in order from left to right, in an array $H t [ 1 ldots n ]$ . (To avoid salary arguments, the producers insisted that no two heroes have the same height.) Then we can compute the Left and Right targets of each hero in $O ( n ^ { 2 } )$ time using the following brute-force algorithm. \n(a) Describe a divide-and-conquer algorithm that computes the output of WhoTargetsWhom in $O ( n log { n } )$ time.   \n(b) Prove that at least $lfloor n / 2 rfloor$ of the $n$ heroes are targets. That is, prove that the output arrays $R [ 0 ldots n - 1 ]$ and $L [ 0 ldots n - 1 ]$ contain at least $lfloor n / 2 rfloor$ distinct values (other than None).   \n(c) Alas, Dr. Metaphor’s diabolical plan is successful. At the prearranged signal, all the heroes simultaneously shoot their targets, and all targets fall over the cliff, apparently dead. Metaphor repeats his dastardly experiment over and over; after each massacre, he forces the remaining heroes to choose new targets, following the same algorithm, and then shoot their targets at the next signal. Eventually, only the shortest member of the Hero Crew/Alliance/Posse is left alive.20 \nDescribe and analyze an algorithm to compute the number of rounds before Dr. Metaphor’s deadly process finally ends. For full credit, your algorithm should run in $O ( n )$ time. \n35. You are a contestant on the hit game show “Beat Your Neighbors!” You are presented with an $m times n$ grid of boxes, each containing a unique number. It costs $$ 100$ to open a box. Your goal is to find a box whose number is larger than its neighbors in the grid (above, below, left, and right). If you spend less money than any of your opponents, you win a week-long trip for two to Las Vegas and a year’s supply of Rice-A-Ronitm, to which you are hopelessly addicted. \n(a) Suppose $m = 1$ . Describe an algorithm that finds a number that is bigger than either of its neighbors. How many boxes does your algorithm open in the worst case? n(b) Suppose $m = n$ . Describe an algorithm that finds a number that is bigger than any of its neighbors. How many boxes does your algorithm open in the worst case? ${ } ^ { mathsf { a w } } ( { mathsf { c } } )$ Prove that your solution to part (b) is optimal up to a constant factor. \n36. (a) Let $n = 2 ^ { ell } - 1$ for some positive integer $ell$ . Suppose someone claims to hold an unsorted array $A [ 1 ldots n ]$ of distinct $ell$ -bit strings; thus, exactly one $ell$ -bit string does not appear in $A$ . Suppose further that the only way we can access $A$ is by calling the function $mathrm { F E T C H B I T } ( i , j )$ , which returns the $j$ th bit of the string $A [ i ]$ in $O ( 1 )$ time. Describe an algorithm to find the missing string in $A$ using only $O ( n )$ calls to FetchBit. \nn(b) Now suppose $n = 2 ^ { ell } - k$ for some positive integers $k$ and $ell$ , and again we are given an array $A [ 1 ldots n ]$ of distinct $ell$ -bit strings. Describe an algorithm to find the $k$ strings that are missing from $A$ using only $O ( n log k )$ calls to FetchBit. \nTrees \n37. For this problem, a subtree of a binary tree means any connected subgraph. A binary tree is complete if every internal node has two children, and every leaf has exactly the same depth. Describe and analyze a recursive algorithm to compute the largest complete subtree of a given binary tree. Your algorithm should return both the root and the depth of this subtree. See Figure 1.26 for an example. \n38. Let $T$ be a binary tree with $n$ vertices. Deleting any vertex $nu$ splits $T$ into at most three subtrees, containing the left child of $nu$ (if any), the right child of $nu$ (if any), and the parent of $nu$ (if any). We call $nu$ a central vertex if each of these smaller trees has at most $n / 2$ vertices. See Figure 1.27 for an example. \nDescribe and analyze an algorithm to find a central vertex in an arbitrary given binary tree. [Hint: First prove that every tree has a central vertex.] \n39. (a) Professor George O’Jungle has a 27-node binary tree, in which every node is labeled with a unique letter of the Roman alphabet or the character &. Preorder and postorder traversals of the tree visit the nodes in the following order: \n• Preorder: I Q J H L E M V O T S B R G Y Z K C A & F P N U D W X • Postorder: H E M L J V Q S G Y R Z B T C P U D N F W & X A K O I \nDraw George’s binary tree. \n(b) Recall that a binary tree is full if every non-leaf node has exactly two children. i. Describe and analyze a recursive algorithm to reconstruct an arbitrary full binary tree, given its preorder and postorder node sequences as input. ii. Prove that there is no algorithm to reconstruct an arbitrary binary tree from its preorder and postorder node sequences.   \n(c) Describe and analyze a recursive algorithm to reconstruct an arbitrary binary tree, given its preorder and inorder node sequences as input.   \n(d) Describe and analyze a recursive algorithm to reconstruct an arbitrary binary search tree, given only its preorder node sequence.   \nn(e) Describe and analyze a recursive algorithm to reconstruct an arbitrary binary search tree, given only its preorder node sequence, in $O ( n )$ time. \nIn parts (b)–(e), assume that all keys are distinct and that the input is consistent with at least one binary tree. \n40. Suppose we have n points scattered inside a two-dimensional box. A kdtree21 recursively subdivides the points as follows. If the box contains no points in its interior, we are done. Otherwise, we split the box into two smaller boxes with a vertical line, through a median point inside the box (not on its boundary), partitioning the points as evenly as possible. Then we recursively build a kd-tree for the points in each of the two smaller boxes, after rotating them 90 degrees. Thus, we alternate between splitting vertically and splitting horizontally at each level of recursion. The final empty boxes are called cells. \n(a) How many cells are there, as a function of $n ?$ Prove your answer is correct.   \n(b) In the worst case, exactly how many cells can a horizontal line cross, as a function of $n ?$ Prove your answer is correct. Assume that $n = 2 ^ { k } - 1$ for some integer $k$ . [Hint: There is more than one function $f$ such that $f ( 1 6 ) = 4 . 7$   \n(c) Suppose we are given $n$ points stored in a kd-tree. Describe and analyze an algorithm that counts the number of points above a horizontal line (such as the dashed line in the figure) as quickly as possible. [Hint: Use part $( b ) . jmath$   \n(d) Describe an analyze an efficient algorithm that counts, given a kd-tree containing $n$ points, the number of points that lie inside a rectangle $R$ with horizontal and vertical sides. [Hint: Use part (c).] \nn41. Bob Ratenbur, a new student in CS 225, is trying to write code to perform preorder, inorder, and postorder traversals of binary trees. Bob sort-of understands the basic idea behind the traversal algorithms, but whenever he actually tries to implement them, he keeps mixing up the recursive calls. Five minutes before the deadline, Bob frantically submits code with the following structure: \nPreOrder(v): InOrder(v): PostOrder(v): if v = Null if v = Null if v = Null return return return else else else print label(v) Order(left(v)) Order(left(v)) Order(left(v)) print label(v) Order(right(v)) Order(right(v)) Order(right(v)) print label(v) \nEach in this pseudocode hides one of the prefixes Pre, In, or Post. Moreover, each of the following function calls appears exactly once in Bob’s submitted code: \nPreOrder(left(v)) PreOrder(right(v)) InOrder(left(v)) InOrder(right(v)) PostOrder(left(v)) PostOrder(right(v)) \nThus, there are precisely 36 possibilities for Bob’s code. Unfortunately, Bob accidentally deleted his source code after submitting the executable, so neither you nor he knows which functions were called where. \nNow suppose you are given the output of Bob’s traversal algorithms, executed on some unknown binary tree $T$ . Bob’s output has been helpfully parsed into three arrays $P r e [ 1 ldots n ] , I n [ 1 ldots n ] $ , and $P o s t [ 1 ldots n ]$ . You may assume that these traversal sequences are consistent with exactly one binary tree $T$ ; in particular, the vertex labels of the unknown tree $T$ are distinct, and every internal node in $T$ has exactly two children. \n(a) Describe an algorithm to reconstruct the unknown tree $T$ from the given traversal sequences.   \n(b) Describe an algorithm that either reconstructs Bob’s code from the given traversal sequences, or correctly reports that the traversal sequences are consistent with more than one set of algorithms. \nFor example, given the input \nyour first algorithm should return the following tree: \nand your second algorithm should reconstruct the following code: \n$^ { bullet } 4 2$ . Let $T$ be a binary tree whose nodes store distinct numerical values. Recall that $T$ is a binary search tree if and only if either (1) $T$ is empty, or (2) $T$ satisfies the following recursive conditions: \n• The left subtree of $T$ is a binary search tree.   \n• All values in the left subtree are smaller than the value at the root.   \n• The right subtree of $T$ is a binary search tree.   \n• All values in the right subtree are larger than the value at the root. \nConsider the following pair of operations on binary trees: \n• Rotate an arbitrary node upward.22 • Swap the left and right subtrees of an arbitrary node. \n\nIn both of these operations, some, all, or none of the subtrees $A , B$ , and $C$ could be empty. \n(a) Describe an algorithm to transform an arbitrary $n$ -node binary tree with distinct node values into a binary search tree, using at most $O ( n ^ { 2 } )$ rotations and swaps. Figure 1.29 shows a sequence of eight operations that transforms a five-node binary tree into a binary search tree. \nYour algorithm is not allowed to directly modify parent or child pointers, create new nodes, or delete old nodes; the only way to modify the tree is through rotations and swaps. \nOn the other hand, you may compute anything you like for free, as long as that computation does not modify the tree; the running time of your algorithm is defined to be the number of rotations and swaps that it performs. \nn(b) Describe an algorithm to transform an arbitrary $n$ -node binary tree into a binary search tree, using at most $O ( n log { n } )$ rotations and swaps. \n(c) Prove that any $n$ -node binary search tree can be transformed into any other binary search tree with the same node values, using only $O ( n )$ rotations (and no swaps).   \nn(d) Open problem: Either describe an algorithm to transform an arbitrary $n$ -node binary tree into a binary search tree using only $O ( n )$ rotations and swaps, or prove that no such algorithm is possible. [Hint: I don’t think it’s possible.] \nWhere, however, the ambiguity cannot be cleared up, either by the rule of faith or by the context, there is nothing to hinder us to point the sentence according to any method we choose of those that suggest themselves. \n— Augustine of Hippo, De doctrina Christiana (397CE) Translated by Marcus Dods (1892) \nI dropped my dinner, and ran back to the laboratory. There, in my excitement, I tasted the contents of every beaker and evaporating dish on the table. Luckily for me, none contained any corrosive or poisonous liquid. \n— Constantine Fahlberg on his discovery of saccharin, Scientific American (1886) \nThe greatest challenge to any thinker is stating the problem in a way that will allow a solution. \n— attributed to Bertrand Russell \nWhen you come to a fork in the road, take it. \n— Yogi Berra (giving directions to his house) \n2 \nBacktracking \nThis chapter describes another important recursive strategy called backtracking. A backtracking algorithm tries to construct a solution to a computational problem incrementally, one small piece at a time. Whenever the algorithm needs to decide between multiple alternatives to the next component of the solution, it recursively evaluates every alternative and then chooses the best one. \n2.1 N Queens \nThe prototypical backtracking problem is the classical $pmb { n }$ Queens Problem, first proposed by German chess enthusiast Max Bezzel in 1848 (under his pseudonym “Schachfreund”) for the standard $8 times 8$ board and by François-Joseph Eustache Lionnet in 1869 for the more general $n times n$ board. The problem is to place $n$ queens on an $n times n$ chessboard, so that no two queens are attacking each other.",
      "metadata": {
        "content": "This algorithm—which might reasonably be called “squaring and mediation”— also performs only $O ( log n )$ multiplications. \nBoth of these algorithms are asymptotically optimal; any algorithm that computes $a ^ { n }$ must perform at least $Omega ( log n )$ multiplications, because each multiplication at most doubles the largest power computed so far. In fact, when $n$ is a power of two, both of these algorithms require exactly $log _ { 2 } n$ multiplications, which is exactly optimal. However, there are slightly faster methods for other values of $n$ . For example, Pin˙galaPower and PeasantPower each compute $a ^ { 1 5 }$ using six multiplications, but in fact only five multiplications are necessary: \n• Pi˙ngala: $a  a ^ { 2 }  a ^ { 3 }  a ^ { 6 }  a ^ { 7 }  a ^ { 1 4 }  a ^ { 1 5 }$ • Peasant: $a  a ^ { 2 }  a ^ { 4 }  a ^ { 8 }  a ^ { 1 2 }  a ^ { 1 4 }  a ^ { 1 5 }$ • Optimal: $a  a ^ { 2 }  a ^ { 3 }  a ^ { 5 }  a ^ { 1 0 }  a ^ { 1 5 }$ \nIt is a long-standing open question whether the absolute minimum number of multiplications for a given exponent $n$ can be computed efficiently. \nExercises \nTower of Hanoi \n1. Prove that the original recursive Tower of Hanoi algorithm performs exactly the same sequence of moves—the same disks, to and from the same pegs, in the same order—as each of the following non-recursive algorithms. The pegs are labeled 0, 1, and 2, and our problem is to move a stack of $n$ disks from peg 0 to peg 2 (as shown on page 24). \n(a) If $n$ is even, swap pegs 1 and 2. At the ith step, make the only legal move that avoids peg i mod 3. If there is no legal move, then all disks are on peg i mod 3, and the puzzle is solved.   \n(b) For the first move, move disk 1 to peg 1 if $n$ is even and to peg 2 if $n$ is odd. Then repeatedly make the only legal move that involves a different disk from the previous move. If no such move exists, the puzzle is solved.   \n(c) Pretend that disks $n + 1 , n + 2 $ , and $n + 3$ are at the bottom of pegs 0, 1, and 2, respectively. Repeatedly make the only legal move that satisfies the following constraints, until no such move is possible. • Do not place an odd disk directly on top of another odd disk.   \n• Do not place an even disk directly on top of another even disk.   \n• Do not undo the previous move. \n\n(d) Let $rho ( n )$ denote the smallest integer $k$ such that $n / 2 ^ { k }$ is not an integer. For example, $rho ( 4 2 ) = 2$ , because $4 2 / 2 ^ { 1 }$ is an integer but $4 2 / 2 ^ { 2 }$ is not. (Equivalently, $rho ( n )$ is one more than the position of the least significant 1 in the binary representation of $n$ .) Because its behavior resembles the marks on a ruler, $rho ( n )$ is sometimes called the ruler function. \nRulerHanoi(n): i 1 while $rho ( i ) leq n$ if $n - i$ is even move disk $rho ( i )$ forward $langle langle 0  1  2  0 rangle rangle$ else move disk $rho ( i )$ backward $langle langle 0 to 2 to 1 to 0 rangle rangle$ $i gets i + 1$ \n2. The Tower of Hanoi is a relatively recent descendant of a much older mechanical puzzle known as the Chinese linked rings, Baguenaudier, Cardan’s Rings, Meleda, Patience, Tiring Irons, Prisoner’s Lock, Spin-Out, and many other names. This puzzle was already well known in both China and Europe by the 16th century. The Italian mathematician Luca Pacioli described the 7-ring puzzle and its solution in his unpublished treatise De Viribus Quantitatis, written between 1498 and 1506;13 only a few years later, the Ming-dynasty poet Yang Shen described the 9-ring puzzle as “a toy for women and children”. The puzzle is apocryphally attributed to a 2nd-century Chinese general, who gave the puzzle to his wife to occupy her time while he was away at war. \nThe Baguenaudier puzzle has many physical forms, but one of the most common consists of a long metal loop and several rings, which are connected to a solid base by movable rods. The loop is initially threaded through the rings as shown in Figure 1.16; the goal of the puzzle is to remove the loop. \nMore abstractly, we can model the puzzle as a sequence of bits, one for each ring, where the ith bit is 1 if the loop passes through the ith ring and 0 otherwise. (Here we index the rings from right to left, as shown in Figure 1.16.) The puzzle allows two legal moves: \n• You can always flip the 1st ( $mathbf { bar { Psi } } = mathbf { Psi }$ rightmost) bit.   \n• If the bit string ends with exactly $z  0 s$ , you can flip the $( z + 2 ) mathrm { t h }$ bit. \nThe goal of the puzzle is to transform a string of $n uparrow s$ into a string of $n  : 0 s$ . For example, the following sequence of 21 moves solves the 5-ring puzzle: \no(a) Call a sequence of moves reduced if no move is the inverse of the previous move. Prove that for any non-negative integer $n$ , there is exactly one reduced sequence of moves that solves the $n$ -ring Baguenaudier puzzle. [Hint: This problem is much easier if you’re already familiar with graphs.]   \n(b) Describe an algorithm to solve the Baguenaudier puzzle. Your input is the number of rings $n$ ; your algorithm should print a reduced sequence of moves that solves the puzzle. For example, given the integer 5 as input, your algorithm should print the sequence $1 , 3 , 1 , 2 , 1 , 5 , 1 , 2 , 1 , 3$ , $1 , 2 , 1 , 4 , 1 , 2 , 1 , 3 , 1 , 2 , 1$ .   \n(c) Exactly how many moves does your algorithm perform, as a function of n? Prove your answer is correct. \n3. A less familiar chapter in the Tower of Hanoi’s history is its brief relocation of the temple from Benares to Pisa in the early 13th century.14 The relocation was organized by the wealthy merchant-mathematician Leonardo Fibonacci, at the request of the Holy Roman Emperor Frederick II, who had heard reports of the temple from soldiers returning from the Crusades. The Towers of Pisa and their attendant monks became famous, helping to establish Pisa as a dominant trading center on the Italian peninsula. \nUnfortunately, almost as soon as the temple was moved, one of the diamond needles began to lean to one side. To avoid the possibility of the leaning tower falling over from too much use, Fibonacci convinced the priests to adopt a more relaxed rule: Any number of disks on the leaning needle can be moved together to another needle in a single move. It was still forbidden to place a larger disk on top of a smaller disk, and disks had to be moved one at a time onto the leaning needle or between the two vertical needles. \nThanks to Fibonacci’s new rule, the priests could bring about the end of the universe somewhat faster from Pisa than they could from Benares. Fortunately, the temple was moved from Pisa back to Benares after the newly crowned Pope Gregory IX excommunicated Frederick II, making the local priests less sympathetic to hosting foreign heretics with strange mathematical habits. Soon afterward, a bell tower was erected on the spot where the temple once stood; it too began to lean almost immediately. \nDescribe an algorithm to transfer a stack of $n$ disks from one vertical needle to the other vertical needle, using the smallest possible number of moves. Exactly how many moves does your algorithm perform? \n4. Consider the following restricted variants of the Tower of Hanoi puzzle In each problem, the pegs are numbered 0, 1, and 2, and your task is to move a stack of $n$ disks from peg 0 to peg 2, exactly as in problem 1. \n(a) Suppose you are forbidden to move any disk directly between peg 1 and peg 2; every move must involve peg 0. Describe an algorithm to solve this version of the puzzle in as few moves as possible. Exactly how many moves does your algorithm make? pn(b) Suppose you are only allowed to move disks from peg 0 to peg 2, from peg 2 to peg 1, or from peg 1 to peg 0. Equivalently, suppose the pegs are arranged in a circle and numbered in clockwise order, and you are only allowed to move disks counterclockwise. Describe an algorithm to solve this version of the puzzle in as few moves as possible. How many moves does your algorithm make? \nn(c) Finally, suppose your only restriction is that you may never move a disk directly from peg 0 to peg 2. Describe an algorithm to solve this version of the puzzle in as few moves as possible. How many moves does your algorithm make? [Hint: Matrices! This variant is considerably harder to analyze than the other two.] \n5. Consider the following more complex variant of the Tower of Hanoi puzzle The puzzle has a row of $k$ pegs, numbered from 1 to $k$ . In a single turn, you are allowed to move the smallest disk on peg $i$ to either peg $i - 1$ or peg $i + 1$ , for any index $i$ ; as usual, you are not allowed to place a bigger disk on a smaller disk. Your mission is to move a stack of $n$ disks from peg 1 to peg $k$ . \n(a) Describe a recursive algorithm for the case $k = 3$ . Exactly how many moves does your algorithm make? (This is exactly the same as problem 4(a).)   \n(b) Describe a recursive algorithm for the case $k = n + 1$ that requires at most $O ( n ^ { 3 } )$ moves. [Hint: Use part (a).]   \nn(c) Describe a recursive algorithm for the case $k = n + 1$ that requires at most $O ( n ^ { 2 } )$ moves. [Hint: Don’t use part (a).]   \nn(d) Describe a recursive algorithm for the case $k = { sqrt { n } }$ that requires at most a polynomial number of moves. (Which polynomial??)   \nn(e) Describe and analyze a recursive algorithm for arbitrary $n$ and $k$ . How small must $k$ be (as a function of $n$ ) so that the number of moves is bounded by a polynomial in $n ?$ \nRecursion Trees \n6. Use recursion trees to solve each of the following recurrences. \n7. Use recursion trees to solve each of the following recurrences. \n$^ { ast } 8$ . Use recursion trees to solve each of the following recurrences. \nSorting \n9. Suppose you are given a stack of $n$ pancakes of different sizes. You want to sort the pancakes so that smaller pancakes are on top of larger pancakes. The only operation you can perform is a flip—insert a spatula under the top $k$ pancakes, for some integer $k$ between 1 and $n$ , and flip them all over. \n(a) Describe an algorithm to sort an arbitrary stack of $n$ pancakes using $O ( n )$ flips. Exactly how many flips does your algorithm perform in the worst case?15 [Hint: This problem has nothing to do with the Tower of Hanoi.] \n(b) For every positive integer $n$ , describe a stack of $n$ pancakes that requires $Omega ( n )$ flips to sort.   \n(c) Now suppose one side of each pancake is burned. Describe an algorithm to sort an arbitrary stack of $n$ pancakes, so that the burned side of every pancake is facing down, using $O ( n )$ flips. Exactly how many flips does your algorithm perform in the worst case? \n10. Recall that the median-of-three heuristic examines the first, last, and middle element of the array, and uses the median of those three elements as a quicksort pivot. Prove that quicksort with the median-of-three heuristic requires $Omega ( n ^ { 2 } )$ time to sort an array of size $n$ in the worst case. Specifically, for any integer $n$ , describe a permutation of the integers 1 through $n$ , such that in every recursive call to median-of-three-quicksort, the pivot is always the second smallest element of the array. Designing this permutation requires intimate knowledge of the Partition subroutine. \n(a) As a warm-up exercise, assume that the Partition subroutine is stable, meaning it preserves the existing order of all elements smaller than the pivot, and it preserves the existing order of all elements smaller than the pivot.   \nn(b) Assume that the Partition subroutine uses the specific algorithm listed on page 29, which is not stable. \n11. (a) Hey, Moe! Hey, Larry! Prove that the following algorithm actually sorts its input! \n(b) Would StoogeSort still sort correctly if we replaced $m = lceil 2 n / 3 rceil$ with $begin{array} { r } { m = lfloor 2 n / 3 rfloor ? } end{array}$ Justify your answer.   \n(c) State a recurrence (including the base case(s)) for the number of comparisons executed by StoogeSort.   \n(d) Solve the recurrence, and prove that your solution is correct. [Hint: Ignore the ceiling.]   \n(e) Prove that the number of swaps executed by StoogeSort is at most $binom { n } { 2 }$ \n12. The following cruel and unusual sorting algorithm was proposed by Gary Miller: \nThe comparisons performed by this algorithm do not depend at all on the values in the input array; such a sorting algorithm is called oblivious. Assume for this problem that the input size $n$ is always a power of 2. \n(a) Prove by induction that Cruel correctly sorts any input array. [Hint: Consider an array that contains $n / 4 1 s , n / 4 2 s , n / 4 3 s ,$ and $n / 4 4 s$ . Why is this special case enough?]   \n(b) Prove that Cruel would not correctly sort if we removed the for-loop from Unusual.   \n(c) Prove that Cruel would not correctly sort if we swapped the last two lines of Unusual.   \n(d) What is the running time of Unusual? Justify your answer.   \n(e) What is the running time of Cruel? Justify your answer. \n13. An inversion in an array $A [ 1 ldots n ]$ is a pair of indices $( i , j )$ such that $i < j$ and $A [ i ] > A [ j ]$ . The number of inversions in an $n$ -element array is between 0 (if the array is sorted) and $binom { n } { 2 }$ (if the array is sorted backward). Describe and analyze an algorithm to count the number of inversions in an $n$ -element array in $O ( n log { n } )$ time. [Hint: Modify mergesort.] \n14. (a) Suppose you are given two sets of $n$ points, one set ${ p _ { 1 } , p _ { 2 } , . . . , p _ { n } }$ on the line $y = 0$ and the other set ${ q _ { 1 } , q _ { 2 } , ldots , q _ { n } }$ on the line $y = 1$ . Create a set of $n$ line segments by connect each point $p _ { i }$ to the corresponding point $q _ { i }$ . Describe and analyze a divide-and-conquer algorithm to determine how many pairs of these line segments intersect, in $O ( n log { n } )$ time. [Hint: See the previous problem.] (b) Now suppose you are given two sets ${ p _ { 1 } , p _ { 2 } , . . . , p _ { n } }$ and ${ q _ { 1 } , q _ { 2 } , ldots , q _ { n } }$ of $n$ points on the unit circle. Connect each point $p _ { i }$ to the corresponding point $q _ { i }$ . Describe and analyze a divide-and-conquer algorithm to determine how many pairs of these line segments intersect in $O ( n log ^ { 2 } n )$ time. [Hint: Use your solution to part (a).] \n\nn(c) Describe an algorithm for part (b) that runs in $O ( n log { n } )$ time. [Hint: Use your solution from part (b)!] \n15. (a) Describe an algorithm that sorts an input array $A [ 1 ldots n ]$ by calling a subroutine $mathsf { S Q R T S o R T } ( k )$ , which sorts the subarray $A { left[ k + 1 ldots k + { sqrt { n } } right] }$ in place, given an arbitrary integer $k$ between 0 and $n - { sqrt { n } }$ as input. (To simplify the problem, assume that $sqrt { n }$ is an integer.) Your algorithm is only allowed to inspect or modify the input array by calling SqrtSort; in particular, your algorithm must not directly compare, move, or copy array elements. How many times does your algorithm call SqrtSort in the worst case? p(b) Prove that your algorithm from part (a) is optimal up to constant factors. In other words, if $f ( n )$ is the number of times your algorithm calls SqrtSort, prove that no algorithm can sort using $o ( f ( n ) )$ calls to SqrtSort. (c) Now suppose SqrtSort is implemented recursively, by calling your sorting algorithm from part (a). For example, at the second level of recursion, the algorithm is sorting arrays roughly of size $n ^ { 1 / 4 }$ . What is the worst-case running time of the resulting sorting algorithm? (To simplify the analysis, assume that the array size $n$ has the form $2 ^ { 2 ^ { k } }$ , so that repeated square roots are always integers.) \nSelection \n16. Suppose we are given a set $s$ of $n$ items, each with a value and a weight. For any element $x in S$ , we define two subsets \n• $S _ { < x }$ is the set of elements of $s$ whose value is less than the value of $x$ . • $S _ { > x }$ is the set of elements of $s$ whose value is more than the value of $x$ \nFor any subset $R subseteq S$ , let $w ( R )$ denote the sum of the weights of elements in $R$ . The weighted median of $R$ is any element $x$ such that $w ( S _ { < x } ) leq w ( S ) / 2$ and $w ( S _ { > x } ) leq w ( S ) / 2$ . \nDescribe and analyze an algorithm to compute the weighted median of a given weighted set in $O ( n )$ time. Your input consists of two unsorted arrays $S [ 1 . . n ]$ and $W [ 1 ldots n ]$ , where for each index i, the ith element has value $s [ i ]$ and weight $W [ i ]$ . You may assume that all values are distinct and all weights are positive. \n17. (a) Describe an algorithm to determine in $O ( n )$ time whether an arbitrary array $A [ 1 ldots n ]$ contains more than $n / 4$ copies of any value. (b) Describe and analyze an algorithm to determine, given an arbitrary array $A [ 1 ldots n ]$ and an integer $k$ , whether $A$ contains more than $k$ copies of any value. Express the running time of your algorithm as a function of both $n$ and $k$ . \nDo not use hashing, or radix sort, or any other method that depends on the precise input values, as opposed to their order. \n18. Describe an algorithm to compute the median of an array $A [ 1 ldots 5 ]$ of distinct numbers using at most 6 comparisons. Instead of writing pseudocode, describe your algorithm using a decision tree: A binary tree where each internal node contains a comparison of the form $begin{array} { r } { { } ^ { * } A [ i ] gtrless A [ j ]  ? { } ^ { * } } end{array}$ and each leaf contains an index into the array. \n19. Consider the generalization of the Blum-Floyd-Pratt-Rivest-Tarjan MomSelect algorithm shown in Figure 1.22, which partitions the input array into $lceil n / b rceil$ blocks of size $b$ , instead of $lceil n / 5 rceil$ blocks of size 5, but is otherwise identical. \n(a) State a recurrence for the running time of $mathbf { M o u } _ { b }$ Select, assuming that $b$ is a constant (so the subroutine MedianOfB runs in $O ( 1 )$ time). In particular, how do the sizes of the recursive subproblems depend on the constant $b ?$ Consider even $b$ and odd $b$ separately. (b) What is the worst-case running time of $mathbf { M o u } _ { 1 }$ Select? [Hint: This is a trick question.] $triangleq left( mathrm { c } right)$ What is the worst-case running time of $mathbf { M o u } _ { 2 }$ Select? [Hint: This is an unfair question!] n(d) What is the worst-case running time of $mathrm { M o M _ { 3 } S E L E C T ? }$ Finding an upper bound on the running time is straightforward; the hard part is showing that this analysis is actually tight. [Hint: See problem 10.] n(e) What is the worst-case running time of $mathbf { M o M _ { 4 } S E L E C T ? }$ Again, the hard part is showing that the analysis cannot be improved.16 (f) For any constants $b geq 5$ , the algorithm $mathbf { M o u } _ { b }$ Select runs in $O ( n )$ time, but different values of $b$ lead to different constant factors. Let $M ( b )$ denote the minimum number of comparisons required to find the median of $b$ numbers. The exact value of $M ( b )$ is known only for $b leq 1 3$ : \nFor each $b$ between 5 and 13, find an upper bound on the running time of $mathrm { M o m } _ { b }$ Select of the form $T ( n ) leq a _ { b } n$ for some explicit constant $boldsymbol { alpha } _ { b }$ . (For example, on page 39 we showed that $alpha _ { 5 } leq 1 6 . $ (g) Which value of $b$ yields the smallest constant $boldsymbol { a _ { b } ? }$ [Hint: This is a trick question!] \n20. Prove that the variant of the Blum-Floyd-Pratt-Rivest-Tarjan Select algorithm shown in Figure 1.23, which uses an extra layer of small medians to choose the main pivot, runs in $O ( n )$ time. \n21. (a) Suppose we are given two sorted arrays $A [ 1 ldots n ]$ and $B [ 1 ldots n ]$ . Describe an algorithm to find the median element in the union of $A$ and $B$ in $Theta ( log n )$ time. You can assume that the arrays contain no duplicate elements. (b) Suppose we are given two sorted arrays $A [ 1 ldots m ]$ and $B [ 1 ldots n ]$ and an integer $k$ . Describe an algorithm to find the $k$ th smallest element in $A cup B$ in $Theta ( log ( m + n ) )$ time. For example, if $k = 1$ , your algorithm should return the smallest element of $A cup B$ .) [Hint: Use your solution to part (a).] n(c) Now suppose we are given three sorted arrays $A [ 1 ldots n ] , B [ 1 ldots n ] .$ and $C [ 1 ldots n ] _ { scriptscriptstyle { cdot } }$ , and an integer $k$ . Describe an algorithm to find the $k$ th smallest element in $A cup B cup C$ in $O ( log n )$ time. \n(d) Finally, suppose we are given a two dimensional array $A [ 1 ldots m , 1 ldots n ]$ in which every row $A [ i , cdot ]$ is sorted, and an integer $k$ . Describe an algorithm to find the kth smallest element in $A$ as quickly as possible. How does the running time of your algorithm depend on m? [Hint: Solve problem 16 first.] \nArithmetic \n22. In 1854, archaeologists discovered Sumerians clay tablets, carved around 2000bce, that list the squares of integers up to 59. This discovery led some scholars to conjecture that ancient Sumerians performed multiplication by reduction to squaring, using an identity like $x cdot y = ( x ^ { 2 } + y ^ { 2 } - ( x - y ) ^ { 2 } ) / 2$ . Unfortunately, those same scholars are silent on how the Sumerians supposedly squared larger numbers. Four thousand years later, we can finally rescue these Sumerian mathematicians from their lives of drudgery through the power of recursion! \n(a) Describe a variant of Karatsuba’s algorithm that squares any $n$ -digit number in $O ( n ^ { lg 3 } )$ time, by reducing to squaring three $lceil n / 2 rceil$ -digit numbers. (Karatsuba actually did this in 1960.)   \n(b) Describe a recursive algorithm that squares any $n$ -digit number in $O ( n ^ { log _ { 3 } 6 } )$ time, by reducing to squaring six $lceil n / 3 rceil$ -digit numbers.   \nn(c) Describe a recursive algorithm that squares any $n$ -digit number in $O ( n ^ { log _ { 3 } 5 } )$ time, by reducing to squaring only five $( n / 3 + O ( 1 ) )$ -digit numbers. [Hint: What is $( a + b + c ) ^ { 2 } + ( a - b + c ) ^ { 2 } ? J$ \n23. (a) Describe and analyze a variant of Karatsuba’s algorithm that multiplies any $m$ -digit number and any $n$ -digit number, for any $n geq m$ , in $O ( n m ^ { lg 3 - 1 } )$ time. (b) Describe an algorithm to compute the decimal representation of $2 ^ { n }$ in $O ( n ^ { lg 3 } )$ time, using the algorithm from part (a) as a subroutine. (The standard algorithm that computes one digit at a time requires $Theta ( n ^ { 2 } )$ time.) (c) Describe a divide-and-conquer algorithm to compute the decimal representation of an arbitrary $n$ -bit binary number in $O ( n ^ { lg 3 } )$ time. [Hint: Watch out for an extra log factor in the running time.] n(d) Suppose we can multiply two $n$ -digit numbers in $O ( M ( n ) )$ time. Describe an algorithm to compute the decimal representation of an arbitrary $n$ -bit binary number in $O ( M ( n ) log n )$ time. [Hint: The analysis is the hard part; use a domain transformation.] \n24. Consider the following classical recursive algorithm for computing the factorial $n !$ of a non-negative integer $n$ : \n(a) How many multiplications does this algorithm perform? \n(b) How many bits are required to write $n !$ in binary? Express your answer in the form $Theta ( f ( n ) )$ , for some familiar function $f ( n )$ . [Hint: $( n / 2 ) ^ { n / 2 } <$ $n ! < n ^ { n } . 7$   \n(c) Your answer to (b) should convince you that the number of multiplications is not a good estimate of the actual running time of Factorial. We can multiply any $k$ -digit number and any l-digit number in $O ( k cdot l )$ time using either the lattice algorithm or duplation and mediation. What is the running time of Factorial if we use this multiplication algorithm as a subroutine? \n(d) The following recursive algorithm also computes the factorial function, but using a different grouping of the multiplications: \nWhat is the running time of $mathrm { F A L L I N G } ( n , n )$ if we use grade-school multiplication? [Hint: As usual, ignore the floors and ceilings.] \n(e) Describe and analyze a variant of Karatsuba’s algorithm that multiplies any $k$ -digit number and any $l$ -digit number, for any $k geq l$ , in $O ( k cdot$ $l ^ { 1 8 3 - 1 } ) = O ( k cdot l ^ { 0 . 5 8 5 } )$ time.   \nn(f) What are the running times of Factorial $( n )$ and $mathrm { F A L L I N G } ( n , n )$ if we use the modified Karatsuba multiplication from part (e)? \n25. The greatest common divisor of two positive integer $x$ and $y$ , denoted $operatorname* { g c d } ( x , y )$ , is the largest integer $d$ such that both $x / d$ and $y / d$ are integers. Euclid’s Elements, written around 300bce, describes the following recursive algorithm to compute $operatorname* { g c d } ( x , y )$ : 17 \n(a) Prove that EuclidGCD correctly computes $operatorname* { g c d } ( x , y )$ .18 Specifically: \ni. Prove that EuclidGCD $( x , y )$ divides both $x$ and $y$ .   \nii. Prove that every divisor of $x$ and $y$ is a divisor of EuclidGCD $( x , y )$ . \n(b) What is the worst-case running time of EuclidGCD $( x , y )$ , as a function of $x$ and $y ?$ (Assume that computing $x - y$ requires $O ( log x + log y )$ time.) \n(c) Prove that the following algorithm also computes $operatorname* { g c d } ( x , y )$ : \n(d) What is the worst-case running time of FastEuclidGCD $( x , y )$ , as a function of $x$ and $y ?$ (Assume that computing $x$ mod $y$ takes $O ( log x cdot$ $log y .$ ) time.) \n(e) Prove that the following algorithm also computes $operatorname* { g c d } ( x , y )$ : \n(f) What is the worst-case running time of BinaryGCD $( x , y )$ , as a function of $x$ and $y ?$ (Assume that computing $x - y$ takes $O ( log x + log y )$ time, and computing $z / 2$ requires $O ( log z )$ time.) \nArrays \n26. Suppose you are given a $2 ^ { n } times 2 ^ { n }$ checkerboard with one (arbitrarily chosen) square removed. Describe and analyze an algorithm to compute a tiling of the board by without gaps or overlaps by L-shaped tiles, each composed of 3 squares. Your input is the integer $n$ and two $n$ -bit integers representing the row and column of the missing square. The output is a list of the positions and orientations of $( 4 ^ { n } - 1 ) / 3$ tiles. Your algorithm should run in $O ( 4 ^ { n } )$ time. [Hint: First prove that such a tiling always exists.] \n27. You are a visitor at a political convention (or perhaps a faculty meeting) with $n$ delegates; each delegate is a member of exactly one political party. It is impossible to tell which political party any delegate belongs to; in particular, you will be summarily ejected from the convention if you ask. However, you can determine whether any pair of delegates belong to the same party by introducing them to each other. Members of the same political party always greet each other with smiles and friendly handshakes; members of different parties always greet each other with angry stares and insults.19 \n(a) Suppose more than half of the delegates belong to the same political party. Describe an efficient algorithm that identifies all members of this majority party.   \n(b) Now suppose there are more than two parties, but one party has a plurality: more people belong to that party than to any other party. Present a practical procedure to precisely pick the people from the plurality political party as parsimoniously as possible, presuming the plurality party is composed of at least $p$ people. Pretty please. \n28. Smullyan Island has three types of inhabitants: knights always speak the truth; knaves always lie; and normals sometimes speak the truth and sometimes don’t. Everyone on the island knows everyone else’s name and type (knight, knave, or normal). You want to learn the type of every inhabitant. \nYou can ask any inhabitant to tell you the type of any other inhabitant. Specifically, if you ask “Hey $X$ , what is Y ’s type?” then $X$ will respond as follows: \n• If $X$ is a knight, then $X$ will respond with Y ’s correct type. • If $X$ is a knave, then $X$ could respond with either of the types that $Y$ is not. • If $X$ is a normal, then $X$ could respond with any of the three types. \nThe inhabitants will ignore any questions not of this precise form; in particular, you may not ask an inhabitant about their own type. Asking the same inhabitant the same question multiple times always yields the same answer, so there’s no point in asking any question more than once. \n(a) Suppose you know that a strict majority of inhabitants are knights. Describe an efficient algorithm to identify the type of every inhabitant.   \n(b) Prove that if at most half the inhabitants are knights, it is impossible to determine the type of every inhabitant. \n29. Most graphics hardware includes support for a low-level operation called blit, or block transfer, which quickly copies a rectangular chunk of a pixel map (a two-dimensional array of pixel values) from one location to another. This is a two-dimensional version of the standard C library function memcpy(). \nSuppose we want to rotate an $n times n$ pixel map $9 0 ^ { circ }$ clockwise. One way to do this, at least when $n$ is a power of two, is to split the pixel map into four $n / 2 times n / 2$ blocks, move each block to its proper position using a sequence of five blits, and then recursively rotate each block. (Why five? For the same reason the Tower of Hanoi puzzle needs a third peg.) Alternately, we could first recursively rotate the blocks and then blit them into place. \n(a) Prove that both versions of the algorithm are correct when $n$ is a power of 2.   \n(b) Exactly how many blits does the algorithm perform when $n$ is a power of 2?   \n(c) Describe how to modify the algorithm so that it works for arbitrary $n$ , not just powers of 2. How many blits does your modified algorithm perform?   \n(d) What is your algorithm’s running time if a $k times k$ blit takes $O ( k ^ { 2 } )$ time?   \n(e) What if a $k times k$ blit takes only $O ( k )$ time? \n30. An array $A [ 0 ldots n - 1 ]$ of $n$ distinct numbers is bitonic if there are unique indices $i$ and $j$ such that $A [ ( i - 1 ) { bmod { n } } ] < A [ i ] > A [ ( i + 1 ) { bmod { n } } ]$ and \n$A [ ( j - 1 ) { bmod { n } } ] > A [ j ] < A [ ( j + 1 ) { bmod { n } } ]$ . In other words, a bitonic sequence either consists of an increasing sequence followed by a decreasing sequence, or can be circularly shifted to become so. For example, \nDescribe and analyze an algorithm to find the smallest element in an $n$ - element bitonic array in $O ( log n )$ time. You may assume that the numbers in the input array are distinct. \n31. Suppose we are given an array $A [ 1 ldots n ]$ of $n$ distinct integers, which could be positive, negative, or zero, sorted in increasing order so that $A [ 1 ] < A [ 2 ] <$ $cdots < A [ n ]$ . \n(a) Describe a fast algorithm that either computes an index $i$ such that $A [ i ] = i$ or correctly reports that no such index exists.   \n(b) Suppose we know in advance that $A [ 1 ] > 0$ . Describe an even faster algorithm that either computes an index i such that $A [ i ] = i$ or correctly reports that no such index exists. [Hint: This is really easy.] \n32. Suppose we are given an array $A [ 1 ldots n ]$ with the special property that $A [ 1 ] geq A [ 2 ]$ and $A [ n - 1 ] leq A [ n ]$ . We say that an element $A [ x ]$ is a local minimum if it is less than or equal to both its neighbors, or more formally, if $A [ x - 1 ] geq A [ x ]$ and $A [ x ] leq A [ x + 1 ]$ . For example, there are six local minima in the following array: \n9 7 7 2 1 3 7 5 4 7 3 3 4 8 6 9 ▲ ▲ ▲ ▲ ▲ ▲ \nWe can obviously find a local minimum in $O ( n )$ time by scanning through the array. Describe and analyze an algorithm that finds a local minimum in $O ( log n )$ time. [Hint: With the given boundary conditions, the array must have at least one local minimum. Why?] \n33. Suppose you are given a sorted array of $n$ distinct numbers that has been rotated $k$ steps, for some unknown integer $k$ between 1 and $n - 1$ . That is, you are given an array $A [ 1 ldots n ]$ such that some prefix $A [ 1 ldots k ]$ is sorted in increasing order, the corresponding suffix $A [ k + 1 ldots n ]$ is sorted in increasing order, and $A [ n ] < A [ 1 ]$ . \nFor example, you might be given the following 16-element array (where $k = 1 0 mathrm {  : }$ ): \n(a) Describe and analyze an algorithm to compute the unknown integer $k$ . (b) Describe and analyze an algorithm to determine if the given array contains a given number $x$ . \n34. At the end of the second act of the action blockbuster Fast and Impossible XIII¾: The Last Guardians of Expendable Justice Reloaded,the villainous Dr. Metaphor hypnotizes the entire Hero League/Force/Squad, arranges them in a long line at the edge of a cliff, and instructs each hero to shoot the closest taller heroes to their left and right, at a prearranged signal. \nSuppose we are given the heights of all $n$ heroes, in order from left to right, in an array $H t [ 1 ldots n ]$ . (To avoid salary arguments, the producers insisted that no two heroes have the same height.) Then we can compute the Left and Right targets of each hero in $O ( n ^ { 2 } )$ time using the following brute-force algorithm. \n(a) Describe a divide-and-conquer algorithm that computes the output of WhoTargetsWhom in $O ( n log { n } )$ time.   \n(b) Prove that at least $lfloor n / 2 rfloor$ of the $n$ heroes are targets. That is, prove that the output arrays $R [ 0 ldots n - 1 ]$ and $L [ 0 ldots n - 1 ]$ contain at least $lfloor n / 2 rfloor$ distinct values (other than None).   \n(c) Alas, Dr. Metaphor’s diabolical plan is successful. At the prearranged signal, all the heroes simultaneously shoot their targets, and all targets fall over the cliff, apparently dead. Metaphor repeats his dastardly experiment over and over; after each massacre, he forces the remaining heroes to choose new targets, following the same algorithm, and then shoot their targets at the next signal. Eventually, only the shortest member of the Hero Crew/Alliance/Posse is left alive.20 \nDescribe and analyze an algorithm to compute the number of rounds before Dr. Metaphor’s deadly process finally ends. For full credit, your algorithm should run in $O ( n )$ time. \n35. You are a contestant on the hit game show “Beat Your Neighbors!” You are presented with an $m times n$ grid of boxes, each containing a unique number. It costs $$ 100$ to open a box. Your goal is to find a box whose number is larger than its neighbors in the grid (above, below, left, and right). If you spend less money than any of your opponents, you win a week-long trip for two to Las Vegas and a year’s supply of Rice-A-Ronitm, to which you are hopelessly addicted. \n(a) Suppose $m = 1$ . Describe an algorithm that finds a number that is bigger than either of its neighbors. How many boxes does your algorithm open in the worst case? n(b) Suppose $m = n$ . Describe an algorithm that finds a number that is bigger than any of its neighbors. How many boxes does your algorithm open in the worst case? ${ } ^ { mathsf { a w } } ( { mathsf { c } } )$ Prove that your solution to part (b) is optimal up to a constant factor. \n36. (a) Let $n = 2 ^ { ell } - 1$ for some positive integer $ell$ . Suppose someone claims to hold an unsorted array $A [ 1 ldots n ]$ of distinct $ell$ -bit strings; thus, exactly one $ell$ -bit string does not appear in $A$ . Suppose further that the only way we can access $A$ is by calling the function $mathrm { F E T C H B I T } ( i , j )$ , which returns the $j$ th bit of the string $A [ i ]$ in $O ( 1 )$ time. Describe an algorithm to find the missing string in $A$ using only $O ( n )$ calls to FetchBit. \nn(b) Now suppose $n = 2 ^ { ell } - k$ for some positive integers $k$ and $ell$ , and again we are given an array $A [ 1 ldots n ]$ of distinct $ell$ -bit strings. Describe an algorithm to find the $k$ strings that are missing from $A$ using only $O ( n log k )$ calls to FetchBit. \nTrees \n37. For this problem, a subtree of a binary tree means any connected subgraph. A binary tree is complete if every internal node has two children, and every leaf has exactly the same depth. Describe and analyze a recursive algorithm to compute the largest complete subtree of a given binary tree. Your algorithm should return both the root and the depth of this subtree. See Figure 1.26 for an example. \n38. Let $T$ be a binary tree with $n$ vertices. Deleting any vertex $nu$ splits $T$ into at most three subtrees, containing the left child of $nu$ (if any), the right child of $nu$ (if any), and the parent of $nu$ (if any). We call $nu$ a central vertex if each of these smaller trees has at most $n / 2$ vertices. See Figure 1.27 for an example. \nDescribe and analyze an algorithm to find a central vertex in an arbitrary given binary tree. [Hint: First prove that every tree has a central vertex.] \n39. (a) Professor George O’Jungle has a 27-node binary tree, in which every node is labeled with a unique letter of the Roman alphabet or the character &. Preorder and postorder traversals of the tree visit the nodes in the following order: \n• Preorder: I Q J H L E M V O T S B R G Y Z K C A & F P N U D W X • Postorder: H E M L J V Q S G Y R Z B T C P U D N F W & X A K O I \nDraw George’s binary tree. \n(b) Recall that a binary tree is full if every non-leaf node has exactly two children. i. Describe and analyze a recursive algorithm to reconstruct an arbitrary full binary tree, given its preorder and postorder node sequences as input. ii. Prove that there is no algorithm to reconstruct an arbitrary binary tree from its preorder and postorder node sequences.   \n(c) Describe and analyze a recursive algorithm to reconstruct an arbitrary binary tree, given its preorder and inorder node sequences as input.   \n(d) Describe and analyze a recursive algorithm to reconstruct an arbitrary binary search tree, given only its preorder node sequence.   \nn(e) Describe and analyze a recursive algorithm to reconstruct an arbitrary binary search tree, given only its preorder node sequence, in $O ( n )$ time. \nIn parts (b)–(e), assume that all keys are distinct and that the input is consistent with at least one binary tree. \n40. Suppose we have n points scattered inside a two-dimensional box. A kdtree21 recursively subdivides the points as follows. If the box contains no points in its interior, we are done. Otherwise, we split the box into two smaller boxes with a vertical line, through a median point inside the box (not on its boundary), partitioning the points as evenly as possible. Then we recursively build a kd-tree for the points in each of the two smaller boxes, after rotating them 90 degrees. Thus, we alternate between splitting vertically and splitting horizontally at each level of recursion. The final empty boxes are called cells. \n(a) How many cells are there, as a function of $n ?$ Prove your answer is correct.   \n(b) In the worst case, exactly how many cells can a horizontal line cross, as a function of $n ?$ Prove your answer is correct. Assume that $n = 2 ^ { k } - 1$ for some integer $k$ . [Hint: There is more than one function $f$ such that $f ( 1 6 ) = 4 . 7$   \n(c) Suppose we are given $n$ points stored in a kd-tree. Describe and analyze an algorithm that counts the number of points above a horizontal line (such as the dashed line in the figure) as quickly as possible. [Hint: Use part $( b ) . jmath$   \n(d) Describe an analyze an efficient algorithm that counts, given a kd-tree containing $n$ points, the number of points that lie inside a rectangle $R$ with horizontal and vertical sides. [Hint: Use part (c).] \nn41. Bob Ratenbur, a new student in CS 225, is trying to write code to perform preorder, inorder, and postorder traversals of binary trees. Bob sort-of understands the basic idea behind the traversal algorithms, but whenever he actually tries to implement them, he keeps mixing up the recursive calls. Five minutes before the deadline, Bob frantically submits code with the following structure: \nPreOrder(v): InOrder(v): PostOrder(v): if v = Null if v = Null if v = Null return return return else else else print label(v) Order(left(v)) Order(left(v)) Order(left(v)) print label(v) Order(right(v)) Order(right(v)) Order(right(v)) print label(v) \nEach in this pseudocode hides one of the prefixes Pre, In, or Post. Moreover, each of the following function calls appears exactly once in Bob’s submitted code: \nPreOrder(left(v)) PreOrder(right(v)) InOrder(left(v)) InOrder(right(v)) PostOrder(left(v)) PostOrder(right(v)) \nThus, there are precisely 36 possibilities for Bob’s code. Unfortunately, Bob accidentally deleted his source code after submitting the executable, so neither you nor he knows which functions were called where. \nNow suppose you are given the output of Bob’s traversal algorithms, executed on some unknown binary tree $T$ . Bob’s output has been helpfully parsed into three arrays $P r e [ 1 ldots n ] , I n [ 1 ldots n ] $ , and $P o s t [ 1 ldots n ]$ . You may assume that these traversal sequences are consistent with exactly one binary tree $T$ ; in particular, the vertex labels of the unknown tree $T$ are distinct, and every internal node in $T$ has exactly two children. \n(a) Describe an algorithm to reconstruct the unknown tree $T$ from the given traversal sequences.   \n(b) Describe an algorithm that either reconstructs Bob’s code from the given traversal sequences, or correctly reports that the traversal sequences are consistent with more than one set of algorithms. \nFor example, given the input \nyour first algorithm should return the following tree: \nand your second algorithm should reconstruct the following code: \n$^ { bullet } 4 2$ . Let $T$ be a binary tree whose nodes store distinct numerical values. Recall that $T$ is a binary search tree if and only if either (1) $T$ is empty, or (2) $T$ satisfies the following recursive conditions: \n• The left subtree of $T$ is a binary search tree.   \n• All values in the left subtree are smaller than the value at the root.   \n• The right subtree of $T$ is a binary search tree.   \n• All values in the right subtree are larger than the value at the root. \nConsider the following pair of operations on binary trees: \n• Rotate an arbitrary node upward.22 • Swap the left and right subtrees of an arbitrary node. \n\nIn both of these operations, some, all, or none of the subtrees $A , B$ , and $C$ could be empty. \n(a) Describe an algorithm to transform an arbitrary $n$ -node binary tree with distinct node values into a binary search tree, using at most $O ( n ^ { 2 } )$ rotations and swaps. Figure 1.29 shows a sequence of eight operations that transforms a five-node binary tree into a binary search tree. \nYour algorithm is not allowed to directly modify parent or child pointers, create new nodes, or delete old nodes; the only way to modify the tree is through rotations and swaps. \nOn the other hand, you may compute anything you like for free, as long as that computation does not modify the tree; the running time of your algorithm is defined to be the number of rotations and swaps that it performs. \nn(b) Describe an algorithm to transform an arbitrary $n$ -node binary tree into a binary search tree, using at most $O ( n log { n } )$ rotations and swaps. \n(c) Prove that any $n$ -node binary search tree can be transformed into any other binary search tree with the same node values, using only $O ( n )$ rotations (and no swaps).   \nn(d) Open problem: Either describe an algorithm to transform an arbitrary $n$ -node binary tree into a binary search tree using only $O ( n )$ rotations and swaps, or prove that no such algorithm is possible. [Hint: I don’t think it’s possible.] \nWhere, however, the ambiguity cannot be cleared up, either by the rule of faith or by the context, there is nothing to hinder us to point the sentence according to any method we choose of those that suggest themselves. \n— Augustine of Hippo, De doctrina Christiana (397CE) Translated by Marcus Dods (1892) \nI dropped my dinner, and ran back to the laboratory. There, in my excitement, I tasted the contents of every beaker and evaporating dish on the table. Luckily for me, none contained any corrosive or poisonous liquid. \n— Constantine Fahlberg on his discovery of saccharin, Scientific American (1886) \nThe greatest challenge to any thinker is stating the problem in a way that will allow a solution. \n— attributed to Bertrand Russell \nWhen you come to a fork in the road, take it. \n— Yogi Berra (giving directions to his house) \n2 \nBacktracking \nThis chapter describes another important recursive strategy called backtracking. A backtracking algorithm tries to construct a solution to a computational problem incrementally, one small piece at a time. Whenever the algorithm needs to decide between multiple alternatives to the next component of the solution, it recursively evaluates every alternative and then chooses the best one. \n2.1 N Queens \nThe prototypical backtracking problem is the classical $pmb { n }$ Queens Problem, first proposed by German chess enthusiast Max Bezzel in 1848 (under his pseudonym “Schachfreund”) for the standard $8 times 8$ board and by François-Joseph Eustache Lionnet in 1869 for the more general $n times n$ board. The problem is to place $n$ queens on an $n times n$ chessboard, so that no two queens are attacking each other.",
        "chapter": "Recursion",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 26
      }
    },
    {
      "node_id": "tb1_node57",
      "content": "The point is, ladies and gentleman, greed is good. Greed works, greed is right. Greed clarifies, cuts through, and captures the essence of the evolutionary spirit. Greed in all its forms, greed for life, money, love, knowledge has marked the upward surge in mankind. And greed—mark my words—will save not only Teldar Paper but the other malfunctioning corporation called the USA. \n— Gordon Gekko [Michael Douglas], Wall Street (1987) \nThere is always an easy solution to every human problem— neat, plausible, and wrong. \n— H. L. Mencken, “The Divine Afflatus”, New York Evening Mail (November 16, 1917) \nGreedy Algorithms \n4.1 Storing Files on Tape \nSuppose we have a set of $n$ files that we want to store on magnetic tape.1 In the future, users will want to read those files from the tape. Reading a file from tape isn’t like reading a file from disk; first we have to fast-forward past all the other files, and that takes a significant amount of time. Let $textstyle L [ 1 ldots n ]$ be an array listing the lengths of each file; specifically, file $i$ has length $L [ i ]$ . If the files are stored in order from 1 to $n$ , then the cost of accessing the $k$ th file is \nThe cost reflects the fact that before we read file $k$ we must first scan past all the earlier files on the tape. If we assume for the moment that each file is equally likely to be accessed, then the expected cost of searching for a random file is \nIf we change the order of the files on the tape, we change the cost of accessing the files; some files become more expensive to read, but others become cheaper. Different file orders are likely to result in different expected costs. Specifically, let $pi ( i )$ denote the index of the file stored at position i on the tape. Then the expected cost of the permutation $pi$ is \nWhich order should we use if we want this expected cost to be as small as possible? The answer seems intuitively clear: Sort the files by increasing length. But intuition is a tricky beast. The only way to be sure that this order works is to take off and nuke the entire site from orbit actually prove that it works! \nLemma 4.1. $operatorname { E } [ c o s t ( { pi } ) ]$ is minimized when $L [ pi ( i ) ] leq L [ pi ( i + 1 ) ]$ for all i. \nProof: Suppose $L [ pi ( i ) ] > L [ pi ( i + 1 ) ]$ for some index i. To simplify notation, let $a = pi ( i )$ and $b = pi ( i + 1 )$ . If we swap files $a$ and $b$ , then the cost of accessing $a$ increases by $L [ b ]$ , and the cost of accessing $b$ decreases by $boldsymbol { L } [ boldsymbol { a } ]$ . Overall, the swap changes the expected cost by $( L [ b ] - L [ a ] ) / n$ . But this change is an improvement, because $L [ b ] < L [ a ]$ . Thus, if the files are out of order, we can decrease the expected cost by swapping some mis-ordered pair of files. □ \nThis is our first example of a correct greedy algorithm. To minimize the total expected cost of accessing the files, we put the file that is cheapest to access first, and then recursively write everything else; no backtracking, no dynamic programming, just make the best local choice and blindly plow ahead. If we use an efficient sorting algorithm, the running time is clearly $O ( n log { n } )$ , plus the time required to actually write the files. To show that the greedy algorithm is actually correct, we proved that the output of any other algorithm can be improved by some sort of exchange \nLet’s generalize this idea further. Suppose we are also given an array $F [ 1 ldots n ]$ of access frequencies for each file; file i will be accessed exactly $F [ i ]$ times over the lifetime of the tape. Now the total cost of accessing all the files on the tape is \nAs before, reordering the files can change this total cost. So what order should we use if we want the total cost to be as small as possible? (This question is similar in spirit to the optimal binary search tree problem, but the target data structure and the cost function are both different, so the algorithm must be different, too.) \nWe already proved that if all the frequencies are equal, we should sort the files by increasing size. If the frequencies are all different but the file lengths $boldsymbol { L } [ i ]$ are all equal, then intuitively, we should sort the files by decreasing access frequency, with the most-accessed file first. In fact, this is not hard to prove (hint, hint) by modifying the proof of Lemma 4.1. But what if the sizes and the frequencies both vary? In this case, we should sort the files by the ratio $L / F$ . \nLemma 4.2. $Sigma c o s t ( pi )$ is minimized when $frac { L [ pi ( i ) ] } { F [ pi ( i ) ] } le frac { L [ pi ( i + 1 ) ] } { F [ pi ( i + 1 ) ] }  }$ for all i. \nProof: Suppose $L [ pi ( i ) ] / F [ pi ( i ) ] > L [ pi ( i + 1 ) ] / F [ pi ( i + i ) ]$ for some index i. To simplify notation, let $a = pi ( i )$ and $b = pi ( i { + } 1 )$ . If we swap files $a$ and $b$ , then the cost of accessing $a$ increases by $L [ b ]$ , and the cost of accessing $b$ decreases by $boldsymbol { L } [ boldsymbol { a } ]$ . Overall, the swap changes the total cost by $L [ b ] F [ a ] - L [ a ] F [ b ]$ . But this change is an improvement, because \nThus, if any two adjacent files are out of order, we can improve the total cost by swapping them. □ \n4.2 Scheduling Classes \nThe next example is slightly more complex. Suppose you decide to drop out of computer science and change your major to Applied Chaos. The Applied Chaos department offers all of its classes on the same day every week, called “Soberday” by the students (but interestingly, not by the faculty). Every class has a different start time and a different ending time: AC 101 (“Toilet Paper Landscape Architecture”) starts at 10:27pm and ends at 11:51pm; AC 666 (“Immanentizing the Eschaton”) starts at 4:18pm and ends at 4:22pm, and so on. In the interest of graduating as quickly as possible, you want to register for as many classes as possible. (Applied Chaos classes don’t require any actual work.) The university’s registration computer won’t let you register for overlapping classes, and no one in the department knows how to override this “feature”. Which classes should you take? \nMore formally, suppose you are given two arrays $S [ 1 . . n ]$ and $F [ 1 ldots n ]$ listing the start and finish times of each class; to be concrete, we can assume that",
      "metadata": {
        "content": "The point is, ladies and gentleman, greed is good. Greed works, greed is right. Greed clarifies, cuts through, and captures the essence of the evolutionary spirit. Greed in all its forms, greed for life, money, love, knowledge has marked the upward surge in mankind. And greed—mark my words—will save not only Teldar Paper but the other malfunctioning corporation called the USA. \n— Gordon Gekko [Michael Douglas], Wall Street (1987) \nThere is always an easy solution to every human problem— neat, plausible, and wrong. \n— H. L. Mencken, “The Divine Afflatus”, New York Evening Mail (November 16, 1917) \nGreedy Algorithms \n4.1 Storing Files on Tape \nSuppose we have a set of $n$ files that we want to store on magnetic tape.1 In the future, users will want to read those files from the tape. Reading a file from tape isn’t like reading a file from disk; first we have to fast-forward past all the other files, and that takes a significant amount of time. Let $textstyle L [ 1 ldots n ]$ be an array listing the lengths of each file; specifically, file $i$ has length $L [ i ]$ . If the files are stored in order from 1 to $n$ , then the cost of accessing the $k$ th file is \nThe cost reflects the fact that before we read file $k$ we must first scan past all the earlier files on the tape. If we assume for the moment that each file is equally likely to be accessed, then the expected cost of searching for a random file is \nIf we change the order of the files on the tape, we change the cost of accessing the files; some files become more expensive to read, but others become cheaper. Different file orders are likely to result in different expected costs. Specifically, let $pi ( i )$ denote the index of the file stored at position i on the tape. Then the expected cost of the permutation $pi$ is \nWhich order should we use if we want this expected cost to be as small as possible? The answer seems intuitively clear: Sort the files by increasing length. But intuition is a tricky beast. The only way to be sure that this order works is to take off and nuke the entire site from orbit actually prove that it works! \nLemma 4.1. $operatorname { E } [ c o s t ( { pi } ) ]$ is minimized when $L [ pi ( i ) ] leq L [ pi ( i + 1 ) ]$ for all i. \nProof: Suppose $L [ pi ( i ) ] > L [ pi ( i + 1 ) ]$ for some index i. To simplify notation, let $a = pi ( i )$ and $b = pi ( i + 1 )$ . If we swap files $a$ and $b$ , then the cost of accessing $a$ increases by $L [ b ]$ , and the cost of accessing $b$ decreases by $boldsymbol { L } [ boldsymbol { a } ]$ . Overall, the swap changes the expected cost by $( L [ b ] - L [ a ] ) / n$ . But this change is an improvement, because $L [ b ] < L [ a ]$ . Thus, if the files are out of order, we can decrease the expected cost by swapping some mis-ordered pair of files. □ \nThis is our first example of a correct greedy algorithm. To minimize the total expected cost of accessing the files, we put the file that is cheapest to access first, and then recursively write everything else; no backtracking, no dynamic programming, just make the best local choice and blindly plow ahead. If we use an efficient sorting algorithm, the running time is clearly $O ( n log { n } )$ , plus the time required to actually write the files. To show that the greedy algorithm is actually correct, we proved that the output of any other algorithm can be improved by some sort of exchange \nLet’s generalize this idea further. Suppose we are also given an array $F [ 1 ldots n ]$ of access frequencies for each file; file i will be accessed exactly $F [ i ]$ times over the lifetime of the tape. Now the total cost of accessing all the files on the tape is \nAs before, reordering the files can change this total cost. So what order should we use if we want the total cost to be as small as possible? (This question is similar in spirit to the optimal binary search tree problem, but the target data structure and the cost function are both different, so the algorithm must be different, too.) \nWe already proved that if all the frequencies are equal, we should sort the files by increasing size. If the frequencies are all different but the file lengths $boldsymbol { L } [ i ]$ are all equal, then intuitively, we should sort the files by decreasing access frequency, with the most-accessed file first. In fact, this is not hard to prove (hint, hint) by modifying the proof of Lemma 4.1. But what if the sizes and the frequencies both vary? In this case, we should sort the files by the ratio $L / F$ . \nLemma 4.2. $Sigma c o s t ( pi )$ is minimized when $frac { L [ pi ( i ) ] } { F [ pi ( i ) ] } le frac { L [ pi ( i + 1 ) ] } { F [ pi ( i + 1 ) ] }  }$ for all i. \nProof: Suppose $L [ pi ( i ) ] / F [ pi ( i ) ] > L [ pi ( i + 1 ) ] / F [ pi ( i + i ) ]$ for some index i. To simplify notation, let $a = pi ( i )$ and $b = pi ( i { + } 1 )$ . If we swap files $a$ and $b$ , then the cost of accessing $a$ increases by $L [ b ]$ , and the cost of accessing $b$ decreases by $boldsymbol { L } [ boldsymbol { a } ]$ . Overall, the swap changes the total cost by $L [ b ] F [ a ] - L [ a ] F [ b ]$ . But this change is an improvement, because \nThus, if any two adjacent files are out of order, we can improve the total cost by swapping them. □ \n4.2 Scheduling Classes \nThe next example is slightly more complex. Suppose you decide to drop out of computer science and change your major to Applied Chaos. The Applied Chaos department offers all of its classes on the same day every week, called “Soberday” by the students (but interestingly, not by the faculty). Every class has a different start time and a different ending time: AC 101 (“Toilet Paper Landscape Architecture”) starts at 10:27pm and ends at 11:51pm; AC 666 (“Immanentizing the Eschaton”) starts at 4:18pm and ends at 4:22pm, and so on. In the interest of graduating as quickly as possible, you want to register for as many classes as possible. (Applied Chaos classes don’t require any actual work.) The university’s registration computer won’t let you register for overlapping classes, and no one in the department knows how to override this “feature”. Which classes should you take? \nMore formally, suppose you are given two arrays $S [ 1 . . n ]$ and $F [ 1 ldots n ]$ listing the start and finish times of each class; to be concrete, we can assume that",
        "chapter": "Greedy Algorithms",
        "section": "Storing Files on Tape",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 57
      }
    }
  ],
  "output": {
    "question": "Based on the provided context, which sorting algorithm is described as having the property that after splitting the array into sorted subarrays, merging them is trivial?",
    "correct_answer": "Quicksort",
    "distractors": [
      "Mergesort",
      "Bubble sort",
      "Insertion sort"
    ],
    "rationale": "Step 1: Identify relevant snippets about sorting algorithms. Snippet [2] explicitly describes Quicksort as a recursive sorting algorithm where 'the hard work is splitting the array into smaller subarrays before recursion, so that merging the sorted subarrays is trivial.' Step 2: Verify no other sorting algorithm in the context has this property. Snippet [1] discusses merging sorted subarrays but doesn't name the algorithm; however, the description matches Mergesort's approach of splitting then merging, not Quicksort's approach. Step 3: Combine information: The question focuses on the specific property from snippet [2] about trivial merging after splitting, which uniquely identifies Quicksort in this context. Step 4: Create distractors using common sorting algorithms not mentioned in the context (Bubble sort, Insertion sort) and one algorithm (Mergesort) that involves merging but doesn't have trivial merging as described."
  },
  "score": {
    "relevance": 1.0,
    "faithfulness": 1.0,
    "integration": 0.0,
    "complexity": 0.0,
    "overall": 0.30000000000000004
  }
}
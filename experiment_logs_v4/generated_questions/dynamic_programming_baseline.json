{
  "context": [
    {
      "node_id": "tb1_node33",
      "content": "We are now left with a simple and natural backtracking strategy: Select the first output word, and recursively segment the rest of the input string. \nTo get a complete recursive algorithm, we need a base case. Our recursive strategy breaks down when we reach the end of the input string, because there is no next word. Fortunately, the empty string has a unique segmentation into zero words! \nPutting all the pieces together, we arrive at the following simple recursive algorithm: \nIndex Formulation \nIn practice, passing arrays as input parameters is rather slow; we should really find a more compact way to describe our recursive subproblems. For purposes of designing the algorithm, it’s incredibly useful to treat the original input array as a global variable, and then reformulate the problem and the algorithm in terms of array indices instead of explicit subarrays. \nFor our string segmentation problem, the argument of any recursive call is always a suffix $A [ i ldots n ]$ of the original input array. So if we treat the input array $A [ 1 ldots n ]$ as a global variable, we can reformulate our recursive problem as follows: \nGiven an index $i$ , find a segmentation of the suffix $A [ i ldots n ]$ . \nTo describe our algorithm, we need two boolean functions: \n• For any indices $i$ and $j$ , let $mathrm { I s W o R D } ( i , j ) = mathrm { T R U E }$ if and only if the substring $A [ i . . j ]$ is a word. (We’re assuming this function is given to us.) • For any index $i$ , let Splittable $( i ) =$ True if and only if the suffix $A [ i ldots n ]$ can be split into words. (This is the function we need to implement.) \nFor example, $mathrm { I s W o R D } ( 1 , n ) = mathrm { T R U E }$ if and only if the entire input string is a single word, and Splittable $( 1 ) =$ True if and only if the entire input string can be segmented. Our earlier recursive strategy gives us the following recurrence: \nThis is exactly the same algorithm as we saw earlier; the only thing we’ve changed is the notation. The similarity is even more apparent if we rewrite the recurrence in pseudocode: \nAlthough it may look like a trivial notational difference, using index notation instead of array notation is an important habit, not only to speed up backtracking algorithms in practice, but for developing dynamic programming algorithms, which we discuss in the next chapter. \nnAnalysis \nIt should come as no surprise that most backtracking algorithms have exponential worst-case running times. Analyzing the precise running times of many of these algorithms requires techniques that are beyond the scope of this book. Fortunately, most of the backtracking algorithms we will encounter in this book are only intermediate results on the way to more efficient algorithms, which means their exact worst-case running time is not actually important. (First make it work; then make it fast.) \nBut just for fun, let’s analyze the running time of our recursive algorithm Splittable. Because we don’t know what IsWord is doing, we can’t know how long each call to IsWord takes, so we’re forced to analyze the running time in terms of the number of calls to IsWord.11 Splittable calls IsWord on every prefix of the input string, and possibly calls itself recursively on every suffix of the output string. Thus, the “running time” of Splittable obeys the scary-looking recurrence \nThis really isn’t as bad as it looks, especially once you’ve seen the trick. \nFirst, we replace the $O ( n )$ term with an explicit expression $a n$ , for some unknown (and ultimately unimportant) constant $alpha$ . Second, we conservatively",
      "metadata": {
        "content": "We are now left with a simple and natural backtracking strategy: Select the first output word, and recursively segment the rest of the input string. \nTo get a complete recursive algorithm, we need a base case. Our recursive strategy breaks down when we reach the end of the input string, because there is no next word. Fortunately, the empty string has a unique segmentation into zero words! \nPutting all the pieces together, we arrive at the following simple recursive algorithm: \nIndex Formulation \nIn practice, passing arrays as input parameters is rather slow; we should really find a more compact way to describe our recursive subproblems. For purposes of designing the algorithm, it’s incredibly useful to treat the original input array as a global variable, and then reformulate the problem and the algorithm in terms of array indices instead of explicit subarrays. \nFor our string segmentation problem, the argument of any recursive call is always a suffix $A [ i ldots n ]$ of the original input array. So if we treat the input array $A [ 1 ldots n ]$ as a global variable, we can reformulate our recursive problem as follows: \nGiven an index $i$ , find a segmentation of the suffix $A [ i ldots n ]$ . \nTo describe our algorithm, we need two boolean functions: \n• For any indices $i$ and $j$ , let $mathrm { I s W o R D } ( i , j ) = mathrm { T R U E }$ if and only if the substring $A [ i . . j ]$ is a word. (We’re assuming this function is given to us.) • For any index $i$ , let Splittable $( i ) =$ True if and only if the suffix $A [ i ldots n ]$ can be split into words. (This is the function we need to implement.) \nFor example, $mathrm { I s W o R D } ( 1 , n ) = mathrm { T R U E }$ if and only if the entire input string is a single word, and Splittable $( 1 ) =$ True if and only if the entire input string can be segmented. Our earlier recursive strategy gives us the following recurrence: \nThis is exactly the same algorithm as we saw earlier; the only thing we’ve changed is the notation. The similarity is even more apparent if we rewrite the recurrence in pseudocode: \nAlthough it may look like a trivial notational difference, using index notation instead of array notation is an important habit, not only to speed up backtracking algorithms in practice, but for developing dynamic programming algorithms, which we discuss in the next chapter. \nnAnalysis \nIt should come as no surprise that most backtracking algorithms have exponential worst-case running times. Analyzing the precise running times of many of these algorithms requires techniques that are beyond the scope of this book. Fortunately, most of the backtracking algorithms we will encounter in this book are only intermediate results on the way to more efficient algorithms, which means their exact worst-case running time is not actually important. (First make it work; then make it fast.) \nBut just for fun, let’s analyze the running time of our recursive algorithm Splittable. Because we don’t know what IsWord is doing, we can’t know how long each call to IsWord takes, so we’re forced to analyze the running time in terms of the number of calls to IsWord.11 Splittable calls IsWord on every prefix of the input string, and possibly calls itself recursively on every suffix of the output string. Thus, the “running time” of Splittable obeys the scary-looking recurrence \nThis really isn’t as bad as it looks, especially once you’ve seen the trick. \nFirst, we replace the $O ( n )$ term with an explicit expression $a n$ , for some unknown (and ultimately unimportant) constant $alpha$ . Second, we conservatively",
        "chapter": "Backtracking",
        "section": "Text Segmentation (Interpunctio Verborum)",
        "subsection": "Index Formulation",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 33
      }
    },
    {
      "node_id": "tb1_node34",
      "content": "This is exactly the same algorithm as we saw earlier; the only thing we’ve changed is the notation. The similarity is even more apparent if we rewrite the recurrence in pseudocode: \nAlthough it may look like a trivial notational difference, using index notation instead of array notation is an important habit, not only to speed up backtracking algorithms in practice, but for developing dynamic programming algorithms, which we discuss in the next chapter. \nnAnalysis \nIt should come as no surprise that most backtracking algorithms have exponential worst-case running times. Analyzing the precise running times of many of these algorithms requires techniques that are beyond the scope of this book. Fortunately, most of the backtracking algorithms we will encounter in this book are only intermediate results on the way to more efficient algorithms, which means their exact worst-case running time is not actually important. (First make it work; then make it fast.) \nBut just for fun, let’s analyze the running time of our recursive algorithm Splittable. Because we don’t know what IsWord is doing, we can’t know how long each call to IsWord takes, so we’re forced to analyze the running time in terms of the number of calls to IsWord.11 Splittable calls IsWord on every prefix of the input string, and possibly calls itself recursively on every suffix of the output string. Thus, the “running time” of Splittable obeys the scary-looking recurrence \nThis really isn’t as bad as it looks, especially once you’ve seen the trick. \nFirst, we replace the $O ( n )$ term with an explicit expression $a n$ , for some unknown (and ultimately unimportant) constant $alpha$ . Second, we conservatively \nassume that the algorithm actually makes every possible recursive call.12 Then we can transform the “full history” recurrence into a “limited history” recurrence by subtracting the recurrence for $T ( n - 1 )$ , as follows: \nThis final recurrence simplifies to $T ( n ) = 2 T ( n - 1 ) + alpha$ . At this point, we can confidently guess (or derive via recursion trees, or remember from our Tower of Hanoi analysis) that $T ( n ) = O ( 2 ^ { n } ) .$ ; indeed, this upper bound is not hard to prove by induction from the original full-history recurrence. \nMoreover, this analysis is tight. There are exactly $2 ^ { n - 1 }$ possible ways to segment a string of length $n$ —each input character either ends a word or doesn’t, except the last input character, which always ends the last word. In the worst case, our Splittable algorithm explores each of these $2 ^ { n - 1 }$ possibilities. \nVariants \nNow that we have the basic recursion pattern in hand, we can use it to solve many different variants of the segmentation problem, just as we did for the SubsetSum problem. Here I’ll describe just one example; more variations are considered in the exercises. As usual, the original input to our problem is an array $A [ 1 ldots n ]$ . \nIf a string can be segmented in more than one sequence of words, we may want to find the best segmentation according to some criterion; conversely, if the input string cannot be segmented into words, we may want to compute the best segmentation we can find, rather than merely reporting failure. To meet both of these goals, suppose we have access to a second function Score that takes a string as input and returns a numerical value. For example, we might assign higher scores to longer or more common words, lower scores to shorter or more obscure words, slightly negative scores for minor spelling errors, and more negative scores to obvious non-words. Our goal is to find a segmentation that maximizes the sum of the scores of the segments.",
      "metadata": {
        "content": "This is exactly the same algorithm as we saw earlier; the only thing we’ve changed is the notation. The similarity is even more apparent if we rewrite the recurrence in pseudocode: \nAlthough it may look like a trivial notational difference, using index notation instead of array notation is an important habit, not only to speed up backtracking algorithms in practice, but for developing dynamic programming algorithms, which we discuss in the next chapter. \nnAnalysis \nIt should come as no surprise that most backtracking algorithms have exponential worst-case running times. Analyzing the precise running times of many of these algorithms requires techniques that are beyond the scope of this book. Fortunately, most of the backtracking algorithms we will encounter in this book are only intermediate results on the way to more efficient algorithms, which means their exact worst-case running time is not actually important. (First make it work; then make it fast.) \nBut just for fun, let’s analyze the running time of our recursive algorithm Splittable. Because we don’t know what IsWord is doing, we can’t know how long each call to IsWord takes, so we’re forced to analyze the running time in terms of the number of calls to IsWord.11 Splittable calls IsWord on every prefix of the input string, and possibly calls itself recursively on every suffix of the output string. Thus, the “running time” of Splittable obeys the scary-looking recurrence \nThis really isn’t as bad as it looks, especially once you’ve seen the trick. \nFirst, we replace the $O ( n )$ term with an explicit expression $a n$ , for some unknown (and ultimately unimportant) constant $alpha$ . Second, we conservatively \nassume that the algorithm actually makes every possible recursive call.12 Then we can transform the “full history” recurrence into a “limited history” recurrence by subtracting the recurrence for $T ( n - 1 )$ , as follows: \nThis final recurrence simplifies to $T ( n ) = 2 T ( n - 1 ) + alpha$ . At this point, we can confidently guess (or derive via recursion trees, or remember from our Tower of Hanoi analysis) that $T ( n ) = O ( 2 ^ { n } ) .$ ; indeed, this upper bound is not hard to prove by induction from the original full-history recurrence. \nMoreover, this analysis is tight. There are exactly $2 ^ { n - 1 }$ possible ways to segment a string of length $n$ —each input character either ends a word or doesn’t, except the last input character, which always ends the last word. In the worst case, our Splittable algorithm explores each of these $2 ^ { n - 1 }$ possibilities. \nVariants \nNow that we have the basic recursion pattern in hand, we can use it to solve many different variants of the segmentation problem, just as we did for the SubsetSum problem. Here I’ll describe just one example; more variations are considered in the exercises. As usual, the original input to our problem is an array $A [ 1 ldots n ]$ . \nIf a string can be segmented in more than one sequence of words, we may want to find the best segmentation according to some criterion; conversely, if the input string cannot be segmented into words, we may want to compute the best segmentation we can find, rather than merely reporting failure. To meet both of these goals, suppose we have access to a second function Score that takes a string as input and returns a numerical value. For example, we might assign higher scores to longer or more common words, lower scores to shorter or more obscure words, slightly negative scores for minor spelling errors, and more negative scores to obvious non-words. Our goal is to find a segmentation that maximizes the sum of the scores of the segments.",
        "chapter": "Backtracking",
        "section": "Text Segmentation (Interpunctio Verborum)",
        "subsection": "♥Analysis",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 34
      }
    },
    {
      "node_id": "tb1_node39",
      "content": "subtraction trick we used before. We replace the $O ( )$ notation with an explicit constant, regroup and collect identical terms, subtract the recurrence for $T ( n { - } 1 )$ to get rid of the summation, and then regroup again. \nHey, that doesn’t look so bad after all. The recursion tree method immediately gives us the solution $T ( n ) = O ( 3 ^ { n } )$ (or we can just guess and confirm by induction). \nThis analysis implies that our recursive algorithm does not examine all possible binary search trees! The number of binary search trees with $n$ vertices satisfies the recurrence \nwhich has the closed-form solution $N ( n ) = Theta ( 4 ^ { n } / sqrt { n } )$ . (No, that’s not obvious.) Our algorithm saves considerable time by searching independently for the optimal left and right subtrees for each root. A full enumeration of binary search trees would consider all possible pairs of left and right subtrees; hence the product in the recurrence for $N ( n )$ . \nExercises \n1. Describe recursive algorithms for the following generalizations of the SubsetSum problem: \n(a) Given an array $X [ 1 ldots n ]$ of positive integers and an integer $T$ , compute the number of subsets of $X$ whose elements sum to $T$ .   \n(b) Given two arrays $X [ 1 ldots n ]$ and $W [ 1 ldots n ]$ of positive integers and an integer $T$ , where each $W [ i ]$ denotes the weight of the corresponding element $X [ i ]$ , compute the maximum weight subset of $X$ whose elements sum to $T$ . If no subset of $X$ sums to $T$ , your algorithm should return $- infty$ . \n2. Describe recursive algorithms for the following variants of the text segmentation problem. Assume that you have a subroutine IsWord that takes an array of characters as input and returns True if and only if that string is a “word”. \n\n(a) Given an array $A [ 1 ldots n ]$ of characters, compute the number of partitions of A into words. For example, given the string ARTISTOIL, your algorithm should return 2, for the partitions ARTIST $cdot$ OIL and ART $cdot$ IS·TOIL.   \n(b) Given two arrays $A [ 1 ldots n ]$ and $B [ 1 ldots n ]$ of characters, decide whether A and $B$ can be partitioned into words at the same indices. For example, the strings BOTHEARTHANDSATURNSPIN and PINSTARTRAPSANDRAGSLAP can be partitioned into words at the same indices as follows: BOT·HEART $cdot$ HAND·SAT·URNS·PIN PIN·START $cdot cdot$ RAPS·AND·RAGS·LAP   \n(c) Given two arrays $A [ 1 ldots n ]$ and $B [ 1 ldots n ]$ of characters, compute the number of different ways that $A$ and $B$ can be partitioned into words at the same indices. \n3. An addition chain for an integer $n$ is an increasing sequence of integers that starts with 1 and ends with $n$ , such that each entry after the first is the sum of two earlier entries. More formally, the integer sequence $x _ { 0 } < x _ { 1 } < x _ { 2 } < dots < x _ { ell }$ is an addition chain for $n$ if and only if \n• $x _ { 0 } = 1$ ,   \n• $x _ { ell } = n$ , and   \n• for every index $k > 0$ , there are indices $i leq j < k$ such that $x _ { k } = x _ { i } + x _ { j }$ \nThe ℓength of an addition chain is the number of elements minus 1; we don’t bother to count the first entry. For example, $langle 1 , 2 , 3 , 5 , 1 0 , 2 0 , 2 3 , 4 6 ,$ , $9 2 , 1 8 4 , 1 8 7 , 3 7 4 rangle$ is an addition chain for 374 of length 11. \n(a) Describe a recursive backtracking algorithm to compute a minimumlength addition chain for a given positive integer n. Don’t analyze or optimize your algorithm’s running time, except to satisfy your own curiosity. A correct algorithm whose running time is exponential in $n$ is sufficient for full credit. [Hint: This problem is a lot more like n Queens than text segmentation.] n(b) Describe a recursive backtracking algorithm to compute a minimumlength addition chain for a given positive integer n in time that is sub-exponential in n. [Hint: You may find the results of certain Egyptian rope-fasteners, Indus-River prosodists, and Russian peasants helpful.] 4. (a) Let $A [ 1 ldots m ]$ and $B [ 1 ldots n ]$ be two arbitrary arrays. A common subsequence of $A$ and $B$ is both a subsequence of $A$ and a subsequence of $B$ . Give a simple recursive definition for the function $l c s ( A , B )$ , which gives the length of the longest common subsequence of $A$ and $B$ . \n(b) Let $A [ 1 ldots m ]$ and $B [ 1 ldots n ]$ be two arbitrary arrays. A common supersequence of $A$ and $B$ is another sequence that contains both $A$ and $B$ as subsequences. Give a simple recursive definition for the function $s c s ( A , B )$ , which gives the length of the shortest common supersequence of $A$ and $B$ . (c) Call a sequence $X [ 1 ldots n ]$ of numbers bitonic if there is an index $i$ with $1 < i < n$ , such that the prefix $X [ 1 ldots i ]$ is increasing and the suffix $X [ i ldots n ]$ is decreasing. Give a simple recursive definition for the function $l b s ( A )$ , which gives the length of the longest bitonic subsequence of an arbitrary array $A$ of integers. (d) Call a sequence $X [ 1 ldots n ]$ oscillating if $X [ i ] < X [ i + 1 ]$ for all even $i$ , and $X [ i ] > X [ i + 1 ]$ for all odd i. Give a simple recursive definition for the function $boldsymbol { l o s } ( boldsymbol { A } )$ , which gives the length of the longest oscillating subsequence of an arbitrary array $A$ of integers. (e) Give a simple recursive definition for the function $s o s ( A )$ , which gives the length of the shortest oscillating supersequence of an arbitrary array $A$ of integers. (f) Call a sequence $X [ 1 ldots n ]$ convex if $2 cdot X [ i ] < X [ i - 1 ] + X [ i + 1 ]$ for all $i$ . Give a simple recursive definition for the function $l x s ( A )$ , which gives the length of the longest convex subsequence of an arbitrary array $A$ of integers. 5. For each of the following problems, the input consists of two arrays $X [ 1 . . k ]$ and $Y [ 1 ldots n ]$ where $k leq n$ . (a) Describe a recursive backtracking algorithm to determine whether $X$ is a subsequence of Y . For example, the string PPAP is a subsequence of the string PENPINEAPPLEAPPLEPEN. (b) Describe a recursive backtracking algorithm to find the smallest number of symbols that can be removed from $Y$ so that $X$ is no longer a subsequence. Equivalently, your algorithm should find the longest subsequence of $Y$ that is not a supersequence of $X$ . For example, after removing removing two symbols from the string PENPINEAPPLEAPPLEPEN, the string PPAP is no longer a subsequence. n(c) Describe a recursive backtracking algorithm to determine whether $X$ occurs as two disjoint subsequences of Y . For example, the string PPAP appears as two disjoint subsequences in the string PENPINEAPPLEAPPLEPEN. Don’t analyze the running times of your algorithms, except to satisfy your own curiosity. All three algorithms run in exponential time; we’ll improve that later, so the precise running time isn’t particularly important. \n\n6. This problem asks you to design backtracking algorithms to find the cost of an optimal binary search tree that satisfies additional balance constraints. Your input consists of a sorted array $A [ 1 ldots n ]$ of search keys and an array $f [ 1 . . n ]$ of frequency counts, where $f [ i ]$ is the number of searches for $A [ i ]$ . This is exactly the same cost function as described in Section 2.8. But now your task is to compute an optimal tree that satisfies some additional constraints. \n(a) AVL trees were the earliest self-balancing balanced binary search trees, first described in 1962 by Georgy Adelson-Velsky and Evgenii Landis. An AVL tree is a binary search tree where for every node $nu$ , the height of the left subtree of $nu$ and the height of the right subtree of $nu$ differ by at most one. \nDescribe a recursive backtracking algorithm to construct an optimal AVL tree for a given set of search keys and frequencies. \n(b) Symmetric binary $B$ -trees are another self-balancing binary trees, first described by Rudolf Bayer in 1972; these are better known by the name red-black trees, after a somewhat simpler reformulation by Leo Guibas and Bob Sedgwick in 1978. A red-black tree is a binary search tree with the following additional constraints: \n• Every node is either red or black.   \n• Every red node has a black parent.   \n• Every root-to-leaf path contains the same number of black nodes. \nDescribe a recursive backtracking algorithm to construct an optimal red-black tree for a given set of search keys and frequencies. \n(c) AA trees were proposed by proposed by Arne Andersson in 1993 and slightly simplified (and named) by Mark Allen Weiss in 2000. AA trees are also known as left-leaning red-black trees, after a symmetric reformulation (with different rebalancing algorithms) by Bob Sedgewick in 2006. An AA tree is a red-black tree with one additional constraint: • No left child is red.14 \nDescribe a recursive backtracking algorithm to construct an optimal AA tree for a given set of search keys and frequencies. \nDon’t analyze the running times of your algorithms, except to satisfy your own curiosity. All three algorithms run in exponential time; we’ll improve that later, so the precise running times aren’t particularly important. \nFor more backtracking exercises, see the next chapter! \nPotes enim videre in hac margine, qualiter hoc operati fuimus, scilicet quod iunximus primum numerum cum secundo, videlicet 1 cum 2; et secundum cum tercio; et tercium cum quarto; et quartum cum quinto, et sic deinceps.... \n[You can see in the margin here how we have worked this; clearly, we combined the first number with the second, namely 1 with 2, and the second with the third, and the third with the fourth, and the fourth with the fifth, and so forth. . . .] \n— Leonardo Pisano, Liber Abaci (1202) \nThose who cannot remember the past are condemned to repeat it. \n— Jorge Agustín Nicolás Ruiz de Santayana y Borrás, The Life of Reason, Book I: Introduction and Reason in Common Sense (1905) \nYou know what a learning experience is? A learning experience is one of those things that says, “You know that thing you just did? Don’t do that.” \n— Douglas Adams, The Salmon of Doubt (2002) \n3 \nDynamic Programming \n3.1 M¯atra¯vr.tta \nOne of the earliest examples of recursion arose in India more than 2000 years ago, in the study of poetic meter, or prosody. Classical Sanskrit poetry distinguishes between two types of syllables (ak.sara): light (laghu) and heavy (guru). In one class of meters, variously called m¯atr¯av.rtta or m¯atr¯achandas, each line of poetry consists of a fixed number of “beats” (ma¯tra¯), where each light syllable lasts one beat and each heavy syllable lasts two beats. The formal study of m¯atr¯a-v.rtta dates back to the Chanda.hś¯astra, written by the scholar Pi˙ngala between 600bce and 200bce. Pi˙ngala observed that there are exactly five 4-beat meters: —, and • • • •. (Here each “—” represents a long syllable and each “•” represents a short syllable.)1",
      "metadata": {
        "content": "subtraction trick we used before. We replace the $O ( )$ notation with an explicit constant, regroup and collect identical terms, subtract the recurrence for $T ( n { - } 1 )$ to get rid of the summation, and then regroup again. \nHey, that doesn’t look so bad after all. The recursion tree method immediately gives us the solution $T ( n ) = O ( 3 ^ { n } )$ (or we can just guess and confirm by induction). \nThis analysis implies that our recursive algorithm does not examine all possible binary search trees! The number of binary search trees with $n$ vertices satisfies the recurrence \nwhich has the closed-form solution $N ( n ) = Theta ( 4 ^ { n } / sqrt { n } )$ . (No, that’s not obvious.) Our algorithm saves considerable time by searching independently for the optimal left and right subtrees for each root. A full enumeration of binary search trees would consider all possible pairs of left and right subtrees; hence the product in the recurrence for $N ( n )$ . \nExercises \n1. Describe recursive algorithms for the following generalizations of the SubsetSum problem: \n(a) Given an array $X [ 1 ldots n ]$ of positive integers and an integer $T$ , compute the number of subsets of $X$ whose elements sum to $T$ .   \n(b) Given two arrays $X [ 1 ldots n ]$ and $W [ 1 ldots n ]$ of positive integers and an integer $T$ , where each $W [ i ]$ denotes the weight of the corresponding element $X [ i ]$ , compute the maximum weight subset of $X$ whose elements sum to $T$ . If no subset of $X$ sums to $T$ , your algorithm should return $- infty$ . \n2. Describe recursive algorithms for the following variants of the text segmentation problem. Assume that you have a subroutine IsWord that takes an array of characters as input and returns True if and only if that string is a “word”. \n\n(a) Given an array $A [ 1 ldots n ]$ of characters, compute the number of partitions of A into words. For example, given the string ARTISTOIL, your algorithm should return 2, for the partitions ARTIST $cdot$ OIL and ART $cdot$ IS·TOIL.   \n(b) Given two arrays $A [ 1 ldots n ]$ and $B [ 1 ldots n ]$ of characters, decide whether A and $B$ can be partitioned into words at the same indices. For example, the strings BOTHEARTHANDSATURNSPIN and PINSTARTRAPSANDRAGSLAP can be partitioned into words at the same indices as follows: BOT·HEART $cdot$ HAND·SAT·URNS·PIN PIN·START $cdot cdot$ RAPS·AND·RAGS·LAP   \n(c) Given two arrays $A [ 1 ldots n ]$ and $B [ 1 ldots n ]$ of characters, compute the number of different ways that $A$ and $B$ can be partitioned into words at the same indices. \n3. An addition chain for an integer $n$ is an increasing sequence of integers that starts with 1 and ends with $n$ , such that each entry after the first is the sum of two earlier entries. More formally, the integer sequence $x _ { 0 } < x _ { 1 } < x _ { 2 } < dots < x _ { ell }$ is an addition chain for $n$ if and only if \n• $x _ { 0 } = 1$ ,   \n• $x _ { ell } = n$ , and   \n• for every index $k > 0$ , there are indices $i leq j < k$ such that $x _ { k } = x _ { i } + x _ { j }$ \nThe ℓength of an addition chain is the number of elements minus 1; we don’t bother to count the first entry. For example, $langle 1 , 2 , 3 , 5 , 1 0 , 2 0 , 2 3 , 4 6 ,$ , $9 2 , 1 8 4 , 1 8 7 , 3 7 4 rangle$ is an addition chain for 374 of length 11. \n(a) Describe a recursive backtracking algorithm to compute a minimumlength addition chain for a given positive integer n. Don’t analyze or optimize your algorithm’s running time, except to satisfy your own curiosity. A correct algorithm whose running time is exponential in $n$ is sufficient for full credit. [Hint: This problem is a lot more like n Queens than text segmentation.] n(b) Describe a recursive backtracking algorithm to compute a minimumlength addition chain for a given positive integer n in time that is sub-exponential in n. [Hint: You may find the results of certain Egyptian rope-fasteners, Indus-River prosodists, and Russian peasants helpful.] 4. (a) Let $A [ 1 ldots m ]$ and $B [ 1 ldots n ]$ be two arbitrary arrays. A common subsequence of $A$ and $B$ is both a subsequence of $A$ and a subsequence of $B$ . Give a simple recursive definition for the function $l c s ( A , B )$ , which gives the length of the longest common subsequence of $A$ and $B$ . \n(b) Let $A [ 1 ldots m ]$ and $B [ 1 ldots n ]$ be two arbitrary arrays. A common supersequence of $A$ and $B$ is another sequence that contains both $A$ and $B$ as subsequences. Give a simple recursive definition for the function $s c s ( A , B )$ , which gives the length of the shortest common supersequence of $A$ and $B$ . (c) Call a sequence $X [ 1 ldots n ]$ of numbers bitonic if there is an index $i$ with $1 < i < n$ , such that the prefix $X [ 1 ldots i ]$ is increasing and the suffix $X [ i ldots n ]$ is decreasing. Give a simple recursive definition for the function $l b s ( A )$ , which gives the length of the longest bitonic subsequence of an arbitrary array $A$ of integers. (d) Call a sequence $X [ 1 ldots n ]$ oscillating if $X [ i ] < X [ i + 1 ]$ for all even $i$ , and $X [ i ] > X [ i + 1 ]$ for all odd i. Give a simple recursive definition for the function $boldsymbol { l o s } ( boldsymbol { A } )$ , which gives the length of the longest oscillating subsequence of an arbitrary array $A$ of integers. (e) Give a simple recursive definition for the function $s o s ( A )$ , which gives the length of the shortest oscillating supersequence of an arbitrary array $A$ of integers. (f) Call a sequence $X [ 1 ldots n ]$ convex if $2 cdot X [ i ] < X [ i - 1 ] + X [ i + 1 ]$ for all $i$ . Give a simple recursive definition for the function $l x s ( A )$ , which gives the length of the longest convex subsequence of an arbitrary array $A$ of integers. 5. For each of the following problems, the input consists of two arrays $X [ 1 . . k ]$ and $Y [ 1 ldots n ]$ where $k leq n$ . (a) Describe a recursive backtracking algorithm to determine whether $X$ is a subsequence of Y . For example, the string PPAP is a subsequence of the string PENPINEAPPLEAPPLEPEN. (b) Describe a recursive backtracking algorithm to find the smallest number of symbols that can be removed from $Y$ so that $X$ is no longer a subsequence. Equivalently, your algorithm should find the longest subsequence of $Y$ that is not a supersequence of $X$ . For example, after removing removing two symbols from the string PENPINEAPPLEAPPLEPEN, the string PPAP is no longer a subsequence. n(c) Describe a recursive backtracking algorithm to determine whether $X$ occurs as two disjoint subsequences of Y . For example, the string PPAP appears as two disjoint subsequences in the string PENPINEAPPLEAPPLEPEN. Don’t analyze the running times of your algorithms, except to satisfy your own curiosity. All three algorithms run in exponential time; we’ll improve that later, so the precise running time isn’t particularly important. \n\n6. This problem asks you to design backtracking algorithms to find the cost of an optimal binary search tree that satisfies additional balance constraints. Your input consists of a sorted array $A [ 1 ldots n ]$ of search keys and an array $f [ 1 . . n ]$ of frequency counts, where $f [ i ]$ is the number of searches for $A [ i ]$ . This is exactly the same cost function as described in Section 2.8. But now your task is to compute an optimal tree that satisfies some additional constraints. \n(a) AVL trees were the earliest self-balancing balanced binary search trees, first described in 1962 by Georgy Adelson-Velsky and Evgenii Landis. An AVL tree is a binary search tree where for every node $nu$ , the height of the left subtree of $nu$ and the height of the right subtree of $nu$ differ by at most one. \nDescribe a recursive backtracking algorithm to construct an optimal AVL tree for a given set of search keys and frequencies. \n(b) Symmetric binary $B$ -trees are another self-balancing binary trees, first described by Rudolf Bayer in 1972; these are better known by the name red-black trees, after a somewhat simpler reformulation by Leo Guibas and Bob Sedgwick in 1978. A red-black tree is a binary search tree with the following additional constraints: \n• Every node is either red or black.   \n• Every red node has a black parent.   \n• Every root-to-leaf path contains the same number of black nodes. \nDescribe a recursive backtracking algorithm to construct an optimal red-black tree for a given set of search keys and frequencies. \n(c) AA trees were proposed by proposed by Arne Andersson in 1993 and slightly simplified (and named) by Mark Allen Weiss in 2000. AA trees are also known as left-leaning red-black trees, after a symmetric reformulation (with different rebalancing algorithms) by Bob Sedgewick in 2006. An AA tree is a red-black tree with one additional constraint: • No left child is red.14 \nDescribe a recursive backtracking algorithm to construct an optimal AA tree for a given set of search keys and frequencies. \nDon’t analyze the running times of your algorithms, except to satisfy your own curiosity. All three algorithms run in exponential time; we’ll improve that later, so the precise running times aren’t particularly important. \nFor more backtracking exercises, see the next chapter! \nPotes enim videre in hac margine, qualiter hoc operati fuimus, scilicet quod iunximus primum numerum cum secundo, videlicet 1 cum 2; et secundum cum tercio; et tercium cum quarto; et quartum cum quinto, et sic deinceps.... \n[You can see in the margin here how we have worked this; clearly, we combined the first number with the second, namely 1 with 2, and the second with the third, and the third with the fourth, and the fourth with the fifth, and so forth. . . .] \n— Leonardo Pisano, Liber Abaci (1202) \nThose who cannot remember the past are condemned to repeat it. \n— Jorge Agustín Nicolás Ruiz de Santayana y Borrás, The Life of Reason, Book I: Introduction and Reason in Common Sense (1905) \nYou know what a learning experience is? A learning experience is one of those things that says, “You know that thing you just did? Don’t do that.” \n— Douglas Adams, The Salmon of Doubt (2002) \n3 \nDynamic Programming \n3.1 M¯atra¯vr.tta \nOne of the earliest examples of recursion arose in India more than 2000 years ago, in the study of poetic meter, or prosody. Classical Sanskrit poetry distinguishes between two types of syllables (ak.sara): light (laghu) and heavy (guru). In one class of meters, variously called m¯atr¯av.rtta or m¯atr¯achandas, each line of poetry consists of a fixed number of “beats” (ma¯tra¯), where each light syllable lasts one beat and each heavy syllable lasts two beats. The formal study of m¯atr¯a-v.rtta dates back to the Chanda.hś¯astra, written by the scholar Pi˙ngala between 600bce and 200bce. Pi˙ngala observed that there are exactly five 4-beat meters: —, and • • • •. (Here each “—” represents a long syllable and each “•” represents a short syllable.)1",
        "chapter": "Backtracking",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 39
      }
    },
    {
      "node_id": "tb1_node41",
      "content": "Unfortunately, this naive recursive algorithm is horribly slow. Except for the recursive calls, the entire algorithm requires only a constant number of steps: one comparison and possibly one addition. Let $T ( n )$ denote the number of recursive calls to RecFibo; this function satisfies the recurrence \nwhich looks an awful lot like the recurrence for Fibonacci numbers themselves! Writing out the first several values of $T ( n )$ suggests the closed-form solution $T ( n ) = 2 F _ { n + 1 } - 1$ , which we can verify by induction (hint, hint). So computing $F _ { n }$ using this algorithm takes about twice as long as just counting to $F _ { n }$ . Methods beyond the scope of this book4 imply that $F _ { n } = Theta ( phi ^ { n } )$ , where $phi = ( sqrt { 5 } + 1 ) / 2 approx 1 . 6 1 8 0 3$ is the so-called golden ratio. In short, the running time of this recursive algorithm is exponential in $n$ . \nWe can actually see this exponential growth directly as follows. Think of the recursion tree for RecFibo as a binary tree of additions, with only 0s and 1s at the leaves. Since the eventual output is $F _ { n }$ , exactly $F _ { n }$ of the leaves must have value 1; these leaves represent the calls to RecRibo(1). An easy inductive argument (hint, hint) implies that RecFibo(0) is called exactly $F _ { n - 1 }$ times. (If we just want an asymptotic bound, it’s enough to observe that the number of calls to RecFibo(0) is at most the number of calls to RecFibo(1).) Thus, the recursion tree has exactly $F _ { n } + F _ { n - 1 } = F _ { n + 1 } = O ( F _ { n } )$ leaves, and therefore, because it’s a full binary tree, $2 F _ { n + 1 } - 1 = O ( F _ { n } )$ nodes altogether. \nMemo(r)ization: Remember Everything \nThe obvious reason for the recursive algorithm’s lack of speed is that it computes the same Fibonacci numbers over and over and over. A single call to RecFibo $( n )$ results in one recursive call to ${ mathrm { R E C F I B O } } ( n - 1 )$ , two recursive calls to ${ mathrm { R E C F I B O } } ( n - 2 )$ , three recursive calls to $operatorname { R E C F I B O } ( n - 3 )$ , five recursive calls to RecFibo $( n - 4 )$ , and in general $F _ { k - 1 }$ recursive calls to $mathrm { R E C F I B O } ( n - k )$ for any integer $0 leq k < n$ . Each call is recomputing some Fibonacci number from scratch. \nWe can speed up our recursive algorithm considerably by writing down the results of our recursive calls and looking them up again if we need them later. \nThis optimization technique, now known as memoization (yes, without an R), is usually credited to Donald Michie in 1967, but essentially the same technique was proposed in 1959 by Arthur Samuel.5 \nMemFibo(n): if $n = 0$ return 0 else if $n = 1$ return 1 else if $F [ n ]$ is undefined F[n] MemFibo(n 1) + MemFibo(n 2) return F [n] \nMemoization clearly decreases the running time of the algorithm, but by how much? If we actually trace through the recursive calls made by MemFibo, we find that the array $F [ ]$ is filled from the bottom up: first $F [ 2 ]$ , then $F [ 3 ]$ , and so on, up to $F [ n ]$ . This pattern can be verified by induction: Each entry $F [ i ]$ is filled only after its predecessor $F [ i - 1 ]$ . If we ignore the time spent in recursive calls, it requires only constant time to evaluate the recurrence for each Fibonacci number $F _ { i }$ . But by design, the recurrence for $F _ { i }$ is evaluated only once for each index i. We conclude that MemFibo performs only $O ( n )$ additions, an exponential improvement over the naïve recursive algorithm! \nDynamic Programming: Fill Deliberately \nOnce we see how the array $F [ ]$ is filled, we can replace the memoized recurrence with a simple for-loop that intentionally fills the array in that order, instead of relying on a more complicated recursive algorithm to do it for us accidentally. \nNow the time analysis is immediate: IterFibo clearly uses $O ( n )$ additions and stores $O ( n )$ integers. \nThis is our first explicit dynamic programming algorithm. The dynamic programming paradigm was formalized and popularized by Richard Bellman in the mid-1950s, while working at the RAND Corporation, although he was far from the first to use the technique. In particular, this iterative algorithm for Fibonacci numbers was already proposed by Virah¯a .nka and later Sanskrit prosodists in the 12th century, and again by Fibonacci at the turn of the 13th century!6",
      "metadata": {
        "content": "Unfortunately, this naive recursive algorithm is horribly slow. Except for the recursive calls, the entire algorithm requires only a constant number of steps: one comparison and possibly one addition. Let $T ( n )$ denote the number of recursive calls to RecFibo; this function satisfies the recurrence \nwhich looks an awful lot like the recurrence for Fibonacci numbers themselves! Writing out the first several values of $T ( n )$ suggests the closed-form solution $T ( n ) = 2 F _ { n + 1 } - 1$ , which we can verify by induction (hint, hint). So computing $F _ { n }$ using this algorithm takes about twice as long as just counting to $F _ { n }$ . Methods beyond the scope of this book4 imply that $F _ { n } = Theta ( phi ^ { n } )$ , where $phi = ( sqrt { 5 } + 1 ) / 2 approx 1 . 6 1 8 0 3$ is the so-called golden ratio. In short, the running time of this recursive algorithm is exponential in $n$ . \nWe can actually see this exponential growth directly as follows. Think of the recursion tree for RecFibo as a binary tree of additions, with only 0s and 1s at the leaves. Since the eventual output is $F _ { n }$ , exactly $F _ { n }$ of the leaves must have value 1; these leaves represent the calls to RecRibo(1). An easy inductive argument (hint, hint) implies that RecFibo(0) is called exactly $F _ { n - 1 }$ times. (If we just want an asymptotic bound, it’s enough to observe that the number of calls to RecFibo(0) is at most the number of calls to RecFibo(1).) Thus, the recursion tree has exactly $F _ { n } + F _ { n - 1 } = F _ { n + 1 } = O ( F _ { n } )$ leaves, and therefore, because it’s a full binary tree, $2 F _ { n + 1 } - 1 = O ( F _ { n } )$ nodes altogether. \nMemo(r)ization: Remember Everything \nThe obvious reason for the recursive algorithm’s lack of speed is that it computes the same Fibonacci numbers over and over and over. A single call to RecFibo $( n )$ results in one recursive call to ${ mathrm { R E C F I B O } } ( n - 1 )$ , two recursive calls to ${ mathrm { R E C F I B O } } ( n - 2 )$ , three recursive calls to $operatorname { R E C F I B O } ( n - 3 )$ , five recursive calls to RecFibo $( n - 4 )$ , and in general $F _ { k - 1 }$ recursive calls to $mathrm { R E C F I B O } ( n - k )$ for any integer $0 leq k < n$ . Each call is recomputing some Fibonacci number from scratch. \nWe can speed up our recursive algorithm considerably by writing down the results of our recursive calls and looking them up again if we need them later. \nThis optimization technique, now known as memoization (yes, without an R), is usually credited to Donald Michie in 1967, but essentially the same technique was proposed in 1959 by Arthur Samuel.5 \nMemFibo(n): if $n = 0$ return 0 else if $n = 1$ return 1 else if $F [ n ]$ is undefined F[n] MemFibo(n 1) + MemFibo(n 2) return F [n] \nMemoization clearly decreases the running time of the algorithm, but by how much? If we actually trace through the recursive calls made by MemFibo, we find that the array $F [ ]$ is filled from the bottom up: first $F [ 2 ]$ , then $F [ 3 ]$ , and so on, up to $F [ n ]$ . This pattern can be verified by induction: Each entry $F [ i ]$ is filled only after its predecessor $F [ i - 1 ]$ . If we ignore the time spent in recursive calls, it requires only constant time to evaluate the recurrence for each Fibonacci number $F _ { i }$ . But by design, the recurrence for $F _ { i }$ is evaluated only once for each index i. We conclude that MemFibo performs only $O ( n )$ additions, an exponential improvement over the naïve recursive algorithm! \nDynamic Programming: Fill Deliberately \nOnce we see how the array $F [ ]$ is filled, we can replace the memoized recurrence with a simple for-loop that intentionally fills the array in that order, instead of relying on a more complicated recursive algorithm to do it for us accidentally. \nNow the time analysis is immediate: IterFibo clearly uses $O ( n )$ additions and stores $O ( n )$ integers. \nThis is our first explicit dynamic programming algorithm. The dynamic programming paradigm was formalized and popularized by Richard Bellman in the mid-1950s, while working at the RAND Corporation, although he was far from the first to use the technique. In particular, this iterative algorithm for Fibonacci numbers was already proposed by Virah¯a .nka and later Sanskrit prosodists in the 12th century, and again by Fibonacci at the turn of the 13th century!6",
        "chapter": "Dynamic Programming",
        "section": "Mātrāvṛtta",
        "subsection": "Memo(r)ization: Remember Everything",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 41
      }
    },
    {
      "node_id": "tb1_node42",
      "content": "Dynamic Programming: Fill Deliberately \nOnce we see how the array $F [ ]$ is filled, we can replace the memoized recurrence with a simple for-loop that intentionally fills the array in that order, instead of relying on a more complicated recursive algorithm to do it for us accidentally. \nNow the time analysis is immediate: IterFibo clearly uses $O ( n )$ additions and stores $O ( n )$ integers. \nThis is our first explicit dynamic programming algorithm. The dynamic programming paradigm was formalized and popularized by Richard Bellman in the mid-1950s, while working at the RAND Corporation, although he was far from the first to use the technique. In particular, this iterative algorithm for Fibonacci numbers was already proposed by Virah¯a .nka and later Sanskrit prosodists in the 12th century, and again by Fibonacci at the turn of the 13th century!6 \nMany years after the fact, Bellman claimed that he deliberately chose the name “dynamic programming” to hide the mathematical character of his work from his military bosses, who were actively hostile toward anything resembling mathematical research.7 The word “programming” does not refer to writing code, but rather to the older sense of planning or scheduling, typically by filling in a table. For example, sports programs and theater programs are schedules of important events (with ads); television programming involves filling each available time slot with a show (and ads); degree programs are schedules of classes to be taken (with ads). The Air Force funded Bellman and others to develop methods for constructing training and logistics schedules, or as they called them, “programs”. The word “dynamic” was not only a reference to the multistage, time-varying processes that Bellman and his colleagues were attempting to optimize, but also a marketing buzzword that would resonate with the Futuristic Can-Do Zeitgeist™ of post-WW II America.8 Thanks in part to Bellman’s proselytizing, dynamic programming is now a standard tool for multistage planning in economics, robotics, control theory, and several other disciplines. \nDon’t Remember Everything After All \nIn many dynamic programming algorithms, it is not necessary to retain all intermediate results through the entire computation. For example, we can significantly reduce the space requirements of our algorithm IterFibo by maintaining only the two newest elements of the array: \nWe had a very interesting gentleman in Washington named Wilson. He was secretary of Defense, and he actually had a pathological fear and hatred of the word “research”. I’m not using the term lightly; I’m using it precisely. His face would suffuse, he would turn red, and he would get violent if people used the term “research” in his presence. You can imagine how he felt, then, about the term “mathematical”. . . . I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. What title, what name, could I choose? \nHowever, Bellman’s first published use of the term “dynamic programming” already appeared in 1952, several months before Wilson took office, so this story is at least slightly embellished. 8. . . and just possibly a riff on the iconic brand name “Dynamic-Tension” for Charles Atlas’s famous series of exercises, which Charles Roman coined in 1928. Hero of the Beach!",
      "metadata": {
        "content": "Dynamic Programming: Fill Deliberately \nOnce we see how the array $F [ ]$ is filled, we can replace the memoized recurrence with a simple for-loop that intentionally fills the array in that order, instead of relying on a more complicated recursive algorithm to do it for us accidentally. \nNow the time analysis is immediate: IterFibo clearly uses $O ( n )$ additions and stores $O ( n )$ integers. \nThis is our first explicit dynamic programming algorithm. The dynamic programming paradigm was formalized and popularized by Richard Bellman in the mid-1950s, while working at the RAND Corporation, although he was far from the first to use the technique. In particular, this iterative algorithm for Fibonacci numbers was already proposed by Virah¯a .nka and later Sanskrit prosodists in the 12th century, and again by Fibonacci at the turn of the 13th century!6 \nMany years after the fact, Bellman claimed that he deliberately chose the name “dynamic programming” to hide the mathematical character of his work from his military bosses, who were actively hostile toward anything resembling mathematical research.7 The word “programming” does not refer to writing code, but rather to the older sense of planning or scheduling, typically by filling in a table. For example, sports programs and theater programs are schedules of important events (with ads); television programming involves filling each available time slot with a show (and ads); degree programs are schedules of classes to be taken (with ads). The Air Force funded Bellman and others to develop methods for constructing training and logistics schedules, or as they called them, “programs”. The word “dynamic” was not only a reference to the multistage, time-varying processes that Bellman and his colleagues were attempting to optimize, but also a marketing buzzword that would resonate with the Futuristic Can-Do Zeitgeist™ of post-WW II America.8 Thanks in part to Bellman’s proselytizing, dynamic programming is now a standard tool for multistage planning in economics, robotics, control theory, and several other disciplines. \nDon’t Remember Everything After All \nIn many dynamic programming algorithms, it is not necessary to retain all intermediate results through the entire computation. For example, we can significantly reduce the space requirements of our algorithm IterFibo by maintaining only the two newest elements of the array: \nWe had a very interesting gentleman in Washington named Wilson. He was secretary of Defense, and he actually had a pathological fear and hatred of the word “research”. I’m not using the term lightly; I’m using it precisely. His face would suffuse, he would turn red, and he would get violent if people used the term “research” in his presence. You can imagine how he felt, then, about the term “mathematical”. . . . I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. What title, what name, could I choose? \nHowever, Bellman’s first published use of the term “dynamic programming” already appeared in 1952, several months before Wilson took office, so this story is at least slightly embellished. 8. . . and just possibly a riff on the iconic brand name “Dynamic-Tension” for Charles Atlas’s famous series of exercises, which Charles Roman coined in 1928. Hero of the Beach!",
        "chapter": "Dynamic Programming",
        "section": "Mātrāvṛtta",
        "subsection": "Dynamic Programming: Fill Deliberately",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 42
      }
    }
  ],
  "output": {
    "question": "Based on the provided context about dynamic programming, which of the following best describes the key advantage of replacing a memoized recurrence with a simple for-loop to fill an array?",
    "correct_answer": "It intentionally fills the array in the correct order, improving efficiency over a recursive approach.",
    "distractors": [
      "It eliminates the need for a base case in the algorithm.",
      "It reduces the space complexity from O(n) to O(1).",
      "It automatically handles backtracking for segmentation problems."
    ],
    "rationale": "Step 1: Snippet [5] explicitly states: 'Dynamic Programming: Fill Deliberately. Once we see how the array F[] is filled, we can replace the memoized recurrence with a simple for-loop that intentionally fills the array in that order, instead of relying on a more complicated recursive algorithm to do it for us accidentally.' This directly describes the advantage. Step 2: Snippet [4] provides context about the inefficiency of naive recursion ('horribly slow'), which the for-loop approach addresses. Step 3: Snippet [2] mentions the importance of notation (index vs. array), which relates to how we structure the iterative filling. The other distractors are not supported: eliminating base cases (contradicted by [1]), reducing space to O(1) (not mentioned), or handling backtracking (from [1], but not the focus of the for-loop advantage)."
  },
  "score": {
    "relevance": 1.0,
    "faithfulness": 1.0,
    "integration": 0.0,
    "complexity": 0.0,
    "overall": 0.30000000000000004
  }
}
{
  "context": [
    {
      "node_id": "tb1_node3",
      "content": "In other words, this algorithm requires $O ( m n )$ time to multiply an $m$ -digit number by an $n$ -digit number; up to constant factors, this is the same running time as the lattice algorithm. This algorithm requires (a constant factor!) more paperwork to execute by hand than the lattice algorithm, but the necessary primitive operations are arguably easier for humans to perform. In fact, the two algorithms are equivalent when numbers are represented in binary. \nCompass and Straightedge \nClassical Greek geometers identified numbers (or more accurately, magnitudes) with line segments of the appropriate length, which they manipulated using two simple mechanical tools—the compass and the straightedge—versions of which had already been in common use by surveyors, architects, and other artisans for centuries. Using only these two tools, these scholars reduced several complex geometric constructions to the following primitive operations, starting with one or more identified reference points. \n• Draw the unique line passing through two distinct identified points.   \n• Draw the unique circle centered at one identified point and passing through another.   \n• Identify the intersection point (if any) of two lines.   \n• Identify the intersection points (if any) of a line and a circle.   \n• Identify the intersection points (if any) of two circles. \nIn practice, Greek geometry students almost certainly drew their constructions on an abax $( overrightarrow { a } vert 3 overrightarrow { a } dot { xi } )$ , a table covered in dust or sand.11 Centuries earlier, Egyptian surveyors carried out many of the same constructions using ropes to determine straight lines and circles on the ground.12 However, Euclid and other Greek geometers presented compass and straightedge constructions as precise mathematical abstractions—points are ideal points; lines are ideal lines; and circles are ideal circles. \nFigure 0.4 shows an algorithm, described in Euclid’s Elements about 2500 years ago, for multiplying or dividing two magnitudes. The input consists of four distinct points $A , B , C$ , and $D$ , and the goal is to construct a point $Z$ such that $| A Z | = | A C | | A D | / | A B |$ . In particular, if we define $left| A B right|$ to be our unit of length, then the algorithm computes the product of $left| A C right|$ and $| A D |$ . \nNotice that Euclid first defines a new primitive operation RightAngle by (as modern programmers would phrase it) writing a subroutine. The correctness \nConstruct the line perpendicular to ℓ passing through P.   \nRightAngle(ℓ, P): Choose a point $A in { ell }$ $A , B gets$ Intersect(Circle(P, A), ℓ) $C , D gets mathrm { I N T E R S E C T } big ( mathrm { C I R C L E } ( A , B ) , mathrm { C I R C L E } ( B , A ) big )$ return Line $( C , D )$   \nConstruct a point $Z$ such that $| A Z | = | A C | | A D | / | A B | . rangle rangle$   \nMultiplyOrDivide(A, B, C, D): $alpha gets$ RightAngle(Line(A, C),A) $E gets$ Intersect(Circle(A, B), α) $beta$ $F gets$ Intersect(Circle(A, D), α) $beta gets$ RightAngle(Line(E, C), F) A E $alpha$ 1 $gamma gets$ RightAngle(β, F) return Intersect(γ, Line(A, C)) γ \nof the algorithm follows from the observation that triangles $A C E$ and $A Z F$ are similar. The second and third lines of the main algorithm are ambiguous, because $alpha$ intersects any circle centered at $A$ at two distinct points, but the algorithm is actually correct no matter which intersection points are chosen for $E$ and $F$ . \nEuclid’s algorithm reduces the problem of multiplying two magnitudes (lengths) to a series of primitive compass-and-straightedge operations. These operations are difficult to implement precisely on a modern digital computer, but Euclid’s algorithm wasn’t designed for a digital computer. It was designed for the Platonic Ideal Geometer, wielding the Platonic Ideal Compass and the Platonic Ideal Straightedge, who could execute each operation perfectly in constant time by definition. In this model of computation, MultiplyOrDivide runs in $O ( 1 )$ time! \n0.3 Congressional Apportionment \nHere is another real-world example of an algorithm of significant political importance. Article I, Section 2 of the United States Constitution requires that \nRepresentatives and direct Taxes shall be apportioned among the several States which may be included within this Union, according to their respective Numbers.... The Number of Representatives shall not exceed one for every thirty Thousand, but each State shall have at Least one Representative... \nBecause there are only a finite number of seats in the House of Representatives, exact proportional representation requires either shared or fractional representatives, neither of which are legal. As a result, over the next several decades, many different apportionment algorithms were proposed and used to round the ideal fractional solution fairly. The algorithm actually used today, called the Huntington-Hill method or the method of equal proportions, was first suggested by Census Bureau statistician Joseph Hill in 1911, refined by Harvard mathematician Edward Huntington in 1920, adopted into Federal law (2 U.S.C. §2a) in 1941, and survived a Supreme Court challenge in 1992.13",
      "metadata": {
        "content": "In other words, this algorithm requires $O ( m n )$ time to multiply an $m$ -digit number by an $n$ -digit number; up to constant factors, this is the same running time as the lattice algorithm. This algorithm requires (a constant factor!) more paperwork to execute by hand than the lattice algorithm, but the necessary primitive operations are arguably easier for humans to perform. In fact, the two algorithms are equivalent when numbers are represented in binary. \nCompass and Straightedge \nClassical Greek geometers identified numbers (or more accurately, magnitudes) with line segments of the appropriate length, which they manipulated using two simple mechanical tools—the compass and the straightedge—versions of which had already been in common use by surveyors, architects, and other artisans for centuries. Using only these two tools, these scholars reduced several complex geometric constructions to the following primitive operations, starting with one or more identified reference points. \n• Draw the unique line passing through two distinct identified points.   \n• Draw the unique circle centered at one identified point and passing through another.   \n• Identify the intersection point (if any) of two lines.   \n• Identify the intersection points (if any) of a line and a circle.   \n• Identify the intersection points (if any) of two circles. \nIn practice, Greek geometry students almost certainly drew their constructions on an abax $( overrightarrow { a } vert 3 overrightarrow { a } dot { xi } )$ , a table covered in dust or sand.11 Centuries earlier, Egyptian surveyors carried out many of the same constructions using ropes to determine straight lines and circles on the ground.12 However, Euclid and other Greek geometers presented compass and straightedge constructions as precise mathematical abstractions—points are ideal points; lines are ideal lines; and circles are ideal circles. \nFigure 0.4 shows an algorithm, described in Euclid’s Elements about 2500 years ago, for multiplying or dividing two magnitudes. The input consists of four distinct points $A , B , C$ , and $D$ , and the goal is to construct a point $Z$ such that $| A Z | = | A C | | A D | / | A B |$ . In particular, if we define $left| A B right|$ to be our unit of length, then the algorithm computes the product of $left| A C right|$ and $| A D |$ . \nNotice that Euclid first defines a new primitive operation RightAngle by (as modern programmers would phrase it) writing a subroutine. The correctness \nConstruct the line perpendicular to ℓ passing through P.   \nRightAngle(ℓ, P): Choose a point $A in { ell }$ $A , B gets$ Intersect(Circle(P, A), ℓ) $C , D gets mathrm { I N T E R S E C T } big ( mathrm { C I R C L E } ( A , B ) , mathrm { C I R C L E } ( B , A ) big )$ return Line $( C , D )$   \nConstruct a point $Z$ such that $| A Z | = | A C | | A D | / | A B | . rangle rangle$   \nMultiplyOrDivide(A, B, C, D): $alpha gets$ RightAngle(Line(A, C),A) $E gets$ Intersect(Circle(A, B), α) $beta$ $F gets$ Intersect(Circle(A, D), α) $beta gets$ RightAngle(Line(E, C), F) A E $alpha$ 1 $gamma gets$ RightAngle(β, F) return Intersect(γ, Line(A, C)) γ \nof the algorithm follows from the observation that triangles $A C E$ and $A Z F$ are similar. The second and third lines of the main algorithm are ambiguous, because $alpha$ intersects any circle centered at $A$ at two distinct points, but the algorithm is actually correct no matter which intersection points are chosen for $E$ and $F$ . \nEuclid’s algorithm reduces the problem of multiplying two magnitudes (lengths) to a series of primitive compass-and-straightedge operations. These operations are difficult to implement precisely on a modern digital computer, but Euclid’s algorithm wasn’t designed for a digital computer. It was designed for the Platonic Ideal Geometer, wielding the Platonic Ideal Compass and the Platonic Ideal Straightedge, who could execute each operation perfectly in constant time by definition. In this model of computation, MultiplyOrDivide runs in $O ( 1 )$ time! \n0.3 Congressional Apportionment \nHere is another real-world example of an algorithm of significant political importance. Article I, Section 2 of the United States Constitution requires that \nRepresentatives and direct Taxes shall be apportioned among the several States which may be included within this Union, according to their respective Numbers.... The Number of Representatives shall not exceed one for every thirty Thousand, but each State shall have at Least one Representative... \nBecause there are only a finite number of seats in the House of Representatives, exact proportional representation requires either shared or fractional representatives, neither of which are legal. As a result, over the next several decades, many different apportionment algorithms were proposed and used to round the ideal fractional solution fairly. The algorithm actually used today, called the Huntington-Hill method or the method of equal proportions, was first suggested by Census Bureau statistician Joseph Hill in 1911, refined by Harvard mathematician Edward Huntington in 1920, adopted into Federal law (2 U.S.C. §2a) in 1941, and survived a Supreme Court challenge in 1992.13",
        "chapter": "Introduction",
        "section": "Multiplication",
        "subsection": "Compass and Straightedge",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 3
      }
    },
    {
      "node_id": "tb1_node6",
      "content": "I cannot emphasize this point enough: Your primary job as an algorithm designer is teaching other people how and why your algorithms work. If you can’t communicate your ideas to other human beings, they may as well not exist. Producing correct and efficient executable code is an important but secondary goal. Convincing yourself, your professors, your (prospective) employers, your colleagues, or your students that you are smart is at best a distant third. \nSpecifying the Problem \nBefore we can even start developing a new algorithm, we have to agree on what problem our algorithm is supposed to solve. Similarly, before we can even start describing an algorithm, we have to describe the problem that the algorithm is supposed to solve. \nAlgorithmic problems are often presented using standard English, in terms of real-world objects. It’s up to us, the algorithm designers, to restate these problems in terms of formal, abstract, mathematical objects—numbers, arrays, lists, graphs, trees, and so on—that we can reason about formally. We must also determine if the problem statement carries any hidden assumptions, and state those assumptions explicitly. (For example, in the song $^ { * } n$ Bottles of Beer on the Wall”, $n$ is always a non-negative integer.19) \nWe may need to refine our specification as we develop the algorithm. For example, our algorithm may require a particular input representation, or produce a particular output representation, that was left unspecified in the original informal problem description. Or our algorithm might actually solve a more general problem than we were originally asked to solve. (This is a common feature of recursive algorithms.) \nThe specification should include just enough detail that someone else could use our algorithm as a black box, without knowing how or why the algorithm actually works. In particular, we must describe the type and meaning of each input parameter, and exactly how the eventual output depends on the input parameters. On the other hand, our specification should deliberately hide any details that are not necessary to use the algorithm as a black box. Let that which does not matter truly slide. \nFor example, the lattice and duplation-and-mediation algorithms both solve the same problem: Given two non-negative integers $x$ and $y$ , each represented as an array of digits, compute the product $x cdot y$ , also represented as an array of digits. To someone using these algorithms, the choice of algorithm is completely irrelevant. On the other hand, the Greek straightedge-and-compass algorithm solves a different problem, because the input and output values are represented by line segments instead of arrays of digits. \nDescribing the Algorithm \nComputer programs are concrete representations of algorithms, but algorithms are not programs. Rather, algorithms are abstract mechanical procedures that can be implemented in any programming language that supports the underlying primitive operations. The idiosyncratic syntactic details of your favorite programming language are utterly irrelevant; focusing on these will only distract you (and your readers) from what’s really going on.20 A good algorithm description is closer to what we should write in the comments of a real program than the code itself. Code is a poor medium for storytelling. \nOn the other hand, a plain English prose description is usually not a good idea either. Algorithms have lots of idiomatic structure—especially conditionals, loops, function calls, and recursion—that are far too easily hidden by unstructured prose. Colloquial English is full of ambiguities and shades of meaning, but algorithms must be described as unambiguously as possible. Prose is a poor medium for precision. \nIn my opinion, the clearest way to present an algorithm is using a combination of pseudocode and structured English. Pseudocode uses the structure of formal programming languages and mathematics to break algorithms into primitive steps; the primitive steps themselves can be written using mathematical notation, pure English, or an appropriate mixture of the two, whatever is clearest. Wellwritten pseudocode reveals the internal structure of the algorithm but hides irrelevant implementation details, making the algorithm easier to understand, analyze, debug, and implement.",
      "metadata": {
        "content": "I cannot emphasize this point enough: Your primary job as an algorithm designer is teaching other people how and why your algorithms work. If you can’t communicate your ideas to other human beings, they may as well not exist. Producing correct and efficient executable code is an important but secondary goal. Convincing yourself, your professors, your (prospective) employers, your colleagues, or your students that you are smart is at best a distant third. \nSpecifying the Problem \nBefore we can even start developing a new algorithm, we have to agree on what problem our algorithm is supposed to solve. Similarly, before we can even start describing an algorithm, we have to describe the problem that the algorithm is supposed to solve. \nAlgorithmic problems are often presented using standard English, in terms of real-world objects. It’s up to us, the algorithm designers, to restate these problems in terms of formal, abstract, mathematical objects—numbers, arrays, lists, graphs, trees, and so on—that we can reason about formally. We must also determine if the problem statement carries any hidden assumptions, and state those assumptions explicitly. (For example, in the song $^ { * } n$ Bottles of Beer on the Wall”, $n$ is always a non-negative integer.19) \nWe may need to refine our specification as we develop the algorithm. For example, our algorithm may require a particular input representation, or produce a particular output representation, that was left unspecified in the original informal problem description. Or our algorithm might actually solve a more general problem than we were originally asked to solve. (This is a common feature of recursive algorithms.) \nThe specification should include just enough detail that someone else could use our algorithm as a black box, without knowing how or why the algorithm actually works. In particular, we must describe the type and meaning of each input parameter, and exactly how the eventual output depends on the input parameters. On the other hand, our specification should deliberately hide any details that are not necessary to use the algorithm as a black box. Let that which does not matter truly slide. \nFor example, the lattice and duplation-and-mediation algorithms both solve the same problem: Given two non-negative integers $x$ and $y$ , each represented as an array of digits, compute the product $x cdot y$ , also represented as an array of digits. To someone using these algorithms, the choice of algorithm is completely irrelevant. On the other hand, the Greek straightedge-and-compass algorithm solves a different problem, because the input and output values are represented by line segments instead of arrays of digits. \nDescribing the Algorithm \nComputer programs are concrete representations of algorithms, but algorithms are not programs. Rather, algorithms are abstract mechanical procedures that can be implemented in any programming language that supports the underlying primitive operations. The idiosyncratic syntactic details of your favorite programming language are utterly irrelevant; focusing on these will only distract you (and your readers) from what’s really going on.20 A good algorithm description is closer to what we should write in the comments of a real program than the code itself. Code is a poor medium for storytelling. \nOn the other hand, a plain English prose description is usually not a good idea either. Algorithms have lots of idiomatic structure—especially conditionals, loops, function calls, and recursion—that are far too easily hidden by unstructured prose. Colloquial English is full of ambiguities and shades of meaning, but algorithms must be described as unambiguously as possible. Prose is a poor medium for precision. \nIn my opinion, the clearest way to present an algorithm is using a combination of pseudocode and structured English. Pseudocode uses the structure of formal programming languages and mathematics to break algorithms into primitive steps; the primitive steps themselves can be written using mathematical notation, pure English, or an appropriate mixture of the two, whatever is clearest. Wellwritten pseudocode reveals the internal structure of the algorithm but hides irrelevant implementation details, making the algorithm easier to understand, analyze, debug, and implement.",
        "chapter": "Introduction",
        "section": "Describing Algorithms",
        "subsection": "Specifying the Problem",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 6
      }
    },
    {
      "node_id": "tb1_node7",
      "content": "For example, the lattice and duplation-and-mediation algorithms both solve the same problem: Given two non-negative integers $x$ and $y$ , each represented as an array of digits, compute the product $x cdot y$ , also represented as an array of digits. To someone using these algorithms, the choice of algorithm is completely irrelevant. On the other hand, the Greek straightedge-and-compass algorithm solves a different problem, because the input and output values are represented by line segments instead of arrays of digits. \nDescribing the Algorithm \nComputer programs are concrete representations of algorithms, but algorithms are not programs. Rather, algorithms are abstract mechanical procedures that can be implemented in any programming language that supports the underlying primitive operations. The idiosyncratic syntactic details of your favorite programming language are utterly irrelevant; focusing on these will only distract you (and your readers) from what’s really going on.20 A good algorithm description is closer to what we should write in the comments of a real program than the code itself. Code is a poor medium for storytelling. \nOn the other hand, a plain English prose description is usually not a good idea either. Algorithms have lots of idiomatic structure—especially conditionals, loops, function calls, and recursion—that are far too easily hidden by unstructured prose. Colloquial English is full of ambiguities and shades of meaning, but algorithms must be described as unambiguously as possible. Prose is a poor medium for precision. \nIn my opinion, the clearest way to present an algorithm is using a combination of pseudocode and structured English. Pseudocode uses the structure of formal programming languages and mathematics to break algorithms into primitive steps; the primitive steps themselves can be written using mathematical notation, pure English, or an appropriate mixture of the two, whatever is clearest. Wellwritten pseudocode reveals the internal structure of the algorithm but hides irrelevant implementation details, making the algorithm easier to understand, analyze, debug, and implement. \nWhenever we describe an algorithm, our description should include every detail necessary to fully specify the algorithm, prove its correctness, and analyze its running time. At the same time, it should exclude any details that are not necessary to fully specify the algorithm, prove its correctness, and analyze its running time. (Slide.) At a more practical level, our description should allow a competent but skeptical programmer who has not read this book to quickly and correctly implement the algorithm in their favorite programming language, without understanding why it works. \nI don’t want to bore you with the rules I follow for writing pseudocode, but I must caution against one especially pernicious habit. Never describe repeated operations informally, as in “Do [this] first, then do [that] second, and so on.” or “Repeat this process until [something]”. As anyone who has taken one of those frustrating “What comes next in this sequence?” tests already knows, describing the first few steps of an algorithm says little or nothing about what happens in later steps. If your algorithm has a loop, write it as a loop, and explicitly describe what happens in an arbitrary iteration. Similarly, if your algorithm is recursive, write it recursively, and explicitly describe the case boundaries and what happens in each case. \n0.6 Analyzing Algorithms \nIt’s not enough just to write down an algorithm and say “Behold!” We must also convince our audience (and ourselves!) that the algorithm actually does what it’s supposed to do, and that it does so efficiently. \nCorrectness \nIn some application settings, it is acceptable for programs to behave correctly most of the time, on all “reasonable” inputs. Not in this book; we require algorithms that are always correct, for all possible inputs. Moreover, we must prove that our algorithms are correct; trusting our instincts, or trying a few test cases, isn’t good enough. Sometimes correctness is truly obvious, especially for algorithms you’ve seen in earlier courses. On the other hand, “obvious” is all too often a synonym for “wrong”. Most of the algorithms we discuss in this course require real work to prove correct. In particular, correctness proofs usually involve induction. We like induction. Induction is our friend.21 \nOf course, before we can formally prove that our algorithm does what it’s supposed to do, we have to formally describe what it’s supposed to do!",
      "metadata": {
        "content": "For example, the lattice and duplation-and-mediation algorithms both solve the same problem: Given two non-negative integers $x$ and $y$ , each represented as an array of digits, compute the product $x cdot y$ , also represented as an array of digits. To someone using these algorithms, the choice of algorithm is completely irrelevant. On the other hand, the Greek straightedge-and-compass algorithm solves a different problem, because the input and output values are represented by line segments instead of arrays of digits. \nDescribing the Algorithm \nComputer programs are concrete representations of algorithms, but algorithms are not programs. Rather, algorithms are abstract mechanical procedures that can be implemented in any programming language that supports the underlying primitive operations. The idiosyncratic syntactic details of your favorite programming language are utterly irrelevant; focusing on these will only distract you (and your readers) from what’s really going on.20 A good algorithm description is closer to what we should write in the comments of a real program than the code itself. Code is a poor medium for storytelling. \nOn the other hand, a plain English prose description is usually not a good idea either. Algorithms have lots of idiomatic structure—especially conditionals, loops, function calls, and recursion—that are far too easily hidden by unstructured prose. Colloquial English is full of ambiguities and shades of meaning, but algorithms must be described as unambiguously as possible. Prose is a poor medium for precision. \nIn my opinion, the clearest way to present an algorithm is using a combination of pseudocode and structured English. Pseudocode uses the structure of formal programming languages and mathematics to break algorithms into primitive steps; the primitive steps themselves can be written using mathematical notation, pure English, or an appropriate mixture of the two, whatever is clearest. Wellwritten pseudocode reveals the internal structure of the algorithm but hides irrelevant implementation details, making the algorithm easier to understand, analyze, debug, and implement. \nWhenever we describe an algorithm, our description should include every detail necessary to fully specify the algorithm, prove its correctness, and analyze its running time. At the same time, it should exclude any details that are not necessary to fully specify the algorithm, prove its correctness, and analyze its running time. (Slide.) At a more practical level, our description should allow a competent but skeptical programmer who has not read this book to quickly and correctly implement the algorithm in their favorite programming language, without understanding why it works. \nI don’t want to bore you with the rules I follow for writing pseudocode, but I must caution against one especially pernicious habit. Never describe repeated operations informally, as in “Do [this] first, then do [that] second, and so on.” or “Repeat this process until [something]”. As anyone who has taken one of those frustrating “What comes next in this sequence?” tests already knows, describing the first few steps of an algorithm says little or nothing about what happens in later steps. If your algorithm has a loop, write it as a loop, and explicitly describe what happens in an arbitrary iteration. Similarly, if your algorithm is recursive, write it recursively, and explicitly describe the case boundaries and what happens in each case. \n0.6 Analyzing Algorithms \nIt’s not enough just to write down an algorithm and say “Behold!” We must also convince our audience (and ourselves!) that the algorithm actually does what it’s supposed to do, and that it does so efficiently. \nCorrectness \nIn some application settings, it is acceptable for programs to behave correctly most of the time, on all “reasonable” inputs. Not in this book; we require algorithms that are always correct, for all possible inputs. Moreover, we must prove that our algorithms are correct; trusting our instincts, or trying a few test cases, isn’t good enough. Sometimes correctness is truly obvious, especially for algorithms you’ve seen in earlier courses. On the other hand, “obvious” is all too often a synonym for “wrong”. Most of the algorithms we discuss in this course require real work to prove correct. In particular, correctness proofs usually involve induction. We like induction. Induction is our friend.21 \nOf course, before we can formally prove that our algorithm does what it’s supposed to do, we have to formally describe what it’s supposed to do!",
        "chapter": "Introduction",
        "section": "Describing Algorithms",
        "subsection": "Describing the Algorithm",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 7
      }
    },
    {
      "node_id": "tb1_node9",
      "content": "Running Time \nThe most common way of ranking different algorithms for the same problem is by how quickly they run. Ideally, we want the fastest possible algorithm for any particular problem. In many application settings, it is acceptable for programs to run efficiently most of the time, on all “reasonable” inputs. Not in this book; we require algorithms that always run efficiently, even in the worst case. \nBut how do we measure running time? As a specific example, how long does it take to sing the song BottlesOfBeer(n)? This is obviously a function of the input value $n$ , but it also depends on how quickly you can sing. Some singers might take ten seconds to sing a verse; others might take twenty. Technology widens the possibilities even further. Dictating the song over a telegraph using Morse code might take a full minute per verse. Downloading an mp3 over the Web might take a tenth of a second per verse. Duplicating the mp3 in a computer’s main memory might take only a few microseconds per verse. \nWhat’s important here is how the singing time changes as n grows. Singing BottlesOfBeer $( 2 n )$ requires about twice much time as singing BottlesOf$mathtt { B E E R } ( n )$ , no matter what technology is being used. This is reflected in the asymptotic singing time $Theta ( n )$ . \nWe can measure time by counting how many times the algorithm executes a certain instruction or reaches a certain milestone in the “code”. For example, we might notice that the word “beer” is sung three times in every verse of BottlesOfBeer, so the number of times you sing “beer” is a good indication of the total singing time. For this question, we can give an exact answer: BottlesOfBeer $. ( n )$ mentions beer exactly $3 n + 3$ times. \nIncidentally, there are lots of songs with quadratic singing time. This one is probably familiar to most English-speakers: \nThe input to NDaysOfChristmas is a list of $n - 1$ gifts, represented here as an array. It’s quite easy to show that the singing time is $Theta ( n ^ { 2 } )$ ; in particular, the singer mentions the name of a gift $textstyle sum _ { i = 1 } ^ { n } i = n ( n + 1 ) / 2$ times (counting the partridge in the pear tree). It’s also easy to see that during the first $n$ days of Christmas, my true love gave to me exactly $textstyle sum _ { i = 1 } ^ { n } sum _ { j = 1 } ^ { i } j = n ( n + 1 ) ( n + 2 ) / 6 =$ $Theta ( n ^ { 3 } )$ gifts. \nOther quadratic-time songs include “Old MacDonald Had a Farm”, “There Was an Old Lady Who Swallowed a Fly”, “Hole in the Bottom of the Sea”, “Green Grow the Rushes $omega ^ { prime }$ , “The Rattlin’ Bog”, “The Court Of King Caractacus”,“The Barley-Mow”, “If I Were Not Upon the Stage”, “Star Trekkin’ ”,“Ist das nicht ein Schnitzelbank?”,22“Il Pulcino Pio”, “Minkurinn í hænsnakofanum”, “Echad Mi Yodea”, and “Το κοκοράκι”. For more examples, consult your favorite preschooler. \nA few songs have even more bizarre singing times. A fairly modern example is “The TELNET Song” by Guy Steele, which actually takes $Theta ( 2 ^ { n } )$ time to sing the first $n$ verses; Steele recommended $n = 4$ . Finally, there are some songs that never end.23 \nExcept for “The TELNET Song”, all of these songs are most naturally expressed as a small set of nested loops, so their running singing times can be computed using nested summations. The running time of a recursive algorithm is more easily expressed as a recurrence. For example, the peasant multiplication algorithm can be expressed recursively as follows: \nLet $T ( x , y )$ denote the number of parity, addition, and mediation operations required to compute $x cdot y$ . This function satisfies the recursive inequality $T ( x , y ) leq T ( lfloor x / 2 rfloor , 2 y ) + 2$ with base case $T ( 0 , y ) = 0$ . Techniques described in the next chapter imply the upper bound $T ( x , y ) = O ( log x )$ . \nSometimes the running time of an algorithm depends on a particular implementation of some underlying data structure of subroutine. For example, the Huntington-Hill apportionment algorithm ApportionCongress runs in $O ( N + R I + ( R - n ) E )$ time, where $N$ denotes the running time of NewPriorityQueue, $I$ denotes the running time of Insert, and $E$ denotes the running time of ExtractMax. Under the reasonable assumption that $R geq 2 n$ (on average, each state gets at least two representatives), we can simplify this bound to $O ( N + R ( I + E ) )$ . The precise running time depends on the implementation of the underlying priority queue. The Census Bureau implements the priority queue as an unsorted array, which gives us $N = I = Theta ( 1 )$ and $E = Theta ( n )$ , so the Census Bureau’s implementation of ApportionCongress runs in $O ( R n )$ time. However, if we implement the priority queue as a binary heap or a heap-ordered array, we have $N = Theta ( 1 )$ and $I = E = O ( log n )$ , so the overall algorithm runs in $O ( R log { n } )$ time. \n\nFinally, sometimes we are interested in computational resources other than time, such as space, number of coin flips, number of cache or page faults, number of inter-process messages, or the number of gifts my true love gave to me. These resources can be analyzed using the same techniques used to analyze running time. For example, lattice multiplication of two $n$ -digit numbers requires $O ( n ^ { 2 } )$ space if we write down all the partial products before adding them, but only $O ( n )$ space if we add them on the fly. \nExercises \n0. Describe and analyze an efficient algorithm that determines, given a legal arrangement of standard pieces on a standard chess board, which player will win at chess from the given starting position if both players play perfectly. [Hint: There is a trivial one-line solution!]   \nn1. (a) Identify (or write) a song that requires $Theta ( n ^ { 3 } )$ time to sing the first $n$ verses. (b) Identify (or write) a song that requires $Theta ( n log n )$ time to sing the first n verses. (c) Identify (or write) a song that requires some other weird amount of time to sing the first n verses. \n2. Careful readers might complain that our analysis of songs like $^ { * } n$ Bottles of Beer on the Wall” or “The n Days of Christmas” is overly simplistic, because larger numbers take longer to sing than shorter numbers. More generally, because there are only so many words of a given length, larger sets of words necessarily contain longer words.24 We can more accurately estimate singing time by counting the number of syllables sung, rather than the number of words. \n(a) How long does it take to sing the integer n?",
      "metadata": {
        "content": "Running Time \nThe most common way of ranking different algorithms for the same problem is by how quickly they run. Ideally, we want the fastest possible algorithm for any particular problem. In many application settings, it is acceptable for programs to run efficiently most of the time, on all “reasonable” inputs. Not in this book; we require algorithms that always run efficiently, even in the worst case. \nBut how do we measure running time? As a specific example, how long does it take to sing the song BottlesOfBeer(n)? This is obviously a function of the input value $n$ , but it also depends on how quickly you can sing. Some singers might take ten seconds to sing a verse; others might take twenty. Technology widens the possibilities even further. Dictating the song over a telegraph using Morse code might take a full minute per verse. Downloading an mp3 over the Web might take a tenth of a second per verse. Duplicating the mp3 in a computer’s main memory might take only a few microseconds per verse. \nWhat’s important here is how the singing time changes as n grows. Singing BottlesOfBeer $( 2 n )$ requires about twice much time as singing BottlesOf$mathtt { B E E R } ( n )$ , no matter what technology is being used. This is reflected in the asymptotic singing time $Theta ( n )$ . \nWe can measure time by counting how many times the algorithm executes a certain instruction or reaches a certain milestone in the “code”. For example, we might notice that the word “beer” is sung three times in every verse of BottlesOfBeer, so the number of times you sing “beer” is a good indication of the total singing time. For this question, we can give an exact answer: BottlesOfBeer $. ( n )$ mentions beer exactly $3 n + 3$ times. \nIncidentally, there are lots of songs with quadratic singing time. This one is probably familiar to most English-speakers: \nThe input to NDaysOfChristmas is a list of $n - 1$ gifts, represented here as an array. It’s quite easy to show that the singing time is $Theta ( n ^ { 2 } )$ ; in particular, the singer mentions the name of a gift $textstyle sum _ { i = 1 } ^ { n } i = n ( n + 1 ) / 2$ times (counting the partridge in the pear tree). It’s also easy to see that during the first $n$ days of Christmas, my true love gave to me exactly $textstyle sum _ { i = 1 } ^ { n } sum _ { j = 1 } ^ { i } j = n ( n + 1 ) ( n + 2 ) / 6 =$ $Theta ( n ^ { 3 } )$ gifts. \nOther quadratic-time songs include “Old MacDonald Had a Farm”, “There Was an Old Lady Who Swallowed a Fly”, “Hole in the Bottom of the Sea”, “Green Grow the Rushes $omega ^ { prime }$ , “The Rattlin’ Bog”, “The Court Of King Caractacus”,“The Barley-Mow”, “If I Were Not Upon the Stage”, “Star Trekkin’ ”,“Ist das nicht ein Schnitzelbank?”,22“Il Pulcino Pio”, “Minkurinn í hænsnakofanum”, “Echad Mi Yodea”, and “Το κοκοράκι”. For more examples, consult your favorite preschooler. \nA few songs have even more bizarre singing times. A fairly modern example is “The TELNET Song” by Guy Steele, which actually takes $Theta ( 2 ^ { n } )$ time to sing the first $n$ verses; Steele recommended $n = 4$ . Finally, there are some songs that never end.23 \nExcept for “The TELNET Song”, all of these songs are most naturally expressed as a small set of nested loops, so their running singing times can be computed using nested summations. The running time of a recursive algorithm is more easily expressed as a recurrence. For example, the peasant multiplication algorithm can be expressed recursively as follows: \nLet $T ( x , y )$ denote the number of parity, addition, and mediation operations required to compute $x cdot y$ . This function satisfies the recursive inequality $T ( x , y ) leq T ( lfloor x / 2 rfloor , 2 y ) + 2$ with base case $T ( 0 , y ) = 0$ . Techniques described in the next chapter imply the upper bound $T ( x , y ) = O ( log x )$ . \nSometimes the running time of an algorithm depends on a particular implementation of some underlying data structure of subroutine. For example, the Huntington-Hill apportionment algorithm ApportionCongress runs in $O ( N + R I + ( R - n ) E )$ time, where $N$ denotes the running time of NewPriorityQueue, $I$ denotes the running time of Insert, and $E$ denotes the running time of ExtractMax. Under the reasonable assumption that $R geq 2 n$ (on average, each state gets at least two representatives), we can simplify this bound to $O ( N + R ( I + E ) )$ . The precise running time depends on the implementation of the underlying priority queue. The Census Bureau implements the priority queue as an unsorted array, which gives us $N = I = Theta ( 1 )$ and $E = Theta ( n )$ , so the Census Bureau’s implementation of ApportionCongress runs in $O ( R n )$ time. However, if we implement the priority queue as a binary heap or a heap-ordered array, we have $N = Theta ( 1 )$ and $I = E = O ( log n )$ , so the overall algorithm runs in $O ( R log { n } )$ time. \n\nFinally, sometimes we are interested in computational resources other than time, such as space, number of coin flips, number of cache or page faults, number of inter-process messages, or the number of gifts my true love gave to me. These resources can be analyzed using the same techniques used to analyze running time. For example, lattice multiplication of two $n$ -digit numbers requires $O ( n ^ { 2 } )$ space if we write down all the partial products before adding them, but only $O ( n )$ space if we add them on the fly. \nExercises \n0. Describe and analyze an efficient algorithm that determines, given a legal arrangement of standard pieces on a standard chess board, which player will win at chess from the given starting position if both players play perfectly. [Hint: There is a trivial one-line solution!]   \nn1. (a) Identify (or write) a song that requires $Theta ( n ^ { 3 } )$ time to sing the first $n$ verses. (b) Identify (or write) a song that requires $Theta ( n log n )$ time to sing the first n verses. (c) Identify (or write) a song that requires some other weird amount of time to sing the first n verses. \n2. Careful readers might complain that our analysis of songs like $^ { * } n$ Bottles of Beer on the Wall” or “The n Days of Christmas” is overly simplistic, because larger numbers take longer to sing than shorter numbers. More generally, because there are only so many words of a given length, larger sets of words necessarily contain longer words.24 We can more accurately estimate singing time by counting the number of syllables sung, rather than the number of words. \n(a) How long does it take to sing the integer n?",
        "chapter": "Introduction",
        "section": "Analyzing Algorithms",
        "subsection": "Running Time",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 9
      }
    },
    {
      "node_id": "tb1_node5",
      "content": "Similar apportionment algorithms are used in multi-party parliamentary elections around the world, where the number of seats allocated to each party is supposed to be proportional to the number of votes that party receives. The two most common are the $D$ ’Hondt method14 and the Webster–Sainte-Laguë method,15 which respectively use priorities $P / ( r + 1 )$ and $P / ( 2 r + 1 )$ in place of the square-root expression in Huntington-Hill. The Huntington-Hill method is essentially unique to the United States House of Representatives, thanks in part to the constitutional requirement that each state must be allocated at least one representative. \n0.4 A Bad Example \nAs a prototypical example of a sequence of instructions that is not actually an algorithm, consider \"Martin’s algorithm”:16 \nBeAMillionaireAndNeverPayTaxes(): Get a million dollars. If the tax man comes to your door and says, “You have never paid taxes!” Say “I forgot.” \nPretty simple, except for that first step; it’s a doozy! A group of billionaire CEOs, Silicon Valley venture capitalists, or New York City real-estate hustlers might consider this an algorithm, because for them the first step is both unambiguous and trivial,17 but for the rest of us poor slobs, Martin’s procedure is too vague to be considered an actual algorithm. On the other hand, this is a perfect example of a reduction—it reduces the problem of being a millionaire and never paying taxes to the “easier” problem of acquiring a million dollars. We’ll see reductions over and over again in this book. As hundreds of businessmen and politicians have demonstrated, if you know how to solve the easier problem, a reduction tells you how to solve the harder one. \nMartin’s algorithm, like some of our previous examples, is not the kind of algorithm that computer scientists are used to thinking about, because it is phrased in terms of operations that are difficult for computers to perform. This book focuses (almost!) exclusively on algorithms that can be reasonably implemented on a standard digital computer. Each step in these algorithms is either directly supported by common programming languages (such as arithmetic, assignments, loops, or recursion) or something that you’ve already learned how to do (like sorting, binary search, tree traversal, or singing $^ { * } n$ Bottles of Beer on the Wall”). \n0.5 Describing Algorithms \nThe skills required to effectively design and analyze algorithms are entangled with the skills required to effectively describe algorithms. At least in my classes, a complete description of any algorithm has four components: \n• What: A precise specification of the problem that the algorithm solves.   \n• How: A precise description of the algorithm itself.   \n• Why: A proof that the algorithm solves the problem it is supposed to solve.   \n• How fast: An analysis of the running time of the algorithm. \nIt is not necessary (or even advisable) to develop these four components in this particular order. Problem specifications, algorithm descriptions, correctness proofs, and time analyses usually evolve simultaneously, with the development of each component informing the development of the others. For example, we may need to tweak the problem description to support a faster algorithm, or modify the algorithm to handle a tricky case in the proof of correctness. Nevertheless, presenting these components separately is usually clearest for the reader. \nAs with any writing, it’s important to aim your descriptions at the right audience; I recommend writing for a competent but skeptical programmer who is not as clever as you are. Think of yourself six months ago. As you develop any new algorithm, you will naturally build up lots of intuition about the problem and about how your algorithm solves it, and your informal reasoning will be guided by that intuition. But anyone reading your algorithm later, or the code you derive from it, won’t share your intuition or experience. Neither will your compiler. Neither will you six months from now. All they will have is your written description. \nEven if you never have to explain your algorithms to anyone else, it’s still important to develop them with an audience in mind. Trying to communicate clearly forces you to think more clearly. In particular, writing for a novice audience, who will interpret your words exactly as written, forces you to work through fine details, no matter how “obvious” or “intuitive” your high-level ideas may seem at the moment. Similarly, writing for a skeptical audience forces you to develop robust arguments for correctness and efficiency, instead of trusting your intuition or your intelligence.18",
      "metadata": {
        "content": "Similar apportionment algorithms are used in multi-party parliamentary elections around the world, where the number of seats allocated to each party is supposed to be proportional to the number of votes that party receives. The two most common are the $D$ ’Hondt method14 and the Webster–Sainte-Laguë method,15 which respectively use priorities $P / ( r + 1 )$ and $P / ( 2 r + 1 )$ in place of the square-root expression in Huntington-Hill. The Huntington-Hill method is essentially unique to the United States House of Representatives, thanks in part to the constitutional requirement that each state must be allocated at least one representative. \n0.4 A Bad Example \nAs a prototypical example of a sequence of instructions that is not actually an algorithm, consider \"Martin’s algorithm”:16 \nBeAMillionaireAndNeverPayTaxes(): Get a million dollars. If the tax man comes to your door and says, “You have never paid taxes!” Say “I forgot.” \nPretty simple, except for that first step; it’s a doozy! A group of billionaire CEOs, Silicon Valley venture capitalists, or New York City real-estate hustlers might consider this an algorithm, because for them the first step is both unambiguous and trivial,17 but for the rest of us poor slobs, Martin’s procedure is too vague to be considered an actual algorithm. On the other hand, this is a perfect example of a reduction—it reduces the problem of being a millionaire and never paying taxes to the “easier” problem of acquiring a million dollars. We’ll see reductions over and over again in this book. As hundreds of businessmen and politicians have demonstrated, if you know how to solve the easier problem, a reduction tells you how to solve the harder one. \nMartin’s algorithm, like some of our previous examples, is not the kind of algorithm that computer scientists are used to thinking about, because it is phrased in terms of operations that are difficult for computers to perform. This book focuses (almost!) exclusively on algorithms that can be reasonably implemented on a standard digital computer. Each step in these algorithms is either directly supported by common programming languages (such as arithmetic, assignments, loops, or recursion) or something that you’ve already learned how to do (like sorting, binary search, tree traversal, or singing $^ { * } n$ Bottles of Beer on the Wall”). \n0.5 Describing Algorithms \nThe skills required to effectively design and analyze algorithms are entangled with the skills required to effectively describe algorithms. At least in my classes, a complete description of any algorithm has four components: \n• What: A precise specification of the problem that the algorithm solves.   \n• How: A precise description of the algorithm itself.   \n• Why: A proof that the algorithm solves the problem it is supposed to solve.   \n• How fast: An analysis of the running time of the algorithm. \nIt is not necessary (or even advisable) to develop these four components in this particular order. Problem specifications, algorithm descriptions, correctness proofs, and time analyses usually evolve simultaneously, with the development of each component informing the development of the others. For example, we may need to tweak the problem description to support a faster algorithm, or modify the algorithm to handle a tricky case in the proof of correctness. Nevertheless, presenting these components separately is usually clearest for the reader. \nAs with any writing, it’s important to aim your descriptions at the right audience; I recommend writing for a competent but skeptical programmer who is not as clever as you are. Think of yourself six months ago. As you develop any new algorithm, you will naturally build up lots of intuition about the problem and about how your algorithm solves it, and your informal reasoning will be guided by that intuition. But anyone reading your algorithm later, or the code you derive from it, won’t share your intuition or experience. Neither will your compiler. Neither will you six months from now. All they will have is your written description. \nEven if you never have to explain your algorithms to anyone else, it’s still important to develop them with an audience in mind. Trying to communicate clearly forces you to think more clearly. In particular, writing for a novice audience, who will interpret your words exactly as written, forces you to work through fine details, no matter how “obvious” or “intuitive” your high-level ideas may seem at the moment. Similarly, writing for a skeptical audience forces you to develop robust arguments for correctness and efficiency, instead of trusting your intuition or your intelligence.18",
        "chapter": "Introduction",
        "section": "A Bad Example",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 5
      }
    },
    {
      "node_id": "tb1_node8",
      "content": "Whenever we describe an algorithm, our description should include every detail necessary to fully specify the algorithm, prove its correctness, and analyze its running time. At the same time, it should exclude any details that are not necessary to fully specify the algorithm, prove its correctness, and analyze its running time. (Slide.) At a more practical level, our description should allow a competent but skeptical programmer who has not read this book to quickly and correctly implement the algorithm in their favorite programming language, without understanding why it works. \nI don’t want to bore you with the rules I follow for writing pseudocode, but I must caution against one especially pernicious habit. Never describe repeated operations informally, as in “Do [this] first, then do [that] second, and so on.” or “Repeat this process until [something]”. As anyone who has taken one of those frustrating “What comes next in this sequence?” tests already knows, describing the first few steps of an algorithm says little or nothing about what happens in later steps. If your algorithm has a loop, write it as a loop, and explicitly describe what happens in an arbitrary iteration. Similarly, if your algorithm is recursive, write it recursively, and explicitly describe the case boundaries and what happens in each case. \n0.6 Analyzing Algorithms \nIt’s not enough just to write down an algorithm and say “Behold!” We must also convince our audience (and ourselves!) that the algorithm actually does what it’s supposed to do, and that it does so efficiently. \nCorrectness \nIn some application settings, it is acceptable for programs to behave correctly most of the time, on all “reasonable” inputs. Not in this book; we require algorithms that are always correct, for all possible inputs. Moreover, we must prove that our algorithms are correct; trusting our instincts, or trying a few test cases, isn’t good enough. Sometimes correctness is truly obvious, especially for algorithms you’ve seen in earlier courses. On the other hand, “obvious” is all too often a synonym for “wrong”. Most of the algorithms we discuss in this course require real work to prove correct. In particular, correctness proofs usually involve induction. We like induction. Induction is our friend.21 \nOf course, before we can formally prove that our algorithm does what it’s supposed to do, we have to formally describe what it’s supposed to do! \nRunning Time \nThe most common way of ranking different algorithms for the same problem is by how quickly they run. Ideally, we want the fastest possible algorithm for any particular problem. In many application settings, it is acceptable for programs to run efficiently most of the time, on all “reasonable” inputs. Not in this book; we require algorithms that always run efficiently, even in the worst case. \nBut how do we measure running time? As a specific example, how long does it take to sing the song BottlesOfBeer(n)? This is obviously a function of the input value $n$ , but it also depends on how quickly you can sing. Some singers might take ten seconds to sing a verse; others might take twenty. Technology widens the possibilities even further. Dictating the song over a telegraph using Morse code might take a full minute per verse. Downloading an mp3 over the Web might take a tenth of a second per verse. Duplicating the mp3 in a computer’s main memory might take only a few microseconds per verse. \nWhat’s important here is how the singing time changes as n grows. Singing BottlesOfBeer $( 2 n )$ requires about twice much time as singing BottlesOf$mathtt { B E E R } ( n )$ , no matter what technology is being used. This is reflected in the asymptotic singing time $Theta ( n )$ . \nWe can measure time by counting how many times the algorithm executes a certain instruction or reaches a certain milestone in the “code”. For example, we might notice that the word “beer” is sung three times in every verse of BottlesOfBeer, so the number of times you sing “beer” is a good indication of the total singing time. For this question, we can give an exact answer: BottlesOfBeer $. ( n )$ mentions beer exactly $3 n + 3$ times. \nIncidentally, there are lots of songs with quadratic singing time. This one is probably familiar to most English-speakers: \nThe input to NDaysOfChristmas is a list of $n - 1$ gifts, represented here as an array. It’s quite easy to show that the singing time is $Theta ( n ^ { 2 } )$ ; in particular, the singer mentions the name of a gift $textstyle sum _ { i = 1 } ^ { n } i = n ( n + 1 ) / 2$ times (counting the partridge in the pear tree). It’s also easy to see that during the first $n$ days of Christmas, my true love gave to me exactly $textstyle sum _ { i = 1 } ^ { n } sum _ { j = 1 } ^ { i } j = n ( n + 1 ) ( n + 2 ) / 6 =$ $Theta ( n ^ { 3 } )$ gifts.",
      "metadata": {
        "content": "Whenever we describe an algorithm, our description should include every detail necessary to fully specify the algorithm, prove its correctness, and analyze its running time. At the same time, it should exclude any details that are not necessary to fully specify the algorithm, prove its correctness, and analyze its running time. (Slide.) At a more practical level, our description should allow a competent but skeptical programmer who has not read this book to quickly and correctly implement the algorithm in their favorite programming language, without understanding why it works. \nI don’t want to bore you with the rules I follow for writing pseudocode, but I must caution against one especially pernicious habit. Never describe repeated operations informally, as in “Do [this] first, then do [that] second, and so on.” or “Repeat this process until [something]”. As anyone who has taken one of those frustrating “What comes next in this sequence?” tests already knows, describing the first few steps of an algorithm says little or nothing about what happens in later steps. If your algorithm has a loop, write it as a loop, and explicitly describe what happens in an arbitrary iteration. Similarly, if your algorithm is recursive, write it recursively, and explicitly describe the case boundaries and what happens in each case. \n0.6 Analyzing Algorithms \nIt’s not enough just to write down an algorithm and say “Behold!” We must also convince our audience (and ourselves!) that the algorithm actually does what it’s supposed to do, and that it does so efficiently. \nCorrectness \nIn some application settings, it is acceptable for programs to behave correctly most of the time, on all “reasonable” inputs. Not in this book; we require algorithms that are always correct, for all possible inputs. Moreover, we must prove that our algorithms are correct; trusting our instincts, or trying a few test cases, isn’t good enough. Sometimes correctness is truly obvious, especially for algorithms you’ve seen in earlier courses. On the other hand, “obvious” is all too often a synonym for “wrong”. Most of the algorithms we discuss in this course require real work to prove correct. In particular, correctness proofs usually involve induction. We like induction. Induction is our friend.21 \nOf course, before we can formally prove that our algorithm does what it’s supposed to do, we have to formally describe what it’s supposed to do! \nRunning Time \nThe most common way of ranking different algorithms for the same problem is by how quickly they run. Ideally, we want the fastest possible algorithm for any particular problem. In many application settings, it is acceptable for programs to run efficiently most of the time, on all “reasonable” inputs. Not in this book; we require algorithms that always run efficiently, even in the worst case. \nBut how do we measure running time? As a specific example, how long does it take to sing the song BottlesOfBeer(n)? This is obviously a function of the input value $n$ , but it also depends on how quickly you can sing. Some singers might take ten seconds to sing a verse; others might take twenty. Technology widens the possibilities even further. Dictating the song over a telegraph using Morse code might take a full minute per verse. Downloading an mp3 over the Web might take a tenth of a second per verse. Duplicating the mp3 in a computer’s main memory might take only a few microseconds per verse. \nWhat’s important here is how the singing time changes as n grows. Singing BottlesOfBeer $( 2 n )$ requires about twice much time as singing BottlesOf$mathtt { B E E R } ( n )$ , no matter what technology is being used. This is reflected in the asymptotic singing time $Theta ( n )$ . \nWe can measure time by counting how many times the algorithm executes a certain instruction or reaches a certain milestone in the “code”. For example, we might notice that the word “beer” is sung three times in every verse of BottlesOfBeer, so the number of times you sing “beer” is a good indication of the total singing time. For this question, we can give an exact answer: BottlesOfBeer $. ( n )$ mentions beer exactly $3 n + 3$ times. \nIncidentally, there are lots of songs with quadratic singing time. This one is probably familiar to most English-speakers: \nThe input to NDaysOfChristmas is a list of $n - 1$ gifts, represented here as an array. It’s quite easy to show that the singing time is $Theta ( n ^ { 2 } )$ ; in particular, the singer mentions the name of a gift $textstyle sum _ { i = 1 } ^ { n } i = n ( n + 1 ) / 2$ times (counting the partridge in the pear tree). It’s also easy to see that during the first $n$ days of Christmas, my true love gave to me exactly $textstyle sum _ { i = 1 } ^ { n } sum _ { j = 1 } ^ { i } j = n ( n + 1 ) ( n + 2 ) / 6 =$ $Theta ( n ^ { 3 } )$ gifts.",
        "chapter": "Introduction",
        "section": "Analyzing Algorithms",
        "subsection": "Correctness",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 8
      }
    },
    {
      "node_id": "tb1_node4",
      "content": "Construct the line perpendicular to ℓ passing through P.   \nRightAngle(ℓ, P): Choose a point $A in { ell }$ $A , B gets$ Intersect(Circle(P, A), ℓ) $C , D gets mathrm { I N T E R S E C T } big ( mathrm { C I R C L E } ( A , B ) , mathrm { C I R C L E } ( B , A ) big )$ return Line $( C , D )$   \nConstruct a point $Z$ such that $| A Z | = | A C | | A D | / | A B | . rangle rangle$   \nMultiplyOrDivide(A, B, C, D): $alpha gets$ RightAngle(Line(A, C),A) $E gets$ Intersect(Circle(A, B), α) $beta$ $F gets$ Intersect(Circle(A, D), α) $beta gets$ RightAngle(Line(E, C), F) A E $alpha$ 1 $gamma gets$ RightAngle(β, F) return Intersect(γ, Line(A, C)) γ \nof the algorithm follows from the observation that triangles $A C E$ and $A Z F$ are similar. The second and third lines of the main algorithm are ambiguous, because $alpha$ intersects any circle centered at $A$ at two distinct points, but the algorithm is actually correct no matter which intersection points are chosen for $E$ and $F$ . \nEuclid’s algorithm reduces the problem of multiplying two magnitudes (lengths) to a series of primitive compass-and-straightedge operations. These operations are difficult to implement precisely on a modern digital computer, but Euclid’s algorithm wasn’t designed for a digital computer. It was designed for the Platonic Ideal Geometer, wielding the Platonic Ideal Compass and the Platonic Ideal Straightedge, who could execute each operation perfectly in constant time by definition. In this model of computation, MultiplyOrDivide runs in $O ( 1 )$ time! \n0.3 Congressional Apportionment \nHere is another real-world example of an algorithm of significant political importance. Article I, Section 2 of the United States Constitution requires that \nRepresentatives and direct Taxes shall be apportioned among the several States which may be included within this Union, according to their respective Numbers.... The Number of Representatives shall not exceed one for every thirty Thousand, but each State shall have at Least one Representative... \nBecause there are only a finite number of seats in the House of Representatives, exact proportional representation requires either shared or fractional representatives, neither of which are legal. As a result, over the next several decades, many different apportionment algorithms were proposed and used to round the ideal fractional solution fairly. The algorithm actually used today, called the Huntington-Hill method or the method of equal proportions, was first suggested by Census Bureau statistician Joseph Hill in 1911, refined by Harvard mathematician Edward Huntington in 1920, adopted into Federal law (2 U.S.C. §2a) in 1941, and survived a Supreme Court challenge in 1992.13 \n\nThe Huntington-Hill method allocates representatives to states one at a time. First, in a preprocessing stage, each state is allocated one representative. Then in each iteration of the main loop, the next representative is assigned to the state with the highest priority. The priority of each state is defined to be $P / { sqrt { r ( r + 1 ) } }$ , where $P$ is the state’s population and $r$ is the number of representatives already allocated to that state. \nThe algorithm is described in pseudocode in Figure 0.5. The input consists of an array $P o p [ 1 ldots n ]$ storing the populations of the $n$ states and an integer $R$ equal to the total number of representatives; the algorithm assumes $R geq n$ . (Currently, in the United States, $n = 5 0$ and $R = 4 3 5 .$ ) The output array $R e p [ 1 ldots n ]$ records the number of representatives allocated to each state. \nApportionCongress(Pop[1 .. n], R): PQ NewPriorityQueue Give every state its first representative for $s gets 1$ to $n$ $begin{array} { l } { R e p [ s ] gets 1 }  { operatorname { I N S E R T } left( P Q , s , P o p [ i ] / sqrt { 2 } right) } end{array}$ Allocate the remaining $n { - } R$ representatives for $i gets 1$ to $n { - } R$ $s gets mathrm { E x T R A C T M A X } ( P Q )$ $R e p [ s ] gets R e p [ s ] + 1$ priorit $left. gamma left. P o p [ s ] right/ sqrt { R e p [ s ] left( R e p [ s ] + 1 right) } right.$ Insert(PQ, s, priority) return Rep[1 .. n] \nThis implementation of Huntington-Hill uses a priority queue that supports the operations NewPriorityQueue, Insert, and ExtractMax. (The actual law doesn’t say anything about priority queues, of course.) The output of the algorithm, and therefore its correctness, does not depend at all on how this priority queue is implemented. The Census Bureau uses a sorted array, stored in a single column of an Excel spreadsheet, which is recalculated from scratch at every iteration. You (should have) learned a more efficient implementation in your undergraduate data structures class. \n\nSimilar apportionment algorithms are used in multi-party parliamentary elections around the world, where the number of seats allocated to each party is supposed to be proportional to the number of votes that party receives. The two most common are the $D$ ’Hondt method14 and the Webster–Sainte-Laguë method,15 which respectively use priorities $P / ( r + 1 )$ and $P / ( 2 r + 1 )$ in place of the square-root expression in Huntington-Hill. The Huntington-Hill method is essentially unique to the United States House of Representatives, thanks in part to the constitutional requirement that each state must be allocated at least one representative. \n0.4 A Bad Example \nAs a prototypical example of a sequence of instructions that is not actually an algorithm, consider \"Martin’s algorithm”:16 \nBeAMillionaireAndNeverPayTaxes(): Get a million dollars. If the tax man comes to your door and says, “You have never paid taxes!” Say “I forgot.” \nPretty simple, except for that first step; it’s a doozy! A group of billionaire CEOs, Silicon Valley venture capitalists, or New York City real-estate hustlers might consider this an algorithm, because for them the first step is both unambiguous and trivial,17 but for the rest of us poor slobs, Martin’s procedure is too vague to be considered an actual algorithm. On the other hand, this is a perfect example of a reduction—it reduces the problem of being a millionaire and never paying taxes to the “easier” problem of acquiring a million dollars. We’ll see reductions over and over again in this book. As hundreds of businessmen and politicians have demonstrated, if you know how to solve the easier problem, a reduction tells you how to solve the harder one.",
      "metadata": {
        "content": "Construct the line perpendicular to ℓ passing through P.   \nRightAngle(ℓ, P): Choose a point $A in { ell }$ $A , B gets$ Intersect(Circle(P, A), ℓ) $C , D gets mathrm { I N T E R S E C T } big ( mathrm { C I R C L E } ( A , B ) , mathrm { C I R C L E } ( B , A ) big )$ return Line $( C , D )$   \nConstruct a point $Z$ such that $| A Z | = | A C | | A D | / | A B | . rangle rangle$   \nMultiplyOrDivide(A, B, C, D): $alpha gets$ RightAngle(Line(A, C),A) $E gets$ Intersect(Circle(A, B), α) $beta$ $F gets$ Intersect(Circle(A, D), α) $beta gets$ RightAngle(Line(E, C), F) A E $alpha$ 1 $gamma gets$ RightAngle(β, F) return Intersect(γ, Line(A, C)) γ \nof the algorithm follows from the observation that triangles $A C E$ and $A Z F$ are similar. The second and third lines of the main algorithm are ambiguous, because $alpha$ intersects any circle centered at $A$ at two distinct points, but the algorithm is actually correct no matter which intersection points are chosen for $E$ and $F$ . \nEuclid’s algorithm reduces the problem of multiplying two magnitudes (lengths) to a series of primitive compass-and-straightedge operations. These operations are difficult to implement precisely on a modern digital computer, but Euclid’s algorithm wasn’t designed for a digital computer. It was designed for the Platonic Ideal Geometer, wielding the Platonic Ideal Compass and the Platonic Ideal Straightedge, who could execute each operation perfectly in constant time by definition. In this model of computation, MultiplyOrDivide runs in $O ( 1 )$ time! \n0.3 Congressional Apportionment \nHere is another real-world example of an algorithm of significant political importance. Article I, Section 2 of the United States Constitution requires that \nRepresentatives and direct Taxes shall be apportioned among the several States which may be included within this Union, according to their respective Numbers.... The Number of Representatives shall not exceed one for every thirty Thousand, but each State shall have at Least one Representative... \nBecause there are only a finite number of seats in the House of Representatives, exact proportional representation requires either shared or fractional representatives, neither of which are legal. As a result, over the next several decades, many different apportionment algorithms were proposed and used to round the ideal fractional solution fairly. The algorithm actually used today, called the Huntington-Hill method or the method of equal proportions, was first suggested by Census Bureau statistician Joseph Hill in 1911, refined by Harvard mathematician Edward Huntington in 1920, adopted into Federal law (2 U.S.C. §2a) in 1941, and survived a Supreme Court challenge in 1992.13 \n\nThe Huntington-Hill method allocates representatives to states one at a time. First, in a preprocessing stage, each state is allocated one representative. Then in each iteration of the main loop, the next representative is assigned to the state with the highest priority. The priority of each state is defined to be $P / { sqrt { r ( r + 1 ) } }$ , where $P$ is the state’s population and $r$ is the number of representatives already allocated to that state. \nThe algorithm is described in pseudocode in Figure 0.5. The input consists of an array $P o p [ 1 ldots n ]$ storing the populations of the $n$ states and an integer $R$ equal to the total number of representatives; the algorithm assumes $R geq n$ . (Currently, in the United States, $n = 5 0$ and $R = 4 3 5 .$ ) The output array $R e p [ 1 ldots n ]$ records the number of representatives allocated to each state. \nApportionCongress(Pop[1 .. n], R): PQ NewPriorityQueue Give every state its first representative for $s gets 1$ to $n$ $begin{array} { l } { R e p [ s ] gets 1 }  { operatorname { I N S E R T } left( P Q , s , P o p [ i ] / sqrt { 2 } right) } end{array}$ Allocate the remaining $n { - } R$ representatives for $i gets 1$ to $n { - } R$ $s gets mathrm { E x T R A C T M A X } ( P Q )$ $R e p [ s ] gets R e p [ s ] + 1$ priorit $left. gamma left. P o p [ s ] right/ sqrt { R e p [ s ] left( R e p [ s ] + 1 right) } right.$ Insert(PQ, s, priority) return Rep[1 .. n] \nThis implementation of Huntington-Hill uses a priority queue that supports the operations NewPriorityQueue, Insert, and ExtractMax. (The actual law doesn’t say anything about priority queues, of course.) The output of the algorithm, and therefore its correctness, does not depend at all on how this priority queue is implemented. The Census Bureau uses a sorted array, stored in a single column of an Excel spreadsheet, which is recalculated from scratch at every iteration. You (should have) learned a more efficient implementation in your undergraduate data structures class. \n\nSimilar apportionment algorithms are used in multi-party parliamentary elections around the world, where the number of seats allocated to each party is supposed to be proportional to the number of votes that party receives. The two most common are the $D$ ’Hondt method14 and the Webster–Sainte-Laguë method,15 which respectively use priorities $P / ( r + 1 )$ and $P / ( 2 r + 1 )$ in place of the square-root expression in Huntington-Hill. The Huntington-Hill method is essentially unique to the United States House of Representatives, thanks in part to the constitutional requirement that each state must be allocated at least one representative. \n0.4 A Bad Example \nAs a prototypical example of a sequence of instructions that is not actually an algorithm, consider \"Martin’s algorithm”:16 \nBeAMillionaireAndNeverPayTaxes(): Get a million dollars. If the tax man comes to your door and says, “You have never paid taxes!” Say “I forgot.” \nPretty simple, except for that first step; it’s a doozy! A group of billionaire CEOs, Silicon Valley venture capitalists, or New York City real-estate hustlers might consider this an algorithm, because for them the first step is both unambiguous and trivial,17 but for the rest of us poor slobs, Martin’s procedure is too vague to be considered an actual algorithm. On the other hand, this is a perfect example of a reduction—it reduces the problem of being a millionaire and never paying taxes to the “easier” problem of acquiring a million dollars. We’ll see reductions over and over again in this book. As hundreds of businessmen and politicians have demonstrated, if you know how to solve the easier problem, a reduction tells you how to solve the harder one.",
        "chapter": "Introduction",
        "section": "Congressional Apportionment",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 4
      }
    }
  ],
  "output": {
    "question": "Considering the emphasis on algorithm communication from snippet [2] and the detailed specification requirements from snippet [6], how would you modify the lattice multiplication algorithm's description (implied in snippets [1] and [3]) to better teach its correctness while maintaining clarity about its O(mn) running time?",
    "correct_answer": "The description should explicitly connect the lattice algorithm's visual grid structure to the standard multiplication algorithm's correctness proof, showing how each cell's partial product corresponds to digit-by-digit multiplication with proper place value alignment, while clearly stating that both the lattice and standard algorithms have O(mn) time complexity for m-digit by n-digit multiplication.",
    "distractors": [
      "The description should focus only on the running time comparison between lattice and duplation-and-mediation algorithms, omitting correctness proofs.",
      "The description should include implementation details for all possible edge cases, even if they obscure the main teaching points about the algorithm.",
      "The description should prioritize mathematical notation over explanatory text to ensure precision, sacrificing teachability."
    ],
    "rationale": "Step 1: Identify relevant snippets - Snippet [2] emphasizes teaching algorithms effectively, snippet [6] discusses including necessary details for specification, correctness, and running time analysis, and snippets [1] and [3] provide context about the lattice multiplication algorithm and its O(mn) running time. Step 2: Connect concepts - Combine the communication focus from [2] with the specification requirements from [6] and apply them to the lattice algorithm context from [1] and [3]. Step 3: Formulate question - Create a multi-hop question requiring synthesis of teaching philosophy ([2]), specification standards ([6]), and algorithm characteristics ([1],[3]). Step 4: Ensure complexity - The question requires understanding how to balance pedagogical clarity with technical completeness when describing an existing algorithm."
  },
  "score": {
    "relevance": 0.0,
    "faithfulness": 1.0,
    "integration": 1.0,
    "complexity": 1.0,
    "overall": 0.9000000000000001
  }
}
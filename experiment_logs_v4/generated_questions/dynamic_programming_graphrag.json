{
  "context": [
    {
      "node_id": "tb1_node40",
      "content": "Although Pin˙gala’s text hints at a systematic rule for counting meters with a given number of beats,2 it took about a millennium for that rule to be stated explicitly. In the 7th century ce, another Indian scholar named Viraha¯ .nka wrote a commentary on Pi˙ngala’s work, in which he observed that the number of meters with $n$ beats is the sum of the number of meters with $( n - 2 )$ beats and the number of meters with $( n - 1 )$ beats. In more modern notation, Viraha¯n. ka’s observation implies a recurrence for the total number $M ( n )$ of $n$ -beat meters: \nIt is not hard to see that $M ( 0 ) = 1$ (there is only one empty meter) and $M ( 1 ) = 1$ (the only one-beat meter consists of a single short syllable). \nThe same recurrence reappeared in Europe about 500 years after Viraha¯ .nka, in Leonardo of Pisa’s 1202 treatise Liber Abaci, one of the most influential early European works on “algorism”. In full compliance with Stigler’s Law of Eponymy,3 the modern Fibonacci numbers are defined using Virah¯an. ka’s recurrence, but with different base cases: \nIn particular, we have $M ( n ) = F _ { n + 1 }$ for all $n$ . \nBacktracking Can Be Slow \nThe recursive definition of Fibonacci numbers immediately gives us a recursive algorithm for computing them. Here is the same algorithm written in pseudocode: \n“dah-pause” is a guru ak.sara, and there are exactly five letters (M, D, R, U, and H) whose codes last four ma¯tra¯. \n2The Chanda.hśa¯stra contains two systematic rules for listing all meters with a given number of syllables, which correspond roughly to writing numbers in binary from left to right (like Greeks) or from right to left (like Egyptians). The same text includes a recursive algorithm to compute $2 ^ { n }$ (the number of meters with $n$ syllables) by repeated squaring, and (arguably) a recursive algorithm to compute binomial coefficients (the number of meters with $k$ short syllables and $n$ syllables overall). \n3“No scientific discovery is named after its original discoverer.” In his 1980 paper that gives the law its name, the statistician Stephen Stigler jokingly claimed that this law was first proposed by sociologist Robert K. Merton. However, similar statements were previously made by Vladimir Arnol’d in the 1970’s (“Discoveries are rarely attributed to the correct person.”), Carl Boyer in 1968 (“Clio, the muse of history, often is fickle in attaching names to theorems!”), Alfred North Whitehead in 1917 (“Everything of importance has been said before by someone who did not discover it.”), and even Stephen’s father George Stigler in 1966 (“If we should ever encounter a case where a theory is named for the correct man, it will be noted.”). We will see many other examples of Stigler’s law in this book. \nUnfortunately, this naive recursive algorithm is horribly slow. Except for the recursive calls, the entire algorithm requires only a constant number of steps: one comparison and possibly one addition. Let $T ( n )$ denote the number of recursive calls to RecFibo; this function satisfies the recurrence \nwhich looks an awful lot like the recurrence for Fibonacci numbers themselves! Writing out the first several values of $T ( n )$ suggests the closed-form solution $T ( n ) = 2 F _ { n + 1 } - 1$ , which we can verify by induction (hint, hint). So computing $F _ { n }$ using this algorithm takes about twice as long as just counting to $F _ { n }$ . Methods beyond the scope of this book4 imply that $F _ { n } = Theta ( phi ^ { n } )$ , where $phi = ( sqrt { 5 } + 1 ) / 2 approx 1 . 6 1 8 0 3$ is the so-called golden ratio. In short, the running time of this recursive algorithm is exponential in $n$ . \nWe can actually see this exponential growth directly as follows. Think of the recursion tree for RecFibo as a binary tree of additions, with only 0s and 1s at the leaves. Since the eventual output is $F _ { n }$ , exactly $F _ { n }$ of the leaves must have value 1; these leaves represent the calls to RecRibo(1). An easy inductive argument (hint, hint) implies that RecFibo(0) is called exactly $F _ { n - 1 }$ times. (If we just want an asymptotic bound, it’s enough to observe that the number of calls to RecFibo(0) is at most the number of calls to RecFibo(1).) Thus, the recursion tree has exactly $F _ { n } + F _ { n - 1 } = F _ { n + 1 } = O ( F _ { n } )$ leaves, and therefore, because it’s a full binary tree, $2 F _ { n + 1 } - 1 = O ( F _ { n } )$ nodes altogether. \nMemo(r)ization: Remember Everything \nThe obvious reason for the recursive algorithm’s lack of speed is that it computes the same Fibonacci numbers over and over and over. A single call to RecFibo $( n )$ results in one recursive call to ${ mathrm { R E C F I B O } } ( n - 1 )$ , two recursive calls to ${ mathrm { R E C F I B O } } ( n - 2 )$ , three recursive calls to $operatorname { R E C F I B O } ( n - 3 )$ , five recursive calls to RecFibo $( n - 4 )$ , and in general $F _ { k - 1 }$ recursive calls to $mathrm { R E C F I B O } ( n - k )$ for any integer $0 leq k < n$ . Each call is recomputing some Fibonacci number from scratch. \nWe can speed up our recursive algorithm considerably by writing down the results of our recursive calls and looking them up again if we need them later.",
      "metadata": {
        "content": "Although Pin˙gala’s text hints at a systematic rule for counting meters with a given number of beats,2 it took about a millennium for that rule to be stated explicitly. In the 7th century ce, another Indian scholar named Viraha¯ .nka wrote a commentary on Pi˙ngala’s work, in which he observed that the number of meters with $n$ beats is the sum of the number of meters with $( n - 2 )$ beats and the number of meters with $( n - 1 )$ beats. In more modern notation, Viraha¯n. ka’s observation implies a recurrence for the total number $M ( n )$ of $n$ -beat meters: \nIt is not hard to see that $M ( 0 ) = 1$ (there is only one empty meter) and $M ( 1 ) = 1$ (the only one-beat meter consists of a single short syllable). \nThe same recurrence reappeared in Europe about 500 years after Viraha¯ .nka, in Leonardo of Pisa’s 1202 treatise Liber Abaci, one of the most influential early European works on “algorism”. In full compliance with Stigler’s Law of Eponymy,3 the modern Fibonacci numbers are defined using Virah¯an. ka’s recurrence, but with different base cases: \nIn particular, we have $M ( n ) = F _ { n + 1 }$ for all $n$ . \nBacktracking Can Be Slow \nThe recursive definition of Fibonacci numbers immediately gives us a recursive algorithm for computing them. Here is the same algorithm written in pseudocode: \n“dah-pause” is a guru ak.sara, and there are exactly five letters (M, D, R, U, and H) whose codes last four ma¯tra¯. \n2The Chanda.hśa¯stra contains two systematic rules for listing all meters with a given number of syllables, which correspond roughly to writing numbers in binary from left to right (like Greeks) or from right to left (like Egyptians). The same text includes a recursive algorithm to compute $2 ^ { n }$ (the number of meters with $n$ syllables) by repeated squaring, and (arguably) a recursive algorithm to compute binomial coefficients (the number of meters with $k$ short syllables and $n$ syllables overall). \n3“No scientific discovery is named after its original discoverer.” In his 1980 paper that gives the law its name, the statistician Stephen Stigler jokingly claimed that this law was first proposed by sociologist Robert K. Merton. However, similar statements were previously made by Vladimir Arnol’d in the 1970’s (“Discoveries are rarely attributed to the correct person.”), Carl Boyer in 1968 (“Clio, the muse of history, often is fickle in attaching names to theorems!”), Alfred North Whitehead in 1917 (“Everything of importance has been said before by someone who did not discover it.”), and even Stephen’s father George Stigler in 1966 (“If we should ever encounter a case where a theory is named for the correct man, it will be noted.”). We will see many other examples of Stigler’s law in this book. \nUnfortunately, this naive recursive algorithm is horribly slow. Except for the recursive calls, the entire algorithm requires only a constant number of steps: one comparison and possibly one addition. Let $T ( n )$ denote the number of recursive calls to RecFibo; this function satisfies the recurrence \nwhich looks an awful lot like the recurrence for Fibonacci numbers themselves! Writing out the first several values of $T ( n )$ suggests the closed-form solution $T ( n ) = 2 F _ { n + 1 } - 1$ , which we can verify by induction (hint, hint). So computing $F _ { n }$ using this algorithm takes about twice as long as just counting to $F _ { n }$ . Methods beyond the scope of this book4 imply that $F _ { n } = Theta ( phi ^ { n } )$ , where $phi = ( sqrt { 5 } + 1 ) / 2 approx 1 . 6 1 8 0 3$ is the so-called golden ratio. In short, the running time of this recursive algorithm is exponential in $n$ . \nWe can actually see this exponential growth directly as follows. Think of the recursion tree for RecFibo as a binary tree of additions, with only 0s and 1s at the leaves. Since the eventual output is $F _ { n }$ , exactly $F _ { n }$ of the leaves must have value 1; these leaves represent the calls to RecRibo(1). An easy inductive argument (hint, hint) implies that RecFibo(0) is called exactly $F _ { n - 1 }$ times. (If we just want an asymptotic bound, it’s enough to observe that the number of calls to RecFibo(0) is at most the number of calls to RecFibo(1).) Thus, the recursion tree has exactly $F _ { n } + F _ { n - 1 } = F _ { n + 1 } = O ( F _ { n } )$ leaves, and therefore, because it’s a full binary tree, $2 F _ { n + 1 } - 1 = O ( F _ { n } )$ nodes altogether. \nMemo(r)ization: Remember Everything \nThe obvious reason for the recursive algorithm’s lack of speed is that it computes the same Fibonacci numbers over and over and over. A single call to RecFibo $( n )$ results in one recursive call to ${ mathrm { R E C F I B O } } ( n - 1 )$ , two recursive calls to ${ mathrm { R E C F I B O } } ( n - 2 )$ , three recursive calls to $operatorname { R E C F I B O } ( n - 3 )$ , five recursive calls to RecFibo $( n - 4 )$ , and in general $F _ { k - 1 }$ recursive calls to $mathrm { R E C F I B O } ( n - k )$ for any integer $0 leq k < n$ . Each call is recomputing some Fibonacci number from scratch. \nWe can speed up our recursive algorithm considerably by writing down the results of our recursive calls and looking them up again if we need them later.",
        "chapter": "Dynamic Programming",
        "section": "Mātrāvṛtta",
        "subsection": "Backtracking Can Be Slow",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 40
      }
    },
    {
      "node_id": "tb1_node36",
      "content": "For any index $i$ , let MaxScore(i) denote the maximum score of any segmentation of the suffix $A [ i ldots n ]$ ; we need to compute MaxScore(1). This function satisfies the following recurrence: \nThis is essentially the same recurrence as the one we developed for Splittable; the only difference is that the boolean operations $vee$ and $wedge$ have been replaced by the numerical operations max and $^ +$ . \n2.6 Longest Increasing Subsequence \nFor any sequence S, a subsequence of $s$ is another sequence obtained from $s$ by deleting zero or more elements, without changing the order of the remaining elements; the elements of the subsequence need not be contiguous in S. For example, when you drive down a major street in any city, you drive through a sequence of intersections with traffic lights, but you only have to stop at a subsequence of those intersections, where the traffic lights are red. If you’re very lucky, you never stop at all: the empty sequence is a subsequence of S. On the other hand, if you’re very unlucky, you may have to stop at every intersection: $s$ is a subsequence of itself. \nAs another example, the strings BENT, ACKACK, SQUARING, and SUBSEQUENT are all subsequences of the string SUBSEQUENCEBACKTRACKING, as are the empty string and the entire string SUBSEQUENCEBACKTRACKING, but the strings QUEUE and EQUUS and TALLYHO are not. A subsequence whose elements are contiguous in the original sequence is called a substring; for example, MASHER and LAUGHTER are both subsequences of MANSLAUGHTER, but only LAUGHTER is a substring. \nNow suppose we are given a sequence of integers, and we need to find the longest subsequence whose elements are in increasing order. More concretely, the input is an integer array $A [ 1 ldots n ]$ , and we need to compute the longest possible sequence of indices $1 leq i _ { 1 } < i _ { 2 } < cdots < i _ { ell } leq n$ such that $A [ i _ { k } ] < A [ i _ { k + 1 } ]$ for all $k$ . \nOne natural approach to building this longest increasing subsequence is to decide, for each index $j$ in order from 1 to $n$ , whether or not to include $A [ j ]$ in the subsequence. Jumping into the middle of this decision sequence, we might imagine the following picture: \nAs in our earlier text segmentation examples, the black bar separates our past decisions from the portion of the input we have not yet processed. Numbers we have already decided to include are highlighted and bold; numbers we have already decided to exclude are grayed out. (Notice that the numbers we’ve decided to include are increasing!) Our algorithm must decide whether or not to include the number immediately after the black bar. \n\nIn this example, we definitely cannot include 5, because then the selected numbers would no longer be in increasing order. So let’s skip ahead to the next decision: \nNow we can include 8, but it’s not obvious whether we should. Rather than trying to be “smart”, our backtracking algorithm will use simple brute force. \nFirst tentatively include the 8, and let the Recursion Fairy make the rest of the decisions.   \n• Then tentatively exclude the 8, and let the Recursion Fairy make the rest of the decisions. \nWhichever choice leads to a longer increasing subsequence is the right one. (This is precisely the same recursion pattern we used to solve SubsetSum.) \nNow for the key question: What do we need to remember about our past decisions? We can only include $boldsymbol { A } [ j ]$ if the resulting subsequence is in increasing order. If we assume (inductively!) that the numbers previously selected from $A [ 1 . . j - 1 ]$ are in increasing order, then we can include $boldsymbol { A } [ j ]$ if and only if $boldsymbol { A } [ j ]$ is larger than the last number selected from $A [ 1 ldots j - 1 ]$ . Thus, the only information we need about the past is the last number selected so far. We can now revise our pictures by erasing everything we don’t need: \n6 5? 8 9 7 9 3 2 3 8 4 6 2 6   \n6 8? 9 7 9 3 2 3 8 4 6 2 6 \nSo the problem our recursive strategy is actually solving is the following: \nGiven an integer prev and an array $A [ 1 ldots n ]$ , find the longest increasing subsequence of $A$ in which every element is larger than prev. \nAs usual, our recursive strategy requires a base case. Our current strategy breaks down when we get to the end of the array, because there is no “next number” to consider. But an empty array has exactly one subsequence, namely, the empty sequence. Vacuously, every element in the empty sequence is larger than whatever value you want, and every pair of elements in the empty sequence appears in increasing order. Thus, the longest increasing subsequence of the empty array has length 0. \nHere’s the resulting recursive algorithm: \nOkay, but remember that passing arrays around on the call stack is expensive; let’s try to rephrase everything in terms of array indices, assuming that the array $A [ 1 ldots n ]$ is a global variable. The integer prev is typically an array element $A [ i ]$ , and the remaining array is always a suffix $A [ j ldots n ]$ of the original input array. So we can reformulate our recursive problem as follows: \nGiven two indices $i$ and $j$ , where $i < j$ , find the longest increasing subsequence of $A [ j ldots n ]$ in which every element is larger than $A [ i ]$ . \nLet LISbigger $( i , j )$ denote the length of the longest increasing subsequence of $A [ j ldots n ]$ in which every element is larger than $A [ i ]$ . Our recursive strategy gives us the following recurrence: \nAlternatively, if you prefer pseudocode: \nFinally, we need to connect our recursive strategy to the original problem: Finding the longest increasing subsequence of an array with no other constraints. The simplest approach is to add an artificial sentinel value $- infty$ to the beginning of the array. \nThe running time of LISbigger satisfies the Hanoi recurrence $T ( n ) leq$ $2 T ( n - 1 ) + O ( 1 )$ , which as usual implies that $T ( n ) = O ( 2 ^ { n } )$ . We really shouldn’t be surprised by this running time; in the worst case, the algorithm examines each of the $2 ^ { n }$ subsequences of the input array. \n2.7 Longest Increasing Subsequence, Take 2 \nThis is not the only backtracking strategy we can use to find longest increasing subsequences. Instead of considering the input sequence one element at a time, we could try to construct the output sequence one element at a time. That is, instead of asking “Is $A [ i ]$ the next element of the output sequence?”, we could ask directly, “Where is the next element of the output sequence, if any?” \nJumping into the middle of this strategy, we might be faced with the following picture. Suppose we just decided to include the 6 just left of the black bar in our output sequence, and we need to decide which element to the right of the bar to include next. \n3 1 4 1 5 9 2 6 5? 3? 5? 8? 9? 7? 9? 3? 2? 3? 8? 4? 6? 2? 6? \nOf course, we can only include numbers on the right that are greater than 6;   \notherwise, our output sequence would not be increasing. \nBut we have no idea which of those larger numbers is the best choice, and trying to cleverly figure out the best choice is too much work, and it’s only going to get us into trouble anyway. Instead, we enumerate all possibilities by brute force, and let the Recursion Fairy evaluate each one.",
      "metadata": {
        "content": "For any index $i$ , let MaxScore(i) denote the maximum score of any segmentation of the suffix $A [ i ldots n ]$ ; we need to compute MaxScore(1). This function satisfies the following recurrence: \nThis is essentially the same recurrence as the one we developed for Splittable; the only difference is that the boolean operations $vee$ and $wedge$ have been replaced by the numerical operations max and $^ +$ . \n2.6 Longest Increasing Subsequence \nFor any sequence S, a subsequence of $s$ is another sequence obtained from $s$ by deleting zero or more elements, without changing the order of the remaining elements; the elements of the subsequence need not be contiguous in S. For example, when you drive down a major street in any city, you drive through a sequence of intersections with traffic lights, but you only have to stop at a subsequence of those intersections, where the traffic lights are red. If you’re very lucky, you never stop at all: the empty sequence is a subsequence of S. On the other hand, if you’re very unlucky, you may have to stop at every intersection: $s$ is a subsequence of itself. \nAs another example, the strings BENT, ACKACK, SQUARING, and SUBSEQUENT are all subsequences of the string SUBSEQUENCEBACKTRACKING, as are the empty string and the entire string SUBSEQUENCEBACKTRACKING, but the strings QUEUE and EQUUS and TALLYHO are not. A subsequence whose elements are contiguous in the original sequence is called a substring; for example, MASHER and LAUGHTER are both subsequences of MANSLAUGHTER, but only LAUGHTER is a substring. \nNow suppose we are given a sequence of integers, and we need to find the longest subsequence whose elements are in increasing order. More concretely, the input is an integer array $A [ 1 ldots n ]$ , and we need to compute the longest possible sequence of indices $1 leq i _ { 1 } < i _ { 2 } < cdots < i _ { ell } leq n$ such that $A [ i _ { k } ] < A [ i _ { k + 1 } ]$ for all $k$ . \nOne natural approach to building this longest increasing subsequence is to decide, for each index $j$ in order from 1 to $n$ , whether or not to include $A [ j ]$ in the subsequence. Jumping into the middle of this decision sequence, we might imagine the following picture: \nAs in our earlier text segmentation examples, the black bar separates our past decisions from the portion of the input we have not yet processed. Numbers we have already decided to include are highlighted and bold; numbers we have already decided to exclude are grayed out. (Notice that the numbers we’ve decided to include are increasing!) Our algorithm must decide whether or not to include the number immediately after the black bar. \n\nIn this example, we definitely cannot include 5, because then the selected numbers would no longer be in increasing order. So let’s skip ahead to the next decision: \nNow we can include 8, but it’s not obvious whether we should. Rather than trying to be “smart”, our backtracking algorithm will use simple brute force. \nFirst tentatively include the 8, and let the Recursion Fairy make the rest of the decisions.   \n• Then tentatively exclude the 8, and let the Recursion Fairy make the rest of the decisions. \nWhichever choice leads to a longer increasing subsequence is the right one. (This is precisely the same recursion pattern we used to solve SubsetSum.) \nNow for the key question: What do we need to remember about our past decisions? We can only include $boldsymbol { A } [ j ]$ if the resulting subsequence is in increasing order. If we assume (inductively!) that the numbers previously selected from $A [ 1 . . j - 1 ]$ are in increasing order, then we can include $boldsymbol { A } [ j ]$ if and only if $boldsymbol { A } [ j ]$ is larger than the last number selected from $A [ 1 ldots j - 1 ]$ . Thus, the only information we need about the past is the last number selected so far. We can now revise our pictures by erasing everything we don’t need: \n6 5? 8 9 7 9 3 2 3 8 4 6 2 6   \n6 8? 9 7 9 3 2 3 8 4 6 2 6 \nSo the problem our recursive strategy is actually solving is the following: \nGiven an integer prev and an array $A [ 1 ldots n ]$ , find the longest increasing subsequence of $A$ in which every element is larger than prev. \nAs usual, our recursive strategy requires a base case. Our current strategy breaks down when we get to the end of the array, because there is no “next number” to consider. But an empty array has exactly one subsequence, namely, the empty sequence. Vacuously, every element in the empty sequence is larger than whatever value you want, and every pair of elements in the empty sequence appears in increasing order. Thus, the longest increasing subsequence of the empty array has length 0. \nHere’s the resulting recursive algorithm: \nOkay, but remember that passing arrays around on the call stack is expensive; let’s try to rephrase everything in terms of array indices, assuming that the array $A [ 1 ldots n ]$ is a global variable. The integer prev is typically an array element $A [ i ]$ , and the remaining array is always a suffix $A [ j ldots n ]$ of the original input array. So we can reformulate our recursive problem as follows: \nGiven two indices $i$ and $j$ , where $i < j$ , find the longest increasing subsequence of $A [ j ldots n ]$ in which every element is larger than $A [ i ]$ . \nLet LISbigger $( i , j )$ denote the length of the longest increasing subsequence of $A [ j ldots n ]$ in which every element is larger than $A [ i ]$ . Our recursive strategy gives us the following recurrence: \nAlternatively, if you prefer pseudocode: \nFinally, we need to connect our recursive strategy to the original problem: Finding the longest increasing subsequence of an array with no other constraints. The simplest approach is to add an artificial sentinel value $- infty$ to the beginning of the array. \nThe running time of LISbigger satisfies the Hanoi recurrence $T ( n ) leq$ $2 T ( n - 1 ) + O ( 1 )$ , which as usual implies that $T ( n ) = O ( 2 ^ { n } )$ . We really shouldn’t be surprised by this running time; in the worst case, the algorithm examines each of the $2 ^ { n }$ subsequences of the input array. \n2.7 Longest Increasing Subsequence, Take 2 \nThis is not the only backtracking strategy we can use to find longest increasing subsequences. Instead of considering the input sequence one element at a time, we could try to construct the output sequence one element at a time. That is, instead of asking “Is $A [ i ]$ the next element of the output sequence?”, we could ask directly, “Where is the next element of the output sequence, if any?” \nJumping into the middle of this strategy, we might be faced with the following picture. Suppose we just decided to include the 6 just left of the black bar in our output sequence, and we need to decide which element to the right of the bar to include next. \n3 1 4 1 5 9 2 6 5? 3? 5? 8? 9? 7? 9? 3? 2? 3? 8? 4? 6? 2? 6? \nOf course, we can only include numbers on the right that are greater than 6;   \notherwise, our output sequence would not be increasing. \nBut we have no idea which of those larger numbers is the best choice, and trying to cleverly figure out the best choice is too much work, and it’s only going to get us into trouble anyway. Instead, we enumerate all possibilities by brute force, and let the Recursion Fairy evaluate each one.",
        "chapter": "Backtracking",
        "section": "Longest Increasing Subsequence",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 36
      }
    },
    {
      "node_id": "tb1_node35",
      "content": "assume that the algorithm actually makes every possible recursive call.12 Then we can transform the “full history” recurrence into a “limited history” recurrence by subtracting the recurrence for $T ( n - 1 )$ , as follows: \nThis final recurrence simplifies to $T ( n ) = 2 T ( n - 1 ) + alpha$ . At this point, we can confidently guess (or derive via recursion trees, or remember from our Tower of Hanoi analysis) that $T ( n ) = O ( 2 ^ { n } ) .$ ; indeed, this upper bound is not hard to prove by induction from the original full-history recurrence. \nMoreover, this analysis is tight. There are exactly $2 ^ { n - 1 }$ possible ways to segment a string of length $n$ —each input character either ends a word or doesn’t, except the last input character, which always ends the last word. In the worst case, our Splittable algorithm explores each of these $2 ^ { n - 1 }$ possibilities. \nVariants \nNow that we have the basic recursion pattern in hand, we can use it to solve many different variants of the segmentation problem, just as we did for the SubsetSum problem. Here I’ll describe just one example; more variations are considered in the exercises. As usual, the original input to our problem is an array $A [ 1 ldots n ]$ . \nIf a string can be segmented in more than one sequence of words, we may want to find the best segmentation according to some criterion; conversely, if the input string cannot be segmented into words, we may want to compute the best segmentation we can find, rather than merely reporting failure. To meet both of these goals, suppose we have access to a second function Score that takes a string as input and returns a numerical value. For example, we might assign higher scores to longer or more common words, lower scores to shorter or more obscure words, slightly negative scores for minor spelling errors, and more negative scores to obvious non-words. Our goal is to find a segmentation that maximizes the sum of the scores of the segments. \nFor any index $i$ , let MaxScore(i) denote the maximum score of any segmentation of the suffix $A [ i ldots n ]$ ; we need to compute MaxScore(1). This function satisfies the following recurrence: \nThis is essentially the same recurrence as the one we developed for Splittable; the only difference is that the boolean operations $vee$ and $wedge$ have been replaced by the numerical operations max and $^ +$ . \n2.6 Longest Increasing Subsequence \nFor any sequence S, a subsequence of $s$ is another sequence obtained from $s$ by deleting zero or more elements, without changing the order of the remaining elements; the elements of the subsequence need not be contiguous in S. For example, when you drive down a major street in any city, you drive through a sequence of intersections with traffic lights, but you only have to stop at a subsequence of those intersections, where the traffic lights are red. If you’re very lucky, you never stop at all: the empty sequence is a subsequence of S. On the other hand, if you’re very unlucky, you may have to stop at every intersection: $s$ is a subsequence of itself. \nAs another example, the strings BENT, ACKACK, SQUARING, and SUBSEQUENT are all subsequences of the string SUBSEQUENCEBACKTRACKING, as are the empty string and the entire string SUBSEQUENCEBACKTRACKING, but the strings QUEUE and EQUUS and TALLYHO are not. A subsequence whose elements are contiguous in the original sequence is called a substring; for example, MASHER and LAUGHTER are both subsequences of MANSLAUGHTER, but only LAUGHTER is a substring. \nNow suppose we are given a sequence of integers, and we need to find the longest subsequence whose elements are in increasing order. More concretely, the input is an integer array $A [ 1 ldots n ]$ , and we need to compute the longest possible sequence of indices $1 leq i _ { 1 } < i _ { 2 } < cdots < i _ { ell } leq n$ such that $A [ i _ { k } ] < A [ i _ { k + 1 } ]$ for all $k$ . \nOne natural approach to building this longest increasing subsequence is to decide, for each index $j$ in order from 1 to $n$ , whether or not to include $A [ j ]$ in the subsequence. Jumping into the middle of this decision sequence, we might imagine the following picture: \nAs in our earlier text segmentation examples, the black bar separates our past decisions from the portion of the input we have not yet processed. Numbers we have already decided to include are highlighted and bold; numbers we have already decided to exclude are grayed out. (Notice that the numbers we’ve decided to include are increasing!) Our algorithm must decide whether or not to include the number immediately after the black bar.",
      "metadata": {
        "content": "assume that the algorithm actually makes every possible recursive call.12 Then we can transform the “full history” recurrence into a “limited history” recurrence by subtracting the recurrence for $T ( n - 1 )$ , as follows: \nThis final recurrence simplifies to $T ( n ) = 2 T ( n - 1 ) + alpha$ . At this point, we can confidently guess (or derive via recursion trees, or remember from our Tower of Hanoi analysis) that $T ( n ) = O ( 2 ^ { n } ) .$ ; indeed, this upper bound is not hard to prove by induction from the original full-history recurrence. \nMoreover, this analysis is tight. There are exactly $2 ^ { n - 1 }$ possible ways to segment a string of length $n$ —each input character either ends a word or doesn’t, except the last input character, which always ends the last word. In the worst case, our Splittable algorithm explores each of these $2 ^ { n - 1 }$ possibilities. \nVariants \nNow that we have the basic recursion pattern in hand, we can use it to solve many different variants of the segmentation problem, just as we did for the SubsetSum problem. Here I’ll describe just one example; more variations are considered in the exercises. As usual, the original input to our problem is an array $A [ 1 ldots n ]$ . \nIf a string can be segmented in more than one sequence of words, we may want to find the best segmentation according to some criterion; conversely, if the input string cannot be segmented into words, we may want to compute the best segmentation we can find, rather than merely reporting failure. To meet both of these goals, suppose we have access to a second function Score that takes a string as input and returns a numerical value. For example, we might assign higher scores to longer or more common words, lower scores to shorter or more obscure words, slightly negative scores for minor spelling errors, and more negative scores to obvious non-words. Our goal is to find a segmentation that maximizes the sum of the scores of the segments. \nFor any index $i$ , let MaxScore(i) denote the maximum score of any segmentation of the suffix $A [ i ldots n ]$ ; we need to compute MaxScore(1). This function satisfies the following recurrence: \nThis is essentially the same recurrence as the one we developed for Splittable; the only difference is that the boolean operations $vee$ and $wedge$ have been replaced by the numerical operations max and $^ +$ . \n2.6 Longest Increasing Subsequence \nFor any sequence S, a subsequence of $s$ is another sequence obtained from $s$ by deleting zero or more elements, without changing the order of the remaining elements; the elements of the subsequence need not be contiguous in S. For example, when you drive down a major street in any city, you drive through a sequence of intersections with traffic lights, but you only have to stop at a subsequence of those intersections, where the traffic lights are red. If you’re very lucky, you never stop at all: the empty sequence is a subsequence of S. On the other hand, if you’re very unlucky, you may have to stop at every intersection: $s$ is a subsequence of itself. \nAs another example, the strings BENT, ACKACK, SQUARING, and SUBSEQUENT are all subsequences of the string SUBSEQUENCEBACKTRACKING, as are the empty string and the entire string SUBSEQUENCEBACKTRACKING, but the strings QUEUE and EQUUS and TALLYHO are not. A subsequence whose elements are contiguous in the original sequence is called a substring; for example, MASHER and LAUGHTER are both subsequences of MANSLAUGHTER, but only LAUGHTER is a substring. \nNow suppose we are given a sequence of integers, and we need to find the longest subsequence whose elements are in increasing order. More concretely, the input is an integer array $A [ 1 ldots n ]$ , and we need to compute the longest possible sequence of indices $1 leq i _ { 1 } < i _ { 2 } < cdots < i _ { ell } leq n$ such that $A [ i _ { k } ] < A [ i _ { k + 1 } ]$ for all $k$ . \nOne natural approach to building this longest increasing subsequence is to decide, for each index $j$ in order from 1 to $n$ , whether or not to include $A [ j ]$ in the subsequence. Jumping into the middle of this decision sequence, we might imagine the following picture: \nAs in our earlier text segmentation examples, the black bar separates our past decisions from the portion of the input we have not yet processed. Numbers we have already decided to include are highlighted and bold; numbers we have already decided to exclude are grayed out. (Notice that the numbers we’ve decided to include are increasing!) Our algorithm must decide whether or not to include the number immediately after the black bar.",
        "chapter": "Backtracking",
        "section": "Text Segmentation (Interpunctio Verborum)",
        "subsection": "Variants",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 35
      }
    },
    {
      "node_id": "tb1_node37",
      "content": "The running time of LISbigger satisfies the Hanoi recurrence $T ( n ) leq$ $2 T ( n - 1 ) + O ( 1 )$ , which as usual implies that $T ( n ) = O ( 2 ^ { n } )$ . We really shouldn’t be surprised by this running time; in the worst case, the algorithm examines each of the $2 ^ { n }$ subsequences of the input array. \n2.7 Longest Increasing Subsequence, Take 2 \nThis is not the only backtracking strategy we can use to find longest increasing subsequences. Instead of considering the input sequence one element at a time, we could try to construct the output sequence one element at a time. That is, instead of asking “Is $A [ i ]$ the next element of the output sequence?”, we could ask directly, “Where is the next element of the output sequence, if any?” \nJumping into the middle of this strategy, we might be faced with the following picture. Suppose we just decided to include the 6 just left of the black bar in our output sequence, and we need to decide which element to the right of the bar to include next. \n3 1 4 1 5 9 2 6 5? 3? 5? 8? 9? 7? 9? 3? 2? 3? 8? 4? 6? 2? 6? \nOf course, we can only include numbers on the right that are greater than 6;   \notherwise, our output sequence would not be increasing. \nBut we have no idea which of those larger numbers is the best choice, and trying to cleverly figure out the best choice is too much work, and it’s only going to get us into trouble anyway. Instead, we enumerate all possibilities by brute force, and let the Recursion Fairy evaluate each one. \nThe subset of numbers we can consider as the next element depends only on the last number we decided to include. Thus, we can simplify our picture of the decision process by discarding everything to the left of the bar except the last number we decided to include. \n6 5 3 5 8 9 7 9 3 2 3 8 4 6 2 6 \nThe remaining sequence of numbers is just a suffix of the original input array. Thus, if we think of the input array $A [ 1 ldots n ]$ as a global variable, we can formally express our recursive problem in terms of indices as follows: \nGiven an index i, find the longest increasing subsequence of $A [ i ldots n ]$ that begins with A[i]. \nLet LISfirst(i) denote the length of the longest increasing subsequence of $A [ i ldots n ]$ that begins with $A [ i ]$ . We can now formulate our recursive backtracking strategy as the following recursive definition: \nBecause we are dealing with sets of natural numbers, we define max $varnothing = 0$ . Then we automatically have $L I S f i r s t ( i ) = 1$ if $A [ j ] leq A [ i ]$ for all $j > i$ ; in particular, $L I S f i r s t ( n ) = 1$ . These are the base cases for our recurrence. \nWe can also express this recursive definition in pseudocode as follows: \nFinally, we need to reconnect this recursive algorithm to our original problem—finding the longest increasing subsequence without knowing its first element. One natural approach that works is to try all possible first elements by brute force. Equivalently, we can add a sentinel element $- infty$ to the beginning of the array, find the longest increasing subsequence that starts with the sentinel, and finally ignore the sentinel. \n2.8 Optimal Binary Search Trees \nOur final example combines recursive backtracking with the divide-and-conquer strategy. Recall that the running time for a successful search in a binary search tree is proportional to the number of ancestors of the target node.13 As a result, the worst-case search time is proportional to the depth of the tree. Thus, to minimize the worst-case search time, the height of the tree should be as small as possible; by this metric, the ideal tree is perfectly balanced. \nIn many applications of binary search trees, however, it is more important to minimize the total cost of several searches rather than the worst-case cost of a single search. If $x$ is a more frequent search target than y, we can save time by building a tree where the depth of $x$ is smaller than the depth of $y$ , even if that means increasing the overall depth of the tree. A perfectly balanced tree is not the best choice if some items are significantly more popular than others. In fact, a totally unbalanced tree with depth $Omega ( n )$ might actually be the best choice! \nThis situation suggests the following problem. Suppose we are given a sorted array of keys $A [ 1 ldots n ]$ and an array of corresponding access frequencies $f [ 1 . . n ]$ . Our task is to build the binary search tree that minimizes the total search time, assuming that there will be exactly $f [ i ]$ searches for each key A[i]. \nBefore we think about how to solve this problem, we should first come up with a good recursive definition of the function we are trying to optimize! Suppose we are also given a binary search tree $T$ with $n$ nodes. Let $nu _ { 1 } , nu _ { 2 } , ldots , nu _ { n }$ be the nodes of $T$ , indexed in sorted order, so that each node $nu _ { i }$ stores the corresponding key $A [ i ]$ . Then ignoring constant factors, the total cost of performing all the binary searches is given by the following expression: \nNow suppose $nu _ { r }$ is the root of $T$ ; by definition, $nu _ { r }$ is an ancestor of every node in $T$ . If $i < r$ , then all ancestors of $nu _ { i }$ except the root are in the left subtree of $T$ . Similarly, if $i > r$ , then all ancestors of $nu _ { i }$ except the root are in the right subtree of $T$ . Thus, we can partition the cost function into three parts as follows: \nThe second and third summations look exactly like our original definition $( * )$",
      "metadata": {
        "content": "The running time of LISbigger satisfies the Hanoi recurrence $T ( n ) leq$ $2 T ( n - 1 ) + O ( 1 )$ , which as usual implies that $T ( n ) = O ( 2 ^ { n } )$ . We really shouldn’t be surprised by this running time; in the worst case, the algorithm examines each of the $2 ^ { n }$ subsequences of the input array. \n2.7 Longest Increasing Subsequence, Take 2 \nThis is not the only backtracking strategy we can use to find longest increasing subsequences. Instead of considering the input sequence one element at a time, we could try to construct the output sequence one element at a time. That is, instead of asking “Is $A [ i ]$ the next element of the output sequence?”, we could ask directly, “Where is the next element of the output sequence, if any?” \nJumping into the middle of this strategy, we might be faced with the following picture. Suppose we just decided to include the 6 just left of the black bar in our output sequence, and we need to decide which element to the right of the bar to include next. \n3 1 4 1 5 9 2 6 5? 3? 5? 8? 9? 7? 9? 3? 2? 3? 8? 4? 6? 2? 6? \nOf course, we can only include numbers on the right that are greater than 6;   \notherwise, our output sequence would not be increasing. \nBut we have no idea which of those larger numbers is the best choice, and trying to cleverly figure out the best choice is too much work, and it’s only going to get us into trouble anyway. Instead, we enumerate all possibilities by brute force, and let the Recursion Fairy evaluate each one. \nThe subset of numbers we can consider as the next element depends only on the last number we decided to include. Thus, we can simplify our picture of the decision process by discarding everything to the left of the bar except the last number we decided to include. \n6 5 3 5 8 9 7 9 3 2 3 8 4 6 2 6 \nThe remaining sequence of numbers is just a suffix of the original input array. Thus, if we think of the input array $A [ 1 ldots n ]$ as a global variable, we can formally express our recursive problem in terms of indices as follows: \nGiven an index i, find the longest increasing subsequence of $A [ i ldots n ]$ that begins with A[i]. \nLet LISfirst(i) denote the length of the longest increasing subsequence of $A [ i ldots n ]$ that begins with $A [ i ]$ . We can now formulate our recursive backtracking strategy as the following recursive definition: \nBecause we are dealing with sets of natural numbers, we define max $varnothing = 0$ . Then we automatically have $L I S f i r s t ( i ) = 1$ if $A [ j ] leq A [ i ]$ for all $j > i$ ; in particular, $L I S f i r s t ( n ) = 1$ . These are the base cases for our recurrence. \nWe can also express this recursive definition in pseudocode as follows: \nFinally, we need to reconnect this recursive algorithm to our original problem—finding the longest increasing subsequence without knowing its first element. One natural approach that works is to try all possible first elements by brute force. Equivalently, we can add a sentinel element $- infty$ to the beginning of the array, find the longest increasing subsequence that starts with the sentinel, and finally ignore the sentinel. \n2.8 Optimal Binary Search Trees \nOur final example combines recursive backtracking with the divide-and-conquer strategy. Recall that the running time for a successful search in a binary search tree is proportional to the number of ancestors of the target node.13 As a result, the worst-case search time is proportional to the depth of the tree. Thus, to minimize the worst-case search time, the height of the tree should be as small as possible; by this metric, the ideal tree is perfectly balanced. \nIn many applications of binary search trees, however, it is more important to minimize the total cost of several searches rather than the worst-case cost of a single search. If $x$ is a more frequent search target than y, we can save time by building a tree where the depth of $x$ is smaller than the depth of $y$ , even if that means increasing the overall depth of the tree. A perfectly balanced tree is not the best choice if some items are significantly more popular than others. In fact, a totally unbalanced tree with depth $Omega ( n )$ might actually be the best choice! \nThis situation suggests the following problem. Suppose we are given a sorted array of keys $A [ 1 ldots n ]$ and an array of corresponding access frequencies $f [ 1 . . n ]$ . Our task is to build the binary search tree that minimizes the total search time, assuming that there will be exactly $f [ i ]$ searches for each key A[i]. \nBefore we think about how to solve this problem, we should first come up with a good recursive definition of the function we are trying to optimize! Suppose we are also given a binary search tree $T$ with $n$ nodes. Let $nu _ { 1 } , nu _ { 2 } , ldots , nu _ { n }$ be the nodes of $T$ , indexed in sorted order, so that each node $nu _ { i }$ stores the corresponding key $A [ i ]$ . Then ignoring constant factors, the total cost of performing all the binary searches is given by the following expression: \nNow suppose $nu _ { r }$ is the root of $T$ ; by definition, $nu _ { r }$ is an ancestor of every node in $T$ . If $i < r$ , then all ancestors of $nu _ { i }$ except the root are in the left subtree of $T$ . Similarly, if $i > r$ , then all ancestors of $nu _ { i }$ except the root are in the right subtree of $T$ . Thus, we can partition the cost function into three parts as follows: \nThe second and third summations look exactly like our original definition $( * )$",
        "chapter": "Backtracking",
        "section": "Longest Increasing Subsequence, Take 2",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 37
      }
    },
    {
      "node_id": "tb1_node41",
      "content": "Unfortunately, this naive recursive algorithm is horribly slow. Except for the recursive calls, the entire algorithm requires only a constant number of steps: one comparison and possibly one addition. Let $T ( n )$ denote the number of recursive calls to RecFibo; this function satisfies the recurrence \nwhich looks an awful lot like the recurrence for Fibonacci numbers themselves! Writing out the first several values of $T ( n )$ suggests the closed-form solution $T ( n ) = 2 F _ { n + 1 } - 1$ , which we can verify by induction (hint, hint). So computing $F _ { n }$ using this algorithm takes about twice as long as just counting to $F _ { n }$ . Methods beyond the scope of this book4 imply that $F _ { n } = Theta ( phi ^ { n } )$ , where $phi = ( sqrt { 5 } + 1 ) / 2 approx 1 . 6 1 8 0 3$ is the so-called golden ratio. In short, the running time of this recursive algorithm is exponential in $n$ . \nWe can actually see this exponential growth directly as follows. Think of the recursion tree for RecFibo as a binary tree of additions, with only 0s and 1s at the leaves. Since the eventual output is $F _ { n }$ , exactly $F _ { n }$ of the leaves must have value 1; these leaves represent the calls to RecRibo(1). An easy inductive argument (hint, hint) implies that RecFibo(0) is called exactly $F _ { n - 1 }$ times. (If we just want an asymptotic bound, it’s enough to observe that the number of calls to RecFibo(0) is at most the number of calls to RecFibo(1).) Thus, the recursion tree has exactly $F _ { n } + F _ { n - 1 } = F _ { n + 1 } = O ( F _ { n } )$ leaves, and therefore, because it’s a full binary tree, $2 F _ { n + 1 } - 1 = O ( F _ { n } )$ nodes altogether. \nMemo(r)ization: Remember Everything \nThe obvious reason for the recursive algorithm’s lack of speed is that it computes the same Fibonacci numbers over and over and over. A single call to RecFibo $( n )$ results in one recursive call to ${ mathrm { R E C F I B O } } ( n - 1 )$ , two recursive calls to ${ mathrm { R E C F I B O } } ( n - 2 )$ , three recursive calls to $operatorname { R E C F I B O } ( n - 3 )$ , five recursive calls to RecFibo $( n - 4 )$ , and in general $F _ { k - 1 }$ recursive calls to $mathrm { R E C F I B O } ( n - k )$ for any integer $0 leq k < n$ . Each call is recomputing some Fibonacci number from scratch. \nWe can speed up our recursive algorithm considerably by writing down the results of our recursive calls and looking them up again if we need them later. \nThis optimization technique, now known as memoization (yes, without an R), is usually credited to Donald Michie in 1967, but essentially the same technique was proposed in 1959 by Arthur Samuel.5 \nMemFibo(n): if $n = 0$ return 0 else if $n = 1$ return 1 else if $F [ n ]$ is undefined F[n] MemFibo(n 1) + MemFibo(n 2) return F [n] \nMemoization clearly decreases the running time of the algorithm, but by how much? If we actually trace through the recursive calls made by MemFibo, we find that the array $F [ ]$ is filled from the bottom up: first $F [ 2 ]$ , then $F [ 3 ]$ , and so on, up to $F [ n ]$ . This pattern can be verified by induction: Each entry $F [ i ]$ is filled only after its predecessor $F [ i - 1 ]$ . If we ignore the time spent in recursive calls, it requires only constant time to evaluate the recurrence for each Fibonacci number $F _ { i }$ . But by design, the recurrence for $F _ { i }$ is evaluated only once for each index i. We conclude that MemFibo performs only $O ( n )$ additions, an exponential improvement over the naïve recursive algorithm! \nDynamic Programming: Fill Deliberately \nOnce we see how the array $F [ ]$ is filled, we can replace the memoized recurrence with a simple for-loop that intentionally fills the array in that order, instead of relying on a more complicated recursive algorithm to do it for us accidentally. \nNow the time analysis is immediate: IterFibo clearly uses $O ( n )$ additions and stores $O ( n )$ integers. \nThis is our first explicit dynamic programming algorithm. The dynamic programming paradigm was formalized and popularized by Richard Bellman in the mid-1950s, while working at the RAND Corporation, although he was far from the first to use the technique. In particular, this iterative algorithm for Fibonacci numbers was already proposed by Virah¯a .nka and later Sanskrit prosodists in the 12th century, and again by Fibonacci at the turn of the 13th century!6",
      "metadata": {
        "content": "Unfortunately, this naive recursive algorithm is horribly slow. Except for the recursive calls, the entire algorithm requires only a constant number of steps: one comparison and possibly one addition. Let $T ( n )$ denote the number of recursive calls to RecFibo; this function satisfies the recurrence \nwhich looks an awful lot like the recurrence for Fibonacci numbers themselves! Writing out the first several values of $T ( n )$ suggests the closed-form solution $T ( n ) = 2 F _ { n + 1 } - 1$ , which we can verify by induction (hint, hint). So computing $F _ { n }$ using this algorithm takes about twice as long as just counting to $F _ { n }$ . Methods beyond the scope of this book4 imply that $F _ { n } = Theta ( phi ^ { n } )$ , where $phi = ( sqrt { 5 } + 1 ) / 2 approx 1 . 6 1 8 0 3$ is the so-called golden ratio. In short, the running time of this recursive algorithm is exponential in $n$ . \nWe can actually see this exponential growth directly as follows. Think of the recursion tree for RecFibo as a binary tree of additions, with only 0s and 1s at the leaves. Since the eventual output is $F _ { n }$ , exactly $F _ { n }$ of the leaves must have value 1; these leaves represent the calls to RecRibo(1). An easy inductive argument (hint, hint) implies that RecFibo(0) is called exactly $F _ { n - 1 }$ times. (If we just want an asymptotic bound, it’s enough to observe that the number of calls to RecFibo(0) is at most the number of calls to RecFibo(1).) Thus, the recursion tree has exactly $F _ { n } + F _ { n - 1 } = F _ { n + 1 } = O ( F _ { n } )$ leaves, and therefore, because it’s a full binary tree, $2 F _ { n + 1 } - 1 = O ( F _ { n } )$ nodes altogether. \nMemo(r)ization: Remember Everything \nThe obvious reason for the recursive algorithm’s lack of speed is that it computes the same Fibonacci numbers over and over and over. A single call to RecFibo $( n )$ results in one recursive call to ${ mathrm { R E C F I B O } } ( n - 1 )$ , two recursive calls to ${ mathrm { R E C F I B O } } ( n - 2 )$ , three recursive calls to $operatorname { R E C F I B O } ( n - 3 )$ , five recursive calls to RecFibo $( n - 4 )$ , and in general $F _ { k - 1 }$ recursive calls to $mathrm { R E C F I B O } ( n - k )$ for any integer $0 leq k < n$ . Each call is recomputing some Fibonacci number from scratch. \nWe can speed up our recursive algorithm considerably by writing down the results of our recursive calls and looking them up again if we need them later. \nThis optimization technique, now known as memoization (yes, without an R), is usually credited to Donald Michie in 1967, but essentially the same technique was proposed in 1959 by Arthur Samuel.5 \nMemFibo(n): if $n = 0$ return 0 else if $n = 1$ return 1 else if $F [ n ]$ is undefined F[n] MemFibo(n 1) + MemFibo(n 2) return F [n] \nMemoization clearly decreases the running time of the algorithm, but by how much? If we actually trace through the recursive calls made by MemFibo, we find that the array $F [ ]$ is filled from the bottom up: first $F [ 2 ]$ , then $F [ 3 ]$ , and so on, up to $F [ n ]$ . This pattern can be verified by induction: Each entry $F [ i ]$ is filled only after its predecessor $F [ i - 1 ]$ . If we ignore the time spent in recursive calls, it requires only constant time to evaluate the recurrence for each Fibonacci number $F _ { i }$ . But by design, the recurrence for $F _ { i }$ is evaluated only once for each index i. We conclude that MemFibo performs only $O ( n )$ additions, an exponential improvement over the naïve recursive algorithm! \nDynamic Programming: Fill Deliberately \nOnce we see how the array $F [ ]$ is filled, we can replace the memoized recurrence with a simple for-loop that intentionally fills the array in that order, instead of relying on a more complicated recursive algorithm to do it for us accidentally. \nNow the time analysis is immediate: IterFibo clearly uses $O ( n )$ additions and stores $O ( n )$ integers. \nThis is our first explicit dynamic programming algorithm. The dynamic programming paradigm was formalized and popularized by Richard Bellman in the mid-1950s, while working at the RAND Corporation, although he was far from the first to use the technique. In particular, this iterative algorithm for Fibonacci numbers was already proposed by Virah¯a .nka and later Sanskrit prosodists in the 12th century, and again by Fibonacci at the turn of the 13th century!6",
        "chapter": "Dynamic Programming",
        "section": "Mātrāvṛtta",
        "subsection": "Memo(r)ization: Remember Everything",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 41
      }
    },
    {
      "node_id": "tb1_node31",
      "content": "Does any subset of X [1 .. i] sum to T ?   \nSubsetSum(X, i, T): if $T = 0$ return True else if $T < 0$ or $i = 0$ return False else with SubsetSum(X , i 1, T X [i]) Recurse! wout $$ SubsetSum(X, i 1, T) Recurse! return (with wout) \nWith these implementation choices, the running time $T ( n )$ of our algorithm satisfies the recurrence $T ( n ) leq 2 T ( n - 1 ) + O ( 1 )$ . The solution $T ( n ) = O ( 2 ^ { n } )$ follows easily using either recursion trees or the even simpler “Oh yeah, we already solved this recurrence for the Tower of Hanoi” method. In the worst case—for example, when $T$ is larger than the sum of all elements of $X$ —the recursion tree for this algorithm is a complete binary tree with depth $n$ , and the algorithm considers all $2 ^ { n }$ subsets of $X$ . \nVariants \nWith only minor changes, we can solve several variants of SubsetSum. For example, Figure 2.6 shows an algorithm that actually constructs a subset of $X$ that sums to $T$ , if one exists, or returns the error value None if no such subset exists; this algorithm uses exactly the same recursive strategy as our earlier decision algorithms. This algorithm also runs in $O ( 2 ^ { n } )$ time; the analysis is simplest if we assume a set data structure that allows us to insert a single element in $O ( 1 )$ time (for example, a linked list), but in fact the running time is still $O ( 2 ^ { n } )$ even if insertion requires $O ( n )$ time (for example, a sorted linked list). Similar variants allow us to count subsets that sum to a particular value, or choose the best subset (according to some other criterion) that sums to a particular value. \nMost other problems that are solved by backtracking have this property: the same recursive strategy can be used to solve many different variants of the same problem. For example, it is easy to modify the recursive strategy described in the previous section, which determines whether a given game position is good or bad, to instead return a good move, or a list of all good moves. For this reason, when we design backtracking algorithms, we should aim for the simplest possible variant of the problem, computing a number or even a single boolean instead of more complex information or structure. \n2.4 The General Pattern \nBacktracking algorithms are commonly used to make a sequence of decisions, with the goal of building a recursively defined structure satisfying certain constraints. Often (but not always) this goal structure is itself a sequence. For example: \n• In the $n$ -queens problem, the goal is a sequence of queen positions, one in each row, such that no two queens attack each other. For each row, the algorithm decides where to place the queen.   \n• In the game tree problem, the goal is a sequence of legal moves, such that each move is as good as possible for the player making it. For each game state, the algorithm decides the best possible next move.   \n• In the SubsetSum problem, the goal is a sequence of input elements that have a particular sum. For each input element, the algorithm decides whether to include it in the output sequence or not. \n(Hang on, why is the goal of subset sum finding a sequence? That was a deliberate design decision. We imposed a convenient ordering on the input set—by representing it using an array, as opposed to some other more amorphous data structure—that we can exploit in our recursive algorithm.) \nIn each recursive call to the backtracking algorithm, we need to make exactly one decision, and our choice must be consistent with all previous decisions. Thus, each recursive call requires not only the portion of the input data we have not yet processed, but also a suitable summary of the decisions we have already made. For the sake of efficiency, the summary of past decisions should be as small as possible. For example:",
      "metadata": {
        "content": "Does any subset of X [1 .. i] sum to T ?   \nSubsetSum(X, i, T): if $T = 0$ return True else if $T < 0$ or $i = 0$ return False else with SubsetSum(X , i 1, T X [i]) Recurse! wout $$ SubsetSum(X, i 1, T) Recurse! return (with wout) \nWith these implementation choices, the running time $T ( n )$ of our algorithm satisfies the recurrence $T ( n ) leq 2 T ( n - 1 ) + O ( 1 )$ . The solution $T ( n ) = O ( 2 ^ { n } )$ follows easily using either recursion trees or the even simpler “Oh yeah, we already solved this recurrence for the Tower of Hanoi” method. In the worst case—for example, when $T$ is larger than the sum of all elements of $X$ —the recursion tree for this algorithm is a complete binary tree with depth $n$ , and the algorithm considers all $2 ^ { n }$ subsets of $X$ . \nVariants \nWith only minor changes, we can solve several variants of SubsetSum. For example, Figure 2.6 shows an algorithm that actually constructs a subset of $X$ that sums to $T$ , if one exists, or returns the error value None if no such subset exists; this algorithm uses exactly the same recursive strategy as our earlier decision algorithms. This algorithm also runs in $O ( 2 ^ { n } )$ time; the analysis is simplest if we assume a set data structure that allows us to insert a single element in $O ( 1 )$ time (for example, a linked list), but in fact the running time is still $O ( 2 ^ { n } )$ even if insertion requires $O ( n )$ time (for example, a sorted linked list). Similar variants allow us to count subsets that sum to a particular value, or choose the best subset (according to some other criterion) that sums to a particular value. \nMost other problems that are solved by backtracking have this property: the same recursive strategy can be used to solve many different variants of the same problem. For example, it is easy to modify the recursive strategy described in the previous section, which determines whether a given game position is good or bad, to instead return a good move, or a list of all good moves. For this reason, when we design backtracking algorithms, we should aim for the simplest possible variant of the problem, computing a number or even a single boolean instead of more complex information or structure. \n2.4 The General Pattern \nBacktracking algorithms are commonly used to make a sequence of decisions, with the goal of building a recursively defined structure satisfying certain constraints. Often (but not always) this goal structure is itself a sequence. For example: \n• In the $n$ -queens problem, the goal is a sequence of queen positions, one in each row, such that no two queens attack each other. For each row, the algorithm decides where to place the queen.   \n• In the game tree problem, the goal is a sequence of legal moves, such that each move is as good as possible for the player making it. For each game state, the algorithm decides the best possible next move.   \n• In the SubsetSum problem, the goal is a sequence of input elements that have a particular sum. For each input element, the algorithm decides whether to include it in the output sequence or not. \n(Hang on, why is the goal of subset sum finding a sequence? That was a deliberate design decision. We imposed a convenient ordering on the input set—by representing it using an array, as opposed to some other more amorphous data structure—that we can exploit in our recursive algorithm.) \nIn each recursive call to the backtracking algorithm, we need to make exactly one decision, and our choice must be consistent with all previous decisions. Thus, each recursive call requires not only the portion of the input data we have not yet processed, but also a suitable summary of the decisions we have already made. For the sake of efficiency, the summary of past decisions should be as small as possible. For example:",
        "chapter": "Backtracking",
        "section": "Subset Sum",
        "subsection": "Variants",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 31
      }
    },
    {
      "node_id": "tb1_node34",
      "content": "This is exactly the same algorithm as we saw earlier; the only thing we’ve changed is the notation. The similarity is even more apparent if we rewrite the recurrence in pseudocode: \nAlthough it may look like a trivial notational difference, using index notation instead of array notation is an important habit, not only to speed up backtracking algorithms in practice, but for developing dynamic programming algorithms, which we discuss in the next chapter. \nnAnalysis \nIt should come as no surprise that most backtracking algorithms have exponential worst-case running times. Analyzing the precise running times of many of these algorithms requires techniques that are beyond the scope of this book. Fortunately, most of the backtracking algorithms we will encounter in this book are only intermediate results on the way to more efficient algorithms, which means their exact worst-case running time is not actually important. (First make it work; then make it fast.) \nBut just for fun, let’s analyze the running time of our recursive algorithm Splittable. Because we don’t know what IsWord is doing, we can’t know how long each call to IsWord takes, so we’re forced to analyze the running time in terms of the number of calls to IsWord.11 Splittable calls IsWord on every prefix of the input string, and possibly calls itself recursively on every suffix of the output string. Thus, the “running time” of Splittable obeys the scary-looking recurrence \nThis really isn’t as bad as it looks, especially once you’ve seen the trick. \nFirst, we replace the $O ( n )$ term with an explicit expression $a n$ , for some unknown (and ultimately unimportant) constant $alpha$ . Second, we conservatively \nassume that the algorithm actually makes every possible recursive call.12 Then we can transform the “full history” recurrence into a “limited history” recurrence by subtracting the recurrence for $T ( n - 1 )$ , as follows: \nThis final recurrence simplifies to $T ( n ) = 2 T ( n - 1 ) + alpha$ . At this point, we can confidently guess (or derive via recursion trees, or remember from our Tower of Hanoi analysis) that $T ( n ) = O ( 2 ^ { n } ) .$ ; indeed, this upper bound is not hard to prove by induction from the original full-history recurrence. \nMoreover, this analysis is tight. There are exactly $2 ^ { n - 1 }$ possible ways to segment a string of length $n$ —each input character either ends a word or doesn’t, except the last input character, which always ends the last word. In the worst case, our Splittable algorithm explores each of these $2 ^ { n - 1 }$ possibilities. \nVariants \nNow that we have the basic recursion pattern in hand, we can use it to solve many different variants of the segmentation problem, just as we did for the SubsetSum problem. Here I’ll describe just one example; more variations are considered in the exercises. As usual, the original input to our problem is an array $A [ 1 ldots n ]$ . \nIf a string can be segmented in more than one sequence of words, we may want to find the best segmentation according to some criterion; conversely, if the input string cannot be segmented into words, we may want to compute the best segmentation we can find, rather than merely reporting failure. To meet both of these goals, suppose we have access to a second function Score that takes a string as input and returns a numerical value. For example, we might assign higher scores to longer or more common words, lower scores to shorter or more obscure words, slightly negative scores for minor spelling errors, and more negative scores to obvious non-words. Our goal is to find a segmentation that maximizes the sum of the scores of the segments.",
      "metadata": {
        "content": "This is exactly the same algorithm as we saw earlier; the only thing we’ve changed is the notation. The similarity is even more apparent if we rewrite the recurrence in pseudocode: \nAlthough it may look like a trivial notational difference, using index notation instead of array notation is an important habit, not only to speed up backtracking algorithms in practice, but for developing dynamic programming algorithms, which we discuss in the next chapter. \nnAnalysis \nIt should come as no surprise that most backtracking algorithms have exponential worst-case running times. Analyzing the precise running times of many of these algorithms requires techniques that are beyond the scope of this book. Fortunately, most of the backtracking algorithms we will encounter in this book are only intermediate results on the way to more efficient algorithms, which means their exact worst-case running time is not actually important. (First make it work; then make it fast.) \nBut just for fun, let’s analyze the running time of our recursive algorithm Splittable. Because we don’t know what IsWord is doing, we can’t know how long each call to IsWord takes, so we’re forced to analyze the running time in terms of the number of calls to IsWord.11 Splittable calls IsWord on every prefix of the input string, and possibly calls itself recursively on every suffix of the output string. Thus, the “running time” of Splittable obeys the scary-looking recurrence \nThis really isn’t as bad as it looks, especially once you’ve seen the trick. \nFirst, we replace the $O ( n )$ term with an explicit expression $a n$ , for some unknown (and ultimately unimportant) constant $alpha$ . Second, we conservatively \nassume that the algorithm actually makes every possible recursive call.12 Then we can transform the “full history” recurrence into a “limited history” recurrence by subtracting the recurrence for $T ( n - 1 )$ , as follows: \nThis final recurrence simplifies to $T ( n ) = 2 T ( n - 1 ) + alpha$ . At this point, we can confidently guess (or derive via recursion trees, or remember from our Tower of Hanoi analysis) that $T ( n ) = O ( 2 ^ { n } ) .$ ; indeed, this upper bound is not hard to prove by induction from the original full-history recurrence. \nMoreover, this analysis is tight. There are exactly $2 ^ { n - 1 }$ possible ways to segment a string of length $n$ —each input character either ends a word or doesn’t, except the last input character, which always ends the last word. In the worst case, our Splittable algorithm explores each of these $2 ^ { n - 1 }$ possibilities. \nVariants \nNow that we have the basic recursion pattern in hand, we can use it to solve many different variants of the segmentation problem, just as we did for the SubsetSum problem. Here I’ll describe just one example; more variations are considered in the exercises. As usual, the original input to our problem is an array $A [ 1 ldots n ]$ . \nIf a string can be segmented in more than one sequence of words, we may want to find the best segmentation according to some criterion; conversely, if the input string cannot be segmented into words, we may want to compute the best segmentation we can find, rather than merely reporting failure. To meet both of these goals, suppose we have access to a second function Score that takes a string as input and returns a numerical value. For example, we might assign higher scores to longer or more common words, lower scores to shorter or more obscure words, slightly negative scores for minor spelling errors, and more negative scores to obvious non-words. Our goal is to find a segmentation that maximizes the sum of the scores of the segments.",
        "chapter": "Backtracking",
        "section": "Text Segmentation (Interpunctio Verborum)",
        "subsection": "♥Analysis",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 34
      }
    },
    {
      "node_id": "tb1_node39",
      "content": "subtraction trick we used before. We replace the $O ( )$ notation with an explicit constant, regroup and collect identical terms, subtract the recurrence for $T ( n { - } 1 )$ to get rid of the summation, and then regroup again. \nHey, that doesn’t look so bad after all. The recursion tree method immediately gives us the solution $T ( n ) = O ( 3 ^ { n } )$ (or we can just guess and confirm by induction). \nThis analysis implies that our recursive algorithm does not examine all possible binary search trees! The number of binary search trees with $n$ vertices satisfies the recurrence \nwhich has the closed-form solution $N ( n ) = Theta ( 4 ^ { n } / sqrt { n } )$ . (No, that’s not obvious.) Our algorithm saves considerable time by searching independently for the optimal left and right subtrees for each root. A full enumeration of binary search trees would consider all possible pairs of left and right subtrees; hence the product in the recurrence for $N ( n )$ . \nExercises \n1. Describe recursive algorithms for the following generalizations of the SubsetSum problem: \n(a) Given an array $X [ 1 ldots n ]$ of positive integers and an integer $T$ , compute the number of subsets of $X$ whose elements sum to $T$ .   \n(b) Given two arrays $X [ 1 ldots n ]$ and $W [ 1 ldots n ]$ of positive integers and an integer $T$ , where each $W [ i ]$ denotes the weight of the corresponding element $X [ i ]$ , compute the maximum weight subset of $X$ whose elements sum to $T$ . If no subset of $X$ sums to $T$ , your algorithm should return $- infty$ . \n2. Describe recursive algorithms for the following variants of the text segmentation problem. Assume that you have a subroutine IsWord that takes an array of characters as input and returns True if and only if that string is a “word”. \n\n(a) Given an array $A [ 1 ldots n ]$ of characters, compute the number of partitions of A into words. For example, given the string ARTISTOIL, your algorithm should return 2, for the partitions ARTIST $cdot$ OIL and ART $cdot$ IS·TOIL.   \n(b) Given two arrays $A [ 1 ldots n ]$ and $B [ 1 ldots n ]$ of characters, decide whether A and $B$ can be partitioned into words at the same indices. For example, the strings BOTHEARTHANDSATURNSPIN and PINSTARTRAPSANDRAGSLAP can be partitioned into words at the same indices as follows: BOT·HEART $cdot$ HAND·SAT·URNS·PIN PIN·START $cdot cdot$ RAPS·AND·RAGS·LAP   \n(c) Given two arrays $A [ 1 ldots n ]$ and $B [ 1 ldots n ]$ of characters, compute the number of different ways that $A$ and $B$ can be partitioned into words at the same indices. \n3. An addition chain for an integer $n$ is an increasing sequence of integers that starts with 1 and ends with $n$ , such that each entry after the first is the sum of two earlier entries. More formally, the integer sequence $x _ { 0 } < x _ { 1 } < x _ { 2 } < dots < x _ { ell }$ is an addition chain for $n$ if and only if \n• $x _ { 0 } = 1$ ,   \n• $x _ { ell } = n$ , and   \n• for every index $k > 0$ , there are indices $i leq j < k$ such that $x _ { k } = x _ { i } + x _ { j }$ \nThe ℓength of an addition chain is the number of elements minus 1; we don’t bother to count the first entry. For example, $langle 1 , 2 , 3 , 5 , 1 0 , 2 0 , 2 3 , 4 6 ,$ , $9 2 , 1 8 4 , 1 8 7 , 3 7 4 rangle$ is an addition chain for 374 of length 11. \n(a) Describe a recursive backtracking algorithm to compute a minimumlength addition chain for a given positive integer n. Don’t analyze or optimize your algorithm’s running time, except to satisfy your own curiosity. A correct algorithm whose running time is exponential in $n$ is sufficient for full credit. [Hint: This problem is a lot more like n Queens than text segmentation.] n(b) Describe a recursive backtracking algorithm to compute a minimumlength addition chain for a given positive integer n in time that is sub-exponential in n. [Hint: You may find the results of certain Egyptian rope-fasteners, Indus-River prosodists, and Russian peasants helpful.] 4. (a) Let $A [ 1 ldots m ]$ and $B [ 1 ldots n ]$ be two arbitrary arrays. A common subsequence of $A$ and $B$ is both a subsequence of $A$ and a subsequence of $B$ . Give a simple recursive definition for the function $l c s ( A , B )$ , which gives the length of the longest common subsequence of $A$ and $B$ . \n(b) Let $A [ 1 ldots m ]$ and $B [ 1 ldots n ]$ be two arbitrary arrays. A common supersequence of $A$ and $B$ is another sequence that contains both $A$ and $B$ as subsequences. Give a simple recursive definition for the function $s c s ( A , B )$ , which gives the length of the shortest common supersequence of $A$ and $B$ . (c) Call a sequence $X [ 1 ldots n ]$ of numbers bitonic if there is an index $i$ with $1 < i < n$ , such that the prefix $X [ 1 ldots i ]$ is increasing and the suffix $X [ i ldots n ]$ is decreasing. Give a simple recursive definition for the function $l b s ( A )$ , which gives the length of the longest bitonic subsequence of an arbitrary array $A$ of integers. (d) Call a sequence $X [ 1 ldots n ]$ oscillating if $X [ i ] < X [ i + 1 ]$ for all even $i$ , and $X [ i ] > X [ i + 1 ]$ for all odd i. Give a simple recursive definition for the function $boldsymbol { l o s } ( boldsymbol { A } )$ , which gives the length of the longest oscillating subsequence of an arbitrary array $A$ of integers. (e) Give a simple recursive definition for the function $s o s ( A )$ , which gives the length of the shortest oscillating supersequence of an arbitrary array $A$ of integers. (f) Call a sequence $X [ 1 ldots n ]$ convex if $2 cdot X [ i ] < X [ i - 1 ] + X [ i + 1 ]$ for all $i$ . Give a simple recursive definition for the function $l x s ( A )$ , which gives the length of the longest convex subsequence of an arbitrary array $A$ of integers. 5. For each of the following problems, the input consists of two arrays $X [ 1 . . k ]$ and $Y [ 1 ldots n ]$ where $k leq n$ . (a) Describe a recursive backtracking algorithm to determine whether $X$ is a subsequence of Y . For example, the string PPAP is a subsequence of the string PENPINEAPPLEAPPLEPEN. (b) Describe a recursive backtracking algorithm to find the smallest number of symbols that can be removed from $Y$ so that $X$ is no longer a subsequence. Equivalently, your algorithm should find the longest subsequence of $Y$ that is not a supersequence of $X$ . For example, after removing removing two symbols from the string PENPINEAPPLEAPPLEPEN, the string PPAP is no longer a subsequence. n(c) Describe a recursive backtracking algorithm to determine whether $X$ occurs as two disjoint subsequences of Y . For example, the string PPAP appears as two disjoint subsequences in the string PENPINEAPPLEAPPLEPEN. Don’t analyze the running times of your algorithms, except to satisfy your own curiosity. All three algorithms run in exponential time; we’ll improve that later, so the precise running time isn’t particularly important. \n\n6. This problem asks you to design backtracking algorithms to find the cost of an optimal binary search tree that satisfies additional balance constraints. Your input consists of a sorted array $A [ 1 ldots n ]$ of search keys and an array $f [ 1 . . n ]$ of frequency counts, where $f [ i ]$ is the number of searches for $A [ i ]$ . This is exactly the same cost function as described in Section 2.8. But now your task is to compute an optimal tree that satisfies some additional constraints. \n(a) AVL trees were the earliest self-balancing balanced binary search trees, first described in 1962 by Georgy Adelson-Velsky and Evgenii Landis. An AVL tree is a binary search tree where for every node $nu$ , the height of the left subtree of $nu$ and the height of the right subtree of $nu$ differ by at most one. \nDescribe a recursive backtracking algorithm to construct an optimal AVL tree for a given set of search keys and frequencies. \n(b) Symmetric binary $B$ -trees are another self-balancing binary trees, first described by Rudolf Bayer in 1972; these are better known by the name red-black trees, after a somewhat simpler reformulation by Leo Guibas and Bob Sedgwick in 1978. A red-black tree is a binary search tree with the following additional constraints: \n• Every node is either red or black.   \n• Every red node has a black parent.   \n• Every root-to-leaf path contains the same number of black nodes. \nDescribe a recursive backtracking algorithm to construct an optimal red-black tree for a given set of search keys and frequencies. \n(c) AA trees were proposed by proposed by Arne Andersson in 1993 and slightly simplified (and named) by Mark Allen Weiss in 2000. AA trees are also known as left-leaning red-black trees, after a symmetric reformulation (with different rebalancing algorithms) by Bob Sedgewick in 2006. An AA tree is a red-black tree with one additional constraint: • No left child is red.14 \nDescribe a recursive backtracking algorithm to construct an optimal AA tree for a given set of search keys and frequencies. \nDon’t analyze the running times of your algorithms, except to satisfy your own curiosity. All three algorithms run in exponential time; we’ll improve that later, so the precise running times aren’t particularly important. \nFor more backtracking exercises, see the next chapter! \nPotes enim videre in hac margine, qualiter hoc operati fuimus, scilicet quod iunximus primum numerum cum secundo, videlicet 1 cum 2; et secundum cum tercio; et tercium cum quarto; et quartum cum quinto, et sic deinceps.... \n[You can see in the margin here how we have worked this; clearly, we combined the first number with the second, namely 1 with 2, and the second with the third, and the third with the fourth, and the fourth with the fifth, and so forth. . . .] \n— Leonardo Pisano, Liber Abaci (1202) \nThose who cannot remember the past are condemned to repeat it. \n— Jorge Agustín Nicolás Ruiz de Santayana y Borrás, The Life of Reason, Book I: Introduction and Reason in Common Sense (1905) \nYou know what a learning experience is? A learning experience is one of those things that says, “You know that thing you just did? Don’t do that.” \n— Douglas Adams, The Salmon of Doubt (2002) \n3 \nDynamic Programming \n3.1 M¯atra¯vr.tta \nOne of the earliest examples of recursion arose in India more than 2000 years ago, in the study of poetic meter, or prosody. Classical Sanskrit poetry distinguishes between two types of syllables (ak.sara): light (laghu) and heavy (guru). In one class of meters, variously called m¯atr¯av.rtta or m¯atr¯achandas, each line of poetry consists of a fixed number of “beats” (ma¯tra¯), where each light syllable lasts one beat and each heavy syllable lasts two beats. The formal study of m¯atr¯a-v.rtta dates back to the Chanda.hś¯astra, written by the scholar Pi˙ngala between 600bce and 200bce. Pi˙ngala observed that there are exactly five 4-beat meters: —, and • • • •. (Here each “—” represents a long syllable and each “•” represents a short syllable.)1",
      "metadata": {
        "content": "subtraction trick we used before. We replace the $O ( )$ notation with an explicit constant, regroup and collect identical terms, subtract the recurrence for $T ( n { - } 1 )$ to get rid of the summation, and then regroup again. \nHey, that doesn’t look so bad after all. The recursion tree method immediately gives us the solution $T ( n ) = O ( 3 ^ { n } )$ (or we can just guess and confirm by induction). \nThis analysis implies that our recursive algorithm does not examine all possible binary search trees! The number of binary search trees with $n$ vertices satisfies the recurrence \nwhich has the closed-form solution $N ( n ) = Theta ( 4 ^ { n } / sqrt { n } )$ . (No, that’s not obvious.) Our algorithm saves considerable time by searching independently for the optimal left and right subtrees for each root. A full enumeration of binary search trees would consider all possible pairs of left and right subtrees; hence the product in the recurrence for $N ( n )$ . \nExercises \n1. Describe recursive algorithms for the following generalizations of the SubsetSum problem: \n(a) Given an array $X [ 1 ldots n ]$ of positive integers and an integer $T$ , compute the number of subsets of $X$ whose elements sum to $T$ .   \n(b) Given two arrays $X [ 1 ldots n ]$ and $W [ 1 ldots n ]$ of positive integers and an integer $T$ , where each $W [ i ]$ denotes the weight of the corresponding element $X [ i ]$ , compute the maximum weight subset of $X$ whose elements sum to $T$ . If no subset of $X$ sums to $T$ , your algorithm should return $- infty$ . \n2. Describe recursive algorithms for the following variants of the text segmentation problem. Assume that you have a subroutine IsWord that takes an array of characters as input and returns True if and only if that string is a “word”. \n\n(a) Given an array $A [ 1 ldots n ]$ of characters, compute the number of partitions of A into words. For example, given the string ARTISTOIL, your algorithm should return 2, for the partitions ARTIST $cdot$ OIL and ART $cdot$ IS·TOIL.   \n(b) Given two arrays $A [ 1 ldots n ]$ and $B [ 1 ldots n ]$ of characters, decide whether A and $B$ can be partitioned into words at the same indices. For example, the strings BOTHEARTHANDSATURNSPIN and PINSTARTRAPSANDRAGSLAP can be partitioned into words at the same indices as follows: BOT·HEART $cdot$ HAND·SAT·URNS·PIN PIN·START $cdot cdot$ RAPS·AND·RAGS·LAP   \n(c) Given two arrays $A [ 1 ldots n ]$ and $B [ 1 ldots n ]$ of characters, compute the number of different ways that $A$ and $B$ can be partitioned into words at the same indices. \n3. An addition chain for an integer $n$ is an increasing sequence of integers that starts with 1 and ends with $n$ , such that each entry after the first is the sum of two earlier entries. More formally, the integer sequence $x _ { 0 } < x _ { 1 } < x _ { 2 } < dots < x _ { ell }$ is an addition chain for $n$ if and only if \n• $x _ { 0 } = 1$ ,   \n• $x _ { ell } = n$ , and   \n• for every index $k > 0$ , there are indices $i leq j < k$ such that $x _ { k } = x _ { i } + x _ { j }$ \nThe ℓength of an addition chain is the number of elements minus 1; we don’t bother to count the first entry. For example, $langle 1 , 2 , 3 , 5 , 1 0 , 2 0 , 2 3 , 4 6 ,$ , $9 2 , 1 8 4 , 1 8 7 , 3 7 4 rangle$ is an addition chain for 374 of length 11. \n(a) Describe a recursive backtracking algorithm to compute a minimumlength addition chain for a given positive integer n. Don’t analyze or optimize your algorithm’s running time, except to satisfy your own curiosity. A correct algorithm whose running time is exponential in $n$ is sufficient for full credit. [Hint: This problem is a lot more like n Queens than text segmentation.] n(b) Describe a recursive backtracking algorithm to compute a minimumlength addition chain for a given positive integer n in time that is sub-exponential in n. [Hint: You may find the results of certain Egyptian rope-fasteners, Indus-River prosodists, and Russian peasants helpful.] 4. (a) Let $A [ 1 ldots m ]$ and $B [ 1 ldots n ]$ be two arbitrary arrays. A common subsequence of $A$ and $B$ is both a subsequence of $A$ and a subsequence of $B$ . Give a simple recursive definition for the function $l c s ( A , B )$ , which gives the length of the longest common subsequence of $A$ and $B$ . \n(b) Let $A [ 1 ldots m ]$ and $B [ 1 ldots n ]$ be two arbitrary arrays. A common supersequence of $A$ and $B$ is another sequence that contains both $A$ and $B$ as subsequences. Give a simple recursive definition for the function $s c s ( A , B )$ , which gives the length of the shortest common supersequence of $A$ and $B$ . (c) Call a sequence $X [ 1 ldots n ]$ of numbers bitonic if there is an index $i$ with $1 < i < n$ , such that the prefix $X [ 1 ldots i ]$ is increasing and the suffix $X [ i ldots n ]$ is decreasing. Give a simple recursive definition for the function $l b s ( A )$ , which gives the length of the longest bitonic subsequence of an arbitrary array $A$ of integers. (d) Call a sequence $X [ 1 ldots n ]$ oscillating if $X [ i ] < X [ i + 1 ]$ for all even $i$ , and $X [ i ] > X [ i + 1 ]$ for all odd i. Give a simple recursive definition for the function $boldsymbol { l o s } ( boldsymbol { A } )$ , which gives the length of the longest oscillating subsequence of an arbitrary array $A$ of integers. (e) Give a simple recursive definition for the function $s o s ( A )$ , which gives the length of the shortest oscillating supersequence of an arbitrary array $A$ of integers. (f) Call a sequence $X [ 1 ldots n ]$ convex if $2 cdot X [ i ] < X [ i - 1 ] + X [ i + 1 ]$ for all $i$ . Give a simple recursive definition for the function $l x s ( A )$ , which gives the length of the longest convex subsequence of an arbitrary array $A$ of integers. 5. For each of the following problems, the input consists of two arrays $X [ 1 . . k ]$ and $Y [ 1 ldots n ]$ where $k leq n$ . (a) Describe a recursive backtracking algorithm to determine whether $X$ is a subsequence of Y . For example, the string PPAP is a subsequence of the string PENPINEAPPLEAPPLEPEN. (b) Describe a recursive backtracking algorithm to find the smallest number of symbols that can be removed from $Y$ so that $X$ is no longer a subsequence. Equivalently, your algorithm should find the longest subsequence of $Y$ that is not a supersequence of $X$ . For example, after removing removing two symbols from the string PENPINEAPPLEAPPLEPEN, the string PPAP is no longer a subsequence. n(c) Describe a recursive backtracking algorithm to determine whether $X$ occurs as two disjoint subsequences of Y . For example, the string PPAP appears as two disjoint subsequences in the string PENPINEAPPLEAPPLEPEN. Don’t analyze the running times of your algorithms, except to satisfy your own curiosity. All three algorithms run in exponential time; we’ll improve that later, so the precise running time isn’t particularly important. \n\n6. This problem asks you to design backtracking algorithms to find the cost of an optimal binary search tree that satisfies additional balance constraints. Your input consists of a sorted array $A [ 1 ldots n ]$ of search keys and an array $f [ 1 . . n ]$ of frequency counts, where $f [ i ]$ is the number of searches for $A [ i ]$ . This is exactly the same cost function as described in Section 2.8. But now your task is to compute an optimal tree that satisfies some additional constraints. \n(a) AVL trees were the earliest self-balancing balanced binary search trees, first described in 1962 by Georgy Adelson-Velsky and Evgenii Landis. An AVL tree is a binary search tree where for every node $nu$ , the height of the left subtree of $nu$ and the height of the right subtree of $nu$ differ by at most one. \nDescribe a recursive backtracking algorithm to construct an optimal AVL tree for a given set of search keys and frequencies. \n(b) Symmetric binary $B$ -trees are another self-balancing binary trees, first described by Rudolf Bayer in 1972; these are better known by the name red-black trees, after a somewhat simpler reformulation by Leo Guibas and Bob Sedgwick in 1978. A red-black tree is a binary search tree with the following additional constraints: \n• Every node is either red or black.   \n• Every red node has a black parent.   \n• Every root-to-leaf path contains the same number of black nodes. \nDescribe a recursive backtracking algorithm to construct an optimal red-black tree for a given set of search keys and frequencies. \n(c) AA trees were proposed by proposed by Arne Andersson in 1993 and slightly simplified (and named) by Mark Allen Weiss in 2000. AA trees are also known as left-leaning red-black trees, after a symmetric reformulation (with different rebalancing algorithms) by Bob Sedgewick in 2006. An AA tree is a red-black tree with one additional constraint: • No left child is red.14 \nDescribe a recursive backtracking algorithm to construct an optimal AA tree for a given set of search keys and frequencies. \nDon’t analyze the running times of your algorithms, except to satisfy your own curiosity. All three algorithms run in exponential time; we’ll improve that later, so the precise running times aren’t particularly important. \nFor more backtracking exercises, see the next chapter! \nPotes enim videre in hac margine, qualiter hoc operati fuimus, scilicet quod iunximus primum numerum cum secundo, videlicet 1 cum 2; et secundum cum tercio; et tercium cum quarto; et quartum cum quinto, et sic deinceps.... \n[You can see in the margin here how we have worked this; clearly, we combined the first number with the second, namely 1 with 2, and the second with the third, and the third with the fourth, and the fourth with the fifth, and so forth. . . .] \n— Leonardo Pisano, Liber Abaci (1202) \nThose who cannot remember the past are condemned to repeat it. \n— Jorge Agustín Nicolás Ruiz de Santayana y Borrás, The Life of Reason, Book I: Introduction and Reason in Common Sense (1905) \nYou know what a learning experience is? A learning experience is one of those things that says, “You know that thing you just did? Don’t do that.” \n— Douglas Adams, The Salmon of Doubt (2002) \n3 \nDynamic Programming \n3.1 M¯atra¯vr.tta \nOne of the earliest examples of recursion arose in India more than 2000 years ago, in the study of poetic meter, or prosody. Classical Sanskrit poetry distinguishes between two types of syllables (ak.sara): light (laghu) and heavy (guru). In one class of meters, variously called m¯atr¯av.rtta or m¯atr¯achandas, each line of poetry consists of a fixed number of “beats” (ma¯tra¯), where each light syllable lasts one beat and each heavy syllable lasts two beats. The formal study of m¯atr¯a-v.rtta dates back to the Chanda.hś¯astra, written by the scholar Pi˙ngala between 600bce and 200bce. Pi˙ngala observed that there are exactly five 4-beat meters: —, and • • • •. (Here each “—” represents a long syllable and each “•” represents a short syllable.)1",
        "chapter": "Backtracking",
        "section": "Exercises",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 39
      }
    },
    {
      "node_id": "tb1_node38",
      "content": "for $C o s t ( T , f [ 1 . . n ] )$ . Simple substitution now gives us a recurrence for Cost: \nThe base case for this recurrence is, as usual, $n = 0$ ; the cost of performing no searches in the empty tree is zero. \nNow our task is to compute the tree $T _ { mathrm { o p t } }$ that minimizes this cost function. Suppose we somehow magically knew that the root of $T _ { mathrm { o p t } }$ is $nu _ { r }$ . Then the recursive definition of $C o s t ( T , f )$ immediately implies that the left subtree left $( T _ { mathrm { o p t } } )$ must be the optimal search tree for the keys $A [ 1 ldots r - 1 ]$ and access frequencies $f [ 1 ldots r - 1 ]$ . Similarly, the right subtree $r i g h t ( T _ { mathrm { o p t } } )$ must be the optimal search tree for the keys $A [ r + 1 ldots n ]$ and access frequencies $f [ r + 1 ldots n ]$ . Once we choose the correct key to store at the root, the Recursion Fairy will construct the rest of the optimal tree. \nMore generally, let $O p t C o s t ( i , k )$ denote the total cost of the optimal search tree for the interval of frequencies $f [ i ldots k ]$ . This function obeys the following recurrence. \nThe base case correctly indicates that the minimum possible cost to perform zero searches into the empty set is zero! Our original problem is to compute $O p t C o s t ( 1 , n )$ . \nThis recursive definition can be translated mechanically into a recursive backtracking algorithm to compute $O p t C o s t ( 1 , n )$ . Not surprisingly, the running time of this algorithm is exponential. In the next chapter, we’ll see how to reduce the running time to polynomial, so there’s not much point in computing the precise running time. . . \nnAnalysis \n. unless you’re into that sort of thing. Just for the fun of it, let’s figure out how slow this backtracking algorithm actually is. The running time satisfies the recurrence \nThe $O ( n )$ term comes from computing the total number of searches $textstyle sum _ { i = 1 } ^ { n } f [ i ]$ . Yeah, that’s one ugly recurrence, but we can solve it using exactly the same \nsubtraction trick we used before. We replace the $O ( )$ notation with an explicit constant, regroup and collect identical terms, subtract the recurrence for $T ( n { - } 1 )$ to get rid of the summation, and then regroup again. \nHey, that doesn’t look so bad after all. The recursion tree method immediately gives us the solution $T ( n ) = O ( 3 ^ { n } )$ (or we can just guess and confirm by induction). \nThis analysis implies that our recursive algorithm does not examine all possible binary search trees! The number of binary search trees with $n$ vertices satisfies the recurrence \nwhich has the closed-form solution $N ( n ) = Theta ( 4 ^ { n } / sqrt { n } )$ . (No, that’s not obvious.) Our algorithm saves considerable time by searching independently for the optimal left and right subtrees for each root. A full enumeration of binary search trees would consider all possible pairs of left and right subtrees; hence the product in the recurrence for $N ( n )$ . \nExercises \n1. Describe recursive algorithms for the following generalizations of the SubsetSum problem: \n(a) Given an array $X [ 1 ldots n ]$ of positive integers and an integer $T$ , compute the number of subsets of $X$ whose elements sum to $T$ .   \n(b) Given two arrays $X [ 1 ldots n ]$ and $W [ 1 ldots n ]$ of positive integers and an integer $T$ , where each $W [ i ]$ denotes the weight of the corresponding element $X [ i ]$ , compute the maximum weight subset of $X$ whose elements sum to $T$ . If no subset of $X$ sums to $T$ , your algorithm should return $- infty$ . \n2. Describe recursive algorithms for the following variants of the text segmentation problem. Assume that you have a subroutine IsWord that takes an array of characters as input and returns True if and only if that string is a “word”.",
      "metadata": {
        "content": "for $C o s t ( T , f [ 1 . . n ] )$ . Simple substitution now gives us a recurrence for Cost: \nThe base case for this recurrence is, as usual, $n = 0$ ; the cost of performing no searches in the empty tree is zero. \nNow our task is to compute the tree $T _ { mathrm { o p t } }$ that minimizes this cost function. Suppose we somehow magically knew that the root of $T _ { mathrm { o p t } }$ is $nu _ { r }$ . Then the recursive definition of $C o s t ( T , f )$ immediately implies that the left subtree left $( T _ { mathrm { o p t } } )$ must be the optimal search tree for the keys $A [ 1 ldots r - 1 ]$ and access frequencies $f [ 1 ldots r - 1 ]$ . Similarly, the right subtree $r i g h t ( T _ { mathrm { o p t } } )$ must be the optimal search tree for the keys $A [ r + 1 ldots n ]$ and access frequencies $f [ r + 1 ldots n ]$ . Once we choose the correct key to store at the root, the Recursion Fairy will construct the rest of the optimal tree. \nMore generally, let $O p t C o s t ( i , k )$ denote the total cost of the optimal search tree for the interval of frequencies $f [ i ldots k ]$ . This function obeys the following recurrence. \nThe base case correctly indicates that the minimum possible cost to perform zero searches into the empty set is zero! Our original problem is to compute $O p t C o s t ( 1 , n )$ . \nThis recursive definition can be translated mechanically into a recursive backtracking algorithm to compute $O p t C o s t ( 1 , n )$ . Not surprisingly, the running time of this algorithm is exponential. In the next chapter, we’ll see how to reduce the running time to polynomial, so there’s not much point in computing the precise running time. . . \nnAnalysis \n. unless you’re into that sort of thing. Just for the fun of it, let’s figure out how slow this backtracking algorithm actually is. The running time satisfies the recurrence \nThe $O ( n )$ term comes from computing the total number of searches $textstyle sum _ { i = 1 } ^ { n } f [ i ]$ . Yeah, that’s one ugly recurrence, but we can solve it using exactly the same \nsubtraction trick we used before. We replace the $O ( )$ notation with an explicit constant, regroup and collect identical terms, subtract the recurrence for $T ( n { - } 1 )$ to get rid of the summation, and then regroup again. \nHey, that doesn’t look so bad after all. The recursion tree method immediately gives us the solution $T ( n ) = O ( 3 ^ { n } )$ (or we can just guess and confirm by induction). \nThis analysis implies that our recursive algorithm does not examine all possible binary search trees! The number of binary search trees with $n$ vertices satisfies the recurrence \nwhich has the closed-form solution $N ( n ) = Theta ( 4 ^ { n } / sqrt { n } )$ . (No, that’s not obvious.) Our algorithm saves considerable time by searching independently for the optimal left and right subtrees for each root. A full enumeration of binary search trees would consider all possible pairs of left and right subtrees; hence the product in the recurrence for $N ( n )$ . \nExercises \n1. Describe recursive algorithms for the following generalizations of the SubsetSum problem: \n(a) Given an array $X [ 1 ldots n ]$ of positive integers and an integer $T$ , compute the number of subsets of $X$ whose elements sum to $T$ .   \n(b) Given two arrays $X [ 1 ldots n ]$ and $W [ 1 ldots n ]$ of positive integers and an integer $T$ , where each $W [ i ]$ denotes the weight of the corresponding element $X [ i ]$ , compute the maximum weight subset of $X$ whose elements sum to $T$ . If no subset of $X$ sums to $T$ , your algorithm should return $- infty$ . \n2. Describe recursive algorithms for the following variants of the text segmentation problem. Assume that you have a subroutine IsWord that takes an array of characters as input and returns True if and only if that string is a “word”.",
        "chapter": "Backtracking",
        "section": "Optimal Binary Search Trees",
        "subsection": "♥Analysis",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 38
      }
    },
    {
      "node_id": "tb1_node32",
      "content": "2.4 The General Pattern \nBacktracking algorithms are commonly used to make a sequence of decisions, with the goal of building a recursively defined structure satisfying certain constraints. Often (but not always) this goal structure is itself a sequence. For example: \n• In the $n$ -queens problem, the goal is a sequence of queen positions, one in each row, such that no two queens attack each other. For each row, the algorithm decides where to place the queen.   \n• In the game tree problem, the goal is a sequence of legal moves, such that each move is as good as possible for the player making it. For each game state, the algorithm decides the best possible next move.   \n• In the SubsetSum problem, the goal is a sequence of input elements that have a particular sum. For each input element, the algorithm decides whether to include it in the output sequence or not. \n(Hang on, why is the goal of subset sum finding a sequence? That was a deliberate design decision. We imposed a convenient ordering on the input set—by representing it using an array, as opposed to some other more amorphous data structure—that we can exploit in our recursive algorithm.) \nIn each recursive call to the backtracking algorithm, we need to make exactly one decision, and our choice must be consistent with all previous decisions. Thus, each recursive call requires not only the portion of the input data we have not yet processed, but also a suitable summary of the decisions we have already made. For the sake of efficiency, the summary of past decisions should be as small as possible. For example: \n• For the $n$ -queens problem, we must pass in not only the number of empty rows, but the positions of all previously placed queens. Here, unfortunately, we must remember our past decisions in complete detail. • For the game tree problem, we only need to pass in the current state of the game, including the identity of the next player. We don’t need to remember anything about our past decisions, because who wins from a given game state does not depend on the moves that created that state.7 • For the SubsetSum problem, we need to pass in both the remaining available integers and the remaining target value, which is the original target value minus the sum of the previously chosen elements. Precisely which elements were previously chosen is unimportant. \nWhen we design new recursive backtracking algorithms, we must figure out in advance what information we will need about past decisions in the middle of the algorithm. If this information is nontrivial, our recursive algorithm might need to solve a more general problem than the one we were originally asked to solve. (We’ve seen this kind of generalization before: To find the median of an unsorted array in linear time, we derived an algorithm to select the kth smallest element for arbitrary $k$ .) \nFinally, once we’ve figured out what recursive problem we really need to solve, we solve that problem by recursive brute force: Try all possibilities for the next decision that are consistent with past decisions, and let the Recursion Fairy worry about the rest. No being clever here. No skipping “obviously” stupid choices. Try everything. You can make the algorithm faster later. \n2.5 Text Segmentation (Interpunctio Verborum) \nSuppose you are given a string of letters representing text in some foreign language, but without any spaces or punctuation, and you want to break this string into its individual constituent words. For example, you might be given the following passage from Cicero’s famous oration in defense of Lucius Licinius Murena in 62bce, in the standard scriptio continua of classical Latin:8",
      "metadata": {
        "content": "2.4 The General Pattern \nBacktracking algorithms are commonly used to make a sequence of decisions, with the goal of building a recursively defined structure satisfying certain constraints. Often (but not always) this goal structure is itself a sequence. For example: \n• In the $n$ -queens problem, the goal is a sequence of queen positions, one in each row, such that no two queens attack each other. For each row, the algorithm decides where to place the queen.   \n• In the game tree problem, the goal is a sequence of legal moves, such that each move is as good as possible for the player making it. For each game state, the algorithm decides the best possible next move.   \n• In the SubsetSum problem, the goal is a sequence of input elements that have a particular sum. For each input element, the algorithm decides whether to include it in the output sequence or not. \n(Hang on, why is the goal of subset sum finding a sequence? That was a deliberate design decision. We imposed a convenient ordering on the input set—by representing it using an array, as opposed to some other more amorphous data structure—that we can exploit in our recursive algorithm.) \nIn each recursive call to the backtracking algorithm, we need to make exactly one decision, and our choice must be consistent with all previous decisions. Thus, each recursive call requires not only the portion of the input data we have not yet processed, but also a suitable summary of the decisions we have already made. For the sake of efficiency, the summary of past decisions should be as small as possible. For example: \n• For the $n$ -queens problem, we must pass in not only the number of empty rows, but the positions of all previously placed queens. Here, unfortunately, we must remember our past decisions in complete detail. • For the game tree problem, we only need to pass in the current state of the game, including the identity of the next player. We don’t need to remember anything about our past decisions, because who wins from a given game state does not depend on the moves that created that state.7 • For the SubsetSum problem, we need to pass in both the remaining available integers and the remaining target value, which is the original target value minus the sum of the previously chosen elements. Precisely which elements were previously chosen is unimportant. \nWhen we design new recursive backtracking algorithms, we must figure out in advance what information we will need about past decisions in the middle of the algorithm. If this information is nontrivial, our recursive algorithm might need to solve a more general problem than the one we were originally asked to solve. (We’ve seen this kind of generalization before: To find the median of an unsorted array in linear time, we derived an algorithm to select the kth smallest element for arbitrary $k$ .) \nFinally, once we’ve figured out what recursive problem we really need to solve, we solve that problem by recursive brute force: Try all possibilities for the next decision that are consistent with past decisions, and let the Recursion Fairy worry about the rest. No being clever here. No skipping “obviously” stupid choices. Try everything. You can make the algorithm faster later. \n2.5 Text Segmentation (Interpunctio Verborum) \nSuppose you are given a string of letters representing text in some foreign language, but without any spaces or punctuation, and you want to break this string into its individual constituent words. For example, you might be given the following passage from Cicero’s famous oration in defense of Lucius Licinius Murena in 62bce, in the standard scriptio continua of classical Latin:8",
        "chapter": "Backtracking",
        "section": "The General Pattern",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 32
      }
    },
    {
      "node_id": "tb1_node33",
      "content": "We are now left with a simple and natural backtracking strategy: Select the first output word, and recursively segment the rest of the input string. \nTo get a complete recursive algorithm, we need a base case. Our recursive strategy breaks down when we reach the end of the input string, because there is no next word. Fortunately, the empty string has a unique segmentation into zero words! \nPutting all the pieces together, we arrive at the following simple recursive algorithm: \nIndex Formulation \nIn practice, passing arrays as input parameters is rather slow; we should really find a more compact way to describe our recursive subproblems. For purposes of designing the algorithm, it’s incredibly useful to treat the original input array as a global variable, and then reformulate the problem and the algorithm in terms of array indices instead of explicit subarrays. \nFor our string segmentation problem, the argument of any recursive call is always a suffix $A [ i ldots n ]$ of the original input array. So if we treat the input array $A [ 1 ldots n ]$ as a global variable, we can reformulate our recursive problem as follows: \nGiven an index $i$ , find a segmentation of the suffix $A [ i ldots n ]$ . \nTo describe our algorithm, we need two boolean functions: \n• For any indices $i$ and $j$ , let $mathrm { I s W o R D } ( i , j ) = mathrm { T R U E }$ if and only if the substring $A [ i . . j ]$ is a word. (We’re assuming this function is given to us.) • For any index $i$ , let Splittable $( i ) =$ True if and only if the suffix $A [ i ldots n ]$ can be split into words. (This is the function we need to implement.) \nFor example, $mathrm { I s W o R D } ( 1 , n ) = mathrm { T R U E }$ if and only if the entire input string is a single word, and Splittable $( 1 ) =$ True if and only if the entire input string can be segmented. Our earlier recursive strategy gives us the following recurrence: \nThis is exactly the same algorithm as we saw earlier; the only thing we’ve changed is the notation. The similarity is even more apparent if we rewrite the recurrence in pseudocode: \nAlthough it may look like a trivial notational difference, using index notation instead of array notation is an important habit, not only to speed up backtracking algorithms in practice, but for developing dynamic programming algorithms, which we discuss in the next chapter. \nnAnalysis \nIt should come as no surprise that most backtracking algorithms have exponential worst-case running times. Analyzing the precise running times of many of these algorithms requires techniques that are beyond the scope of this book. Fortunately, most of the backtracking algorithms we will encounter in this book are only intermediate results on the way to more efficient algorithms, which means their exact worst-case running time is not actually important. (First make it work; then make it fast.) \nBut just for fun, let’s analyze the running time of our recursive algorithm Splittable. Because we don’t know what IsWord is doing, we can’t know how long each call to IsWord takes, so we’re forced to analyze the running time in terms of the number of calls to IsWord.11 Splittable calls IsWord on every prefix of the input string, and possibly calls itself recursively on every suffix of the output string. Thus, the “running time” of Splittable obeys the scary-looking recurrence \nThis really isn’t as bad as it looks, especially once you’ve seen the trick. \nFirst, we replace the $O ( n )$ term with an explicit expression $a n$ , for some unknown (and ultimately unimportant) constant $alpha$ . Second, we conservatively",
      "metadata": {
        "content": "We are now left with a simple and natural backtracking strategy: Select the first output word, and recursively segment the rest of the input string. \nTo get a complete recursive algorithm, we need a base case. Our recursive strategy breaks down when we reach the end of the input string, because there is no next word. Fortunately, the empty string has a unique segmentation into zero words! \nPutting all the pieces together, we arrive at the following simple recursive algorithm: \nIndex Formulation \nIn practice, passing arrays as input parameters is rather slow; we should really find a more compact way to describe our recursive subproblems. For purposes of designing the algorithm, it’s incredibly useful to treat the original input array as a global variable, and then reformulate the problem and the algorithm in terms of array indices instead of explicit subarrays. \nFor our string segmentation problem, the argument of any recursive call is always a suffix $A [ i ldots n ]$ of the original input array. So if we treat the input array $A [ 1 ldots n ]$ as a global variable, we can reformulate our recursive problem as follows: \nGiven an index $i$ , find a segmentation of the suffix $A [ i ldots n ]$ . \nTo describe our algorithm, we need two boolean functions: \n• For any indices $i$ and $j$ , let $mathrm { I s W o R D } ( i , j ) = mathrm { T R U E }$ if and only if the substring $A [ i . . j ]$ is a word. (We’re assuming this function is given to us.) • For any index $i$ , let Splittable $( i ) =$ True if and only if the suffix $A [ i ldots n ]$ can be split into words. (This is the function we need to implement.) \nFor example, $mathrm { I s W o R D } ( 1 , n ) = mathrm { T R U E }$ if and only if the entire input string is a single word, and Splittable $( 1 ) =$ True if and only if the entire input string can be segmented. Our earlier recursive strategy gives us the following recurrence: \nThis is exactly the same algorithm as we saw earlier; the only thing we’ve changed is the notation. The similarity is even more apparent if we rewrite the recurrence in pseudocode: \nAlthough it may look like a trivial notational difference, using index notation instead of array notation is an important habit, not only to speed up backtracking algorithms in practice, but for developing dynamic programming algorithms, which we discuss in the next chapter. \nnAnalysis \nIt should come as no surprise that most backtracking algorithms have exponential worst-case running times. Analyzing the precise running times of many of these algorithms requires techniques that are beyond the scope of this book. Fortunately, most of the backtracking algorithms we will encounter in this book are only intermediate results on the way to more efficient algorithms, which means their exact worst-case running time is not actually important. (First make it work; then make it fast.) \nBut just for fun, let’s analyze the running time of our recursive algorithm Splittable. Because we don’t know what IsWord is doing, we can’t know how long each call to IsWord takes, so we’re forced to analyze the running time in terms of the number of calls to IsWord.11 Splittable calls IsWord on every prefix of the input string, and possibly calls itself recursively on every suffix of the output string. Thus, the “running time” of Splittable obeys the scary-looking recurrence \nThis really isn’t as bad as it looks, especially once you’ve seen the trick. \nFirst, we replace the $O ( n )$ term with an explicit expression $a n$ , for some unknown (and ultimately unimportant) constant $alpha$ . Second, we conservatively",
        "chapter": "Backtracking",
        "section": "Text Segmentation (Interpunctio Verborum)",
        "subsection": "Index Formulation",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 33
      }
    }
  ],
  "output": {
    "question": "Consider the recurrence relation for the running time of a naive recursive Fibonacci algorithm, which is given as T(n) = T(n-1) + T(n-2) + O(1). Using the 'subtraction trick' method described for transforming recurrences, how would you derive a simplified recurrence for T(n) that eliminates the summation of terms, and what would the resulting closed-form time complexity be?",
    "correct_answer": "Applying the subtraction trick: Write T(n) = T(n-1) + T(n-2) + c. Then write T(n-1) = T(n-2) + T(n-3) + c. Subtracting the second from the first gives T(n) - T(n-1) = T(n-1) - T(n-3). Rearranging yields T(n) = 2T(n-1) - T(n-3). This recurrence simplifies analysis. The original recurrence has exponential time complexity O(φⁿ) where φ is the golden ratio (~1.618), which is approximately O(1.618ⁿ).",
    "distractors": [
      "The subtraction trick yields T(n) = T(n-1) + O(1), leading to O(n) time complexity.",
      "After subtraction, we get T(n) = T(n-1) + T(n-2) - T(n-3), which simplifies to O(n²) time.",
      "The method transforms it to T(n) = 3T(n-1) + O(1), resulting in O(3ⁿ) exponential time."
    ],
    "rationale": "Step 1: Extract the naive Fibonacci recurrence from snippet [5], which states: 'Let T(n) denote the number of recursive calls to RecFibo; this function satisfies the recurrence...' (implied as T(n) = T(n-1) + T(n-2) + O(1)). Step 2: Connect to the 'subtraction trick' method from snippet [8], which describes: 'subtract the recurrence for T(n-1) to get rid of the summation'. Step 3: Synthesize by applying the subtraction trick to the Fibonacci recurrence to derive a transformed recurrence. Step 4: Infer the exponential time complexity from the original recurrence's behavior, consistent with exponential patterns mentioned in snippet [4] for similar recurrences."
  },
  "score": {
    "relevance": 1.0,
    "faithfulness": 0.8,
    "integration": 1.0,
    "complexity": 1.0,
    "overall": 0.96
  }
}
{
  "context": [
    {
      "node_id": "tb13_node56",
      "content": "6 \nRecursion \nRecursion is a programming technique in which a method (function) calls itself. This may sound like a strange thing to do, or even a catastrophic mistake. Recursion is, however, one of the most interesting, and one of the most surprisingly effective, techniques in programming. Like pulling yourself up by your bootstraps (you do have bootstraps, don’t you?), recursion seems incredible when you first encounter it. However, it not only works, it also provides a unique conceptual framework for solving many problems. \nIn this chapter we’ll examine numerous examples to show the wide variety of situations to which recursion can be applied. We will calculate triangular numbers and factorials, generate anagrams, perform a recursive binary search, solve the Towers of Hanoi puzzle, and investigate a sorting technique called mergesort. Workshop applets are provided to demonstrate the Towers of Hanoi and mergesort. \nWe’ll also discuss the strengths and weaknesses of recursion, and show how a recursive approach can be transformed into a stack-based approach. \nTriangular Numbers \nIt’s said that the Pythagorians, a band of mathematicians in ancient Greece who worked under Pythagoras (of Pythagorian theorem fame), felt a mystical connection with the series of numbers 1, 3, 6, 10, 15, 21, … (where the … means the series continues indefinitely). Can you find the next member of this series? \nThe nth term in the series is obtained by adding n to the previous term. Thus, the second term is found by adding 2 to the first term (which is 1), giving 3. The third term is 3 added to the second term (which is 3) giving 6, and so on. \nIN THIS CHAPTER \n• Triangular Numbers   \n• Factorials   \n• Anagrams   \n• A Recursive Binary Search   \n• The Towers of Hanoi   \n• Mergesort   \n• Eliminating Recursion   \n• Some Interesting Recursive Applications \nThe numbers in this series are called triangular numbers because they can be visualized as a triangular arrangement of objects, shown as little squares in Figure 6.1. \nFinding the nth Term Using a Loop \nSuppose you wanted to find the value of some arbitrary nth term in the series—say the fourth term (whose value is 10). How would you calculate it? Looking at Figure 6.2, you might decide that the value of any term can be obtained by adding up all the vertical columns of squares. \nIn the fourth term, the first column has four little squares, the second column has three, and so on. Adding $4 + 3 + 2 + 1$ gives 10. \nThe following triangle() method uses this column-based technique to find a triangular number. It sums all the columns, from a height of n to a height of 1: \nint triangle(int n) { int total $mathit { Theta } = mathit { Theta } 0$ ; while $( boldsymbol { mathsf { n } } > boldsymbol { mathsf { 0 } } )$ // until n is 1 { $mathsf { t o t a l  l =  t o t a l  l +  n _ { i } }$ ; $bigstar / / bigstar$ add n (column height) to total $cdots 1$ ; $bigstar / / bigstar$ decrement column height ${ }$ return total; } \nThe method cycles around the loop n times, adding n to total the first time, n-1 the second time, and so on down to 1, quitting the loop when n becomes 0. \nFinding the nth Term Using Recursion \nThe loop approach may seem straightforward, but there’s another way to look at this problem. The value of the nth term can be thought of as the sum of only two things, instead of a whole series. They are \n1. The first (tallest) column, which has the value n.   \n2. The sum of all the remaining columns. \nThis is shown in Figure 6.3. \nFinding the Remaining Columns \nIf we knew about a method that found the sum of all the remaining columns, we could write our triangle() method, which returns the value of the nth triangular number, like this: \nint triangle(int n) { return( n $^ +$ sumRemainingColumns(n) );  // (incomplete version) } \nBut what have we gained here? It looks like writing the sumRemainingColumns() method is just as hard as writing the triangle() method in the first place. \nNotice in Figure 6.3, however, that the sum of all the remaining columns for term n is the same as the sum of all the columns for term n-1. Thus, if we knew about a method that summed all the columns for term n, we could call it with an argument of $mathsf { n } cdot mathsf { 1 }$ to find the sum of all the remaining columns for term n: \nint triangle(int n) { return( n $^ +$ sumAllColumns(n-1) ); $bigstar / / bigstar$ (incomplete version) } \nBut when you think about it, the sumAllColumns() method is doing exactly the same thing the triangle() method is: summing all the columns for some number n passed as an argument. So why not use the triangle() method itself, instead of some other method? That would look like this: \nint triangle(int n) { return( n $^ +$ triangle(n-1) );  // (incomplete version) } \nYou may be amazed that a method can call itself, but why shouldn’t it be able to? A method call is (among other things) a transfer of control to the start of the method. This transfer of control can take place from within the method as well as from outside. \nPassing the Buck \nAll these approaches may seem like passing the buck. Someone tells me to find the 9th triangular number. I know this is 9 plus the 8th triangular number, so I call Harry and ask him to find the 8th triangular number. When I hear back from him, I’ll add 9 to whatever he tells me, and that will be the answer. \nHarry knows the 8th triangular number is 8 plus the 7th triangular number, so he calls Sally and asks her to find the 7th triangular number. This process continues with each person passing the buck to another one. \nWhere does this buck-passing end? Someone at some point must be able to figure out an answer that doesn’t involve asking another person to help. If this didn’t happen, there would be an infinite chain of people asking other people questions—a sort of arithmetic Ponzi scheme that would never end. In the case of triangle(), this would mean the method calling itself over and over in an infinite series that would eventually crash the program. \nThe Buck Stops Here \nTo prevent an infinite regress, the person who is asked to find the first triangular number of the series, when n is 1, must know, without asking anyone else, that the answer is 1. There are no smaller numbers to ask anyone about, there’s nothing left to add to anything else, so the buck stops there. We can express this by adding a condition to the triangle() method: \nint triangle(int n) { if $n = = 1$ ) return 1; else return( n $^ +$ triangle(n-1) ); } \nThe condition that leads to a recursive method returning without making another recursive call is referred to as the base case. It’s critical that every recursive method have a base case to prevent infinite recursion and the consequent demise of the program. \nThe triangle.java Program \nDoes recursion actually work? If you run the triangle.java program, you’ll see that it does. Enter a value for the term number, n, and the program will display the value of the corresponding triangular number. Listing 6.1 shows the triangle.java program. \nLISTING 6.1 The triangle.java Program \n// triangle.java   \n// evaluates triangular numbers   \n// to run this program: $complement >$ java TriangleApp   \nimport java.io.*; // for I/O   \n//////////////////// //////////////////// /////////////// \nLISTING 6.1 Continued \nclass TriangleApp { static int theNumber; public static void main(String[] args) throws IOException { System.out.print(“Enter a number: “); theNumber $mathbf { tau } = mathbf { tau }$ getInt(); int theAnswer $mathbf { tau } = mathbf { tau }$ triangle(theNumber); System.out.println(“Triangle $mathbf { tau } = mathbf { tau }$ ”+theAnswer); }  // end main() public static int triangle(int n) { if $scriptstyle mathsf { n } = = 1$ ) return 1; else return( n $^ +$ triangle(n-1) ); }   \n/ public static String getString() throws IOException { InputStreamReader isr $mathbf { tau } = mathbf { tau }$ new InputStreamReader(System.in); BufferedReader br $mathbf { tau } = mathbf { tau }$ new BufferedReader(isr); String $textsf { s } =$ br.readLine(); return s; }   \n// public static int getInt() throws IOException ${$ String s $mathbf { tau } = mathbf { tau }$ getString(); return Integer.parseInt(s); } }  // end class TriangleApp   \n//////////////////////////////////////////////////////////////// \nThe main() routine prompts the user for a value for n, calls triangle(), and displays the return value. The triangle() method calls itself repeatedly to do all the work. \nHere’s some sample output: \nEnter a number: 1000   \nTriangle $mathbf { tau } = mathbf { tau }$ 500500 \nIncidentally, if you’re skeptical of the results returned from triangle(), you can check them by using the following formula: \nnth triangular number $= ( mathrm { n } ^ { 2 } { + } mathrm { n } ) / 2$ \nWhat’s Really Happening? \nLet’s modify the triangle() method to provide an insight into what’s happening when it executes. We’ll insert some output statements to keep track of the arguments and return values: \npublic static int triangle(int n) { System.out.println(“Entering: ${ mathsf { n } } = \" + { mathsf { n } } )$ ); if $scriptstyle mathtt { n = = 1 }$ ) ${$ System.out.println(“Returning 1”); return 1; } else { int temp $mathbf { zeta } = mathbf { zeta } mathsf { n beta } + mathbf { beta }$ triangle(n-1); System.out.println(“Returning “ $^ +$ temp); return temp; } } \nHere’s the interaction when this method is substituted for the earlier triangle() method and the user enters 5: \nEnter a number: 5 \nEntering: $n = 5$ Entering: $scriptstyle { mathsf { n } } = 4$ Entering: $scriptstyle { mathsf { n } } = 3$ Entering: $scriptstyle { mathsf { n } } = 2$ Entering: $mathsf { n } = 1$ Returning 1 Returning 3 Returning 6 \nReturning 10   \nReturning 15 \nTriangle $mathbf { tau } = mathbf { tau }$ 15 \nEach time the triangle() method calls itself, its argument, which starts at 5, is reduced by 1. The method plunges down into itself again and again until its argument is reduced to 1. Then it returns. This triggers an entire series of returns. The method rises back up, phoenix-like, out of the discarded versions of itself. Each time it returns, it adds the value of n it was called with to the return value from the method it called. \nThe return values recapitulate the series of triangular numbers, until the answer is returned to main(). Figure 6.4 shows how each invocation of the triangle() method can be imagined as being “inside” the previous one. \nNotice that, just before the innermost version returns a 1, there are actually five different incarnations of triangle() in existence at the same time. The outer one was passed the argument 5; the inner one was passed the argument 1. \nCharacteristics of Recursive Methods \nAlthough it’s short, the triangle() method possesses the key features common to all recursive routines: \n• It calls itself.   \n• When it calls itself, it does so to solve a smaller problem.   \n• There’s some version of the problem that is simple enough that the routine can solve it, and return, without calling itself. \nIn each successive call of a recursive method to itself, the argument becomes smaller (or perhaps a range described by multiple arguments becomes smaller), reflecting the fact that the problem has become “smaller” or easier. When the argument or range reaches a certain minimum size, a condition is triggered and the method returns without calling itself. \nIs Recursion Efficient? \nCalling a method involves certain overhead. Control must be transferred from the location of the call to the beginning of the method. In addition, the arguments to the method and the address to which the method should return must be pushed onto an internal stack so that the method can access the argument values and know where to return. \nIn the case of the triangle() method, it’s probable that, as a result of this overhead, the while loop approach executes more quickly than the recursive approach. The penalty may not be significant, but if there are a large number of method calls as a result of a recursive method, it might be desirable to eliminate the recursion. We’ll talk about this issue more at the end of this chapter. \nAnother inefficiency is that memory is used to store all the intermediate arguments and return values on the system’s internal stack. This may cause problems if there is a large amount of data, leading to stack overflow. \nRecursion is usually used because it simplifies a problem conceptually, not because it’s inherently more efficient. \nMathematical Induction \nRecursion is the programming equivalent of mathematical induction. Mathematical induction is a way of defining something in terms of itself. (The term is also used to \ndescribe a related approach to proving theorems.) Using induction, we could define the triangular numbers mathematically by saying \n$begin{array} { l } { { mathrm { t r i } ( mathrm { n } ) = 1 } }  { { mathrm { t r i } ( mathrm { n } ) = mathrm { n } + mathrm { t r i } ( mathrm { n } - 1 ) } } end{array}$ $begin{array} { c } { operatorname { i f } mathrm { n } = 1 }  { operatorname { i f } mathrm { n } > 1 } end{array}$ \nDefining something in terms of itself may seem circular, but in fact it’s perfectly valid (provided there’s a base case). \nFactorials \nFactorials are similar in concept to triangular numbers, except that multiplication is used instead of addition. The triangular number corresponding to n is found by adding n to the triangular number of $mathsf { n } - 1$ , while the factorial of n is found by multiplying n by the factorial of ${ n - 1 }$ . That is, the fifth triangular number is $5 + 4 + 3 + 2 + 1$ , while the factorial of 5 is $5 ^ { star } 4 ^ { star } 3 ^ { star } 2 ^ { star } 1$ , which equals 120. Table 6.1 shows the factorials of the first 10 numbers. \nThe factorial of 0 is defined to be 1. Factorial numbers grow large very rapidly, as you can see. \nA recursive method similar to triangle() can be used to calculate factorials. It looks like this: \nint factorial(int n) { if $scriptstyle mathsf { n = = } 0$ ) return 1; else return (n * factorial(n-1) ); }",
      "metadata": {
        "content": "6 \nRecursion \nRecursion is a programming technique in which a method (function) calls itself. This may sound like a strange thing to do, or even a catastrophic mistake. Recursion is, however, one of the most interesting, and one of the most surprisingly effective, techniques in programming. Like pulling yourself up by your bootstraps (you do have bootstraps, don’t you?), recursion seems incredible when you first encounter it. However, it not only works, it also provides a unique conceptual framework for solving many problems. \nIn this chapter we’ll examine numerous examples to show the wide variety of situations to which recursion can be applied. We will calculate triangular numbers and factorials, generate anagrams, perform a recursive binary search, solve the Towers of Hanoi puzzle, and investigate a sorting technique called mergesort. Workshop applets are provided to demonstrate the Towers of Hanoi and mergesort. \nWe’ll also discuss the strengths and weaknesses of recursion, and show how a recursive approach can be transformed into a stack-based approach. \nTriangular Numbers \nIt’s said that the Pythagorians, a band of mathematicians in ancient Greece who worked under Pythagoras (of Pythagorian theorem fame), felt a mystical connection with the series of numbers 1, 3, 6, 10, 15, 21, … (where the … means the series continues indefinitely). Can you find the next member of this series? \nThe nth term in the series is obtained by adding n to the previous term. Thus, the second term is found by adding 2 to the first term (which is 1), giving 3. The third term is 3 added to the second term (which is 3) giving 6, and so on. \nIN THIS CHAPTER \n• Triangular Numbers   \n• Factorials   \n• Anagrams   \n• A Recursive Binary Search   \n• The Towers of Hanoi   \n• Mergesort   \n• Eliminating Recursion   \n• Some Interesting Recursive Applications \nThe numbers in this series are called triangular numbers because they can be visualized as a triangular arrangement of objects, shown as little squares in Figure 6.1. \nFinding the nth Term Using a Loop \nSuppose you wanted to find the value of some arbitrary nth term in the series—say the fourth term (whose value is 10). How would you calculate it? Looking at Figure 6.2, you might decide that the value of any term can be obtained by adding up all the vertical columns of squares. \nIn the fourth term, the first column has four little squares, the second column has three, and so on. Adding $4 + 3 + 2 + 1$ gives 10. \nThe following triangle() method uses this column-based technique to find a triangular number. It sums all the columns, from a height of n to a height of 1: \nint triangle(int n) { int total $mathit { Theta } = mathit { Theta } 0$ ; while $( boldsymbol { mathsf { n } } > boldsymbol { mathsf { 0 } } )$ // until n is 1 { $mathsf { t o t a l  l =  t o t a l  l +  n _ { i } }$ ; $bigstar / / bigstar$ add n (column height) to total $cdots 1$ ; $bigstar / / bigstar$ decrement column height ${ }$ return total; } \nThe method cycles around the loop n times, adding n to total the first time, n-1 the second time, and so on down to 1, quitting the loop when n becomes 0. \nFinding the nth Term Using Recursion \nThe loop approach may seem straightforward, but there’s another way to look at this problem. The value of the nth term can be thought of as the sum of only two things, instead of a whole series. They are \n1. The first (tallest) column, which has the value n.   \n2. The sum of all the remaining columns. \nThis is shown in Figure 6.3. \nFinding the Remaining Columns \nIf we knew about a method that found the sum of all the remaining columns, we could write our triangle() method, which returns the value of the nth triangular number, like this: \nint triangle(int n) { return( n $^ +$ sumRemainingColumns(n) );  // (incomplete version) } \nBut what have we gained here? It looks like writing the sumRemainingColumns() method is just as hard as writing the triangle() method in the first place. \nNotice in Figure 6.3, however, that the sum of all the remaining columns for term n is the same as the sum of all the columns for term n-1. Thus, if we knew about a method that summed all the columns for term n, we could call it with an argument of $mathsf { n } cdot mathsf { 1 }$ to find the sum of all the remaining columns for term n: \nint triangle(int n) { return( n $^ +$ sumAllColumns(n-1) ); $bigstar / / bigstar$ (incomplete version) } \nBut when you think about it, the sumAllColumns() method is doing exactly the same thing the triangle() method is: summing all the columns for some number n passed as an argument. So why not use the triangle() method itself, instead of some other method? That would look like this: \nint triangle(int n) { return( n $^ +$ triangle(n-1) );  // (incomplete version) } \nYou may be amazed that a method can call itself, but why shouldn’t it be able to? A method call is (among other things) a transfer of control to the start of the method. This transfer of control can take place from within the method as well as from outside. \nPassing the Buck \nAll these approaches may seem like passing the buck. Someone tells me to find the 9th triangular number. I know this is 9 plus the 8th triangular number, so I call Harry and ask him to find the 8th triangular number. When I hear back from him, I’ll add 9 to whatever he tells me, and that will be the answer. \nHarry knows the 8th triangular number is 8 plus the 7th triangular number, so he calls Sally and asks her to find the 7th triangular number. This process continues with each person passing the buck to another one. \nWhere does this buck-passing end? Someone at some point must be able to figure out an answer that doesn’t involve asking another person to help. If this didn’t happen, there would be an infinite chain of people asking other people questions—a sort of arithmetic Ponzi scheme that would never end. In the case of triangle(), this would mean the method calling itself over and over in an infinite series that would eventually crash the program. \nThe Buck Stops Here \nTo prevent an infinite regress, the person who is asked to find the first triangular number of the series, when n is 1, must know, without asking anyone else, that the answer is 1. There are no smaller numbers to ask anyone about, there’s nothing left to add to anything else, so the buck stops there. We can express this by adding a condition to the triangle() method: \nint triangle(int n) { if $n = = 1$ ) return 1; else return( n $^ +$ triangle(n-1) ); } \nThe condition that leads to a recursive method returning without making another recursive call is referred to as the base case. It’s critical that every recursive method have a base case to prevent infinite recursion and the consequent demise of the program. \nThe triangle.java Program \nDoes recursion actually work? If you run the triangle.java program, you’ll see that it does. Enter a value for the term number, n, and the program will display the value of the corresponding triangular number. Listing 6.1 shows the triangle.java program. \nLISTING 6.1 The triangle.java Program \n// triangle.java   \n// evaluates triangular numbers   \n// to run this program: $complement >$ java TriangleApp   \nimport java.io.*; // for I/O   \n//////////////////// //////////////////// /////////////// \nLISTING 6.1 Continued \nclass TriangleApp { static int theNumber; public static void main(String[] args) throws IOException { System.out.print(“Enter a number: “); theNumber $mathbf { tau } = mathbf { tau }$ getInt(); int theAnswer $mathbf { tau } = mathbf { tau }$ triangle(theNumber); System.out.println(“Triangle $mathbf { tau } = mathbf { tau }$ ”+theAnswer); }  // end main() public static int triangle(int n) { if $scriptstyle mathsf { n } = = 1$ ) return 1; else return( n $^ +$ triangle(n-1) ); }   \n/ public static String getString() throws IOException { InputStreamReader isr $mathbf { tau } = mathbf { tau }$ new InputStreamReader(System.in); BufferedReader br $mathbf { tau } = mathbf { tau }$ new BufferedReader(isr); String $textsf { s } =$ br.readLine(); return s; }   \n// public static int getInt() throws IOException ${$ String s $mathbf { tau } = mathbf { tau }$ getString(); return Integer.parseInt(s); } }  // end class TriangleApp   \n//////////////////////////////////////////////////////////////// \nThe main() routine prompts the user for a value for n, calls triangle(), and displays the return value. The triangle() method calls itself repeatedly to do all the work. \nHere’s some sample output: \nEnter a number: 1000   \nTriangle $mathbf { tau } = mathbf { tau }$ 500500 \nIncidentally, if you’re skeptical of the results returned from triangle(), you can check them by using the following formula: \nnth triangular number $= ( mathrm { n } ^ { 2 } { + } mathrm { n } ) / 2$ \nWhat’s Really Happening? \nLet’s modify the triangle() method to provide an insight into what’s happening when it executes. We’ll insert some output statements to keep track of the arguments and return values: \npublic static int triangle(int n) { System.out.println(“Entering: ${ mathsf { n } } = \" + { mathsf { n } } )$ ); if $scriptstyle mathtt { n = = 1 }$ ) ${$ System.out.println(“Returning 1”); return 1; } else { int temp $mathbf { zeta } = mathbf { zeta } mathsf { n beta } + mathbf { beta }$ triangle(n-1); System.out.println(“Returning “ $^ +$ temp); return temp; } } \nHere’s the interaction when this method is substituted for the earlier triangle() method and the user enters 5: \nEnter a number: 5 \nEntering: $n = 5$ Entering: $scriptstyle { mathsf { n } } = 4$ Entering: $scriptstyle { mathsf { n } } = 3$ Entering: $scriptstyle { mathsf { n } } = 2$ Entering: $mathsf { n } = 1$ Returning 1 Returning 3 Returning 6 \nReturning 10   \nReturning 15 \nTriangle $mathbf { tau } = mathbf { tau }$ 15 \nEach time the triangle() method calls itself, its argument, which starts at 5, is reduced by 1. The method plunges down into itself again and again until its argument is reduced to 1. Then it returns. This triggers an entire series of returns. The method rises back up, phoenix-like, out of the discarded versions of itself. Each time it returns, it adds the value of n it was called with to the return value from the method it called. \nThe return values recapitulate the series of triangular numbers, until the answer is returned to main(). Figure 6.4 shows how each invocation of the triangle() method can be imagined as being “inside” the previous one. \nNotice that, just before the innermost version returns a 1, there are actually five different incarnations of triangle() in existence at the same time. The outer one was passed the argument 5; the inner one was passed the argument 1. \nCharacteristics of Recursive Methods \nAlthough it’s short, the triangle() method possesses the key features common to all recursive routines: \n• It calls itself.   \n• When it calls itself, it does so to solve a smaller problem.   \n• There’s some version of the problem that is simple enough that the routine can solve it, and return, without calling itself. \nIn each successive call of a recursive method to itself, the argument becomes smaller (or perhaps a range described by multiple arguments becomes smaller), reflecting the fact that the problem has become “smaller” or easier. When the argument or range reaches a certain minimum size, a condition is triggered and the method returns without calling itself. \nIs Recursion Efficient? \nCalling a method involves certain overhead. Control must be transferred from the location of the call to the beginning of the method. In addition, the arguments to the method and the address to which the method should return must be pushed onto an internal stack so that the method can access the argument values and know where to return. \nIn the case of the triangle() method, it’s probable that, as a result of this overhead, the while loop approach executes more quickly than the recursive approach. The penalty may not be significant, but if there are a large number of method calls as a result of a recursive method, it might be desirable to eliminate the recursion. We’ll talk about this issue more at the end of this chapter. \nAnother inefficiency is that memory is used to store all the intermediate arguments and return values on the system’s internal stack. This may cause problems if there is a large amount of data, leading to stack overflow. \nRecursion is usually used because it simplifies a problem conceptually, not because it’s inherently more efficient. \nMathematical Induction \nRecursion is the programming equivalent of mathematical induction. Mathematical induction is a way of defining something in terms of itself. (The term is also used to \ndescribe a related approach to proving theorems.) Using induction, we could define the triangular numbers mathematically by saying \n$begin{array} { l } { { mathrm { t r i } ( mathrm { n } ) = 1 } }  { { mathrm { t r i } ( mathrm { n } ) = mathrm { n } + mathrm { t r i } ( mathrm { n } - 1 ) } } end{array}$ $begin{array} { c } { operatorname { i f } mathrm { n } = 1 }  { operatorname { i f } mathrm { n } > 1 } end{array}$ \nDefining something in terms of itself may seem circular, but in fact it’s perfectly valid (provided there’s a base case). \nFactorials \nFactorials are similar in concept to triangular numbers, except that multiplication is used instead of addition. The triangular number corresponding to n is found by adding n to the triangular number of $mathsf { n } - 1$ , while the factorial of n is found by multiplying n by the factorial of ${ n - 1 }$ . That is, the fifth triangular number is $5 + 4 + 3 + 2 + 1$ , while the factorial of 5 is $5 ^ { star } 4 ^ { star } 3 ^ { star } 2 ^ { star } 1$ , which equals 120. Table 6.1 shows the factorials of the first 10 numbers. \nThe factorial of 0 is defined to be 1. Factorial numbers grow large very rapidly, as you can see. \nA recursive method similar to triangle() can be used to calculate factorials. It looks like this: \nint factorial(int n) { if $scriptstyle mathsf { n = = } 0$ ) return 1; else return (n * factorial(n-1) ); }",
        "chapter": "CHAPTER 6 Recursion",
        "section": "Triangular Numbers",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 13,
        "node_index": 56
      }
    },
    {
      "node_id": "tb15_node33",
      "content": "### Building a heap\n\nThe procedure Build-Max-Heap converts an array \\(A[1:n]\\) into a max-heap by calling Max-Heapify in a bottom-up manner. Exercise 6.1-8 says that the elements in the subarray \\(A[\\lfloor n/2\\rfloor+1:n]\\) are all leaves of the tree, and so each is a 1-element heap to begin with. Build-Max-Heap goes through the remaining nodes of the tree and runs Max-Heapify on each one. Figure 6.3 shows an example of the action of Build-Max-Heap.\n\nBuild-Max-Heap(\\(A,n\\))\n\n\\(A.\\)_heap-size \\(=n\\)_\n\n\\(i\\ =\\lfloor n/2\\rfloor\\)**downto \\(1\\)**\n\n\\(\\\nFigure 6.3: The operation of Build-Max-Heap, showing the data structure before the call to Max-Heapify in line 3 of Build-Max-Heap. The node indexed by \\(i\\) in each iteration is shown in blue. **(a)** A 10-element input array \\(A\\) and the binary tree it represents. The loop index \\(i\\) refers to node 5 before the call Max-Heapify\\((A,i)\\). **(b)** The data structure that results. The loop index \\(i\\) for the next iteration refers to node 4. **(c)–(e)** Subsequent iterations of the **for** loop in Build-Max-Heap. Observe that whenever Max-Heapify is called on a node, the two subtrees of that node are both max-heaps. **(f)** The max-heap after Build-Max-Heap finishes.\n\n**Termination:**: The loop makes exactly \\(\\lfloor n/2\\rfloor\\) iterations, and so it terminates. At termination, \\(i=0\\). By the loop invariant, each node \\(1,2,\\ldots,n\\) is the root of a max-heap. In particular, node 1 is.\n\nWe can compute a simple upper bound on the running time of Build-Max-Heap as follows. Each call to Max-Heapify costs \\(O(\\lg n)\\) time, and Build-Max-Heap makes \\(O(n)\\) such calls. Thus, the running time is \\(O(n\\lg n)\\). This upper bound, though correct, is not as tight as it can be.\n\nWe can derive a tighter asymptotic bound by observing that the time for Max-Heapify to run at a node varies with the height of the node in the tree, and that the heights of most nodes are small. Our tighter analysis relies on the properties that an \\(n\\)-element heap has height \\(\\lfloor\\lg n\\rfloor\\) (see Exercise 6.1-2) and at most \\(\\left\\lceil n/2^{h+1}\\right\\rceil\\) nodes of any height \\(h\\) (see Exercise 6.3-4).\n\nThe time required by Max-Heapify when called on a node of height \\(h\\) is \\(O(h)\\). Letting \\(c\\) be the constant implicit in the asymptotic notation, we can express the total cost of Build-Max-Heap as being bounded from above by \\(\\sum_{h=0}^{\\lfloor\\lg n\\rfloor}\\left\\lceil n/2^{h+1}\\right\\rceil c\\,h\\). As Exercise 6.3-2 shows, we have \\(\\left\\lceil n/2^{h+1}\\right\\rceil\\geq 1/2\\) for \\(0\\leq h\\leq\\lfloor\\lg n\\rfloor\\). Since \\(\\left\\lceil x\\right\\rceil\\leq 2x\\) for any \\(x\\geq 1/2\\), we have \\(\\left\\lceil n/2^{h+1}\\right\\rceil\\leq n/2^{h}\\). We thus obtain\n\n\\[\\sum_{h=0}^{\\lfloor\\lg n\\rfloor}\\left\\lceil\\frac{n}{2^{h+1}} \\right\\rceil c\\,h\\] \\[\\leq\\,\\sum_{h=0}^{\\lfloor\\lg n\\rfloor}\\frac{n}{2^{h}}ch\\] \\[=\\,cn\\,\\sum_{h=0}^{\\lfloor\\lg n\\rfloor}\\frac{h}{2^{h}}\\] \\[\\leq\\,cn\\,\\sum_{h=0}^{\\infty}\\frac{h}{2^{h}}\\] \\[\\leq\\,cn\\cdot\\frac{1/2}{(1-1/2)^{2}}\\quad\\text{(by equation (A.11) on page \\ref{eq:H11} with $x=1/2$)}\\] \\[=\\,O(n)\\.\\]\n\nHence, we can build a max-heap from an unordered array in linear time.\n\nTo build a min-heap, use the procedure Build-Min-Heap, which is the same as Build-Max-Heap but with the call to Max-Heapify in line 3 replaced by a call to Min-Heapify (see Exercise 6.2-3). Build-Min-Heap produces a min-heap from an unordered linear array in linear time.\n\n### Exercises\n\n#### 6.3-1\n\nUsing Figure 6.3 as a model, illustrate the operation of Build-Max-Heap on the array \\(A=\\langle 5,3,17,10,84,19,6,22,9\\rangle\\).\n\n#### 6.3-2\n\nShow that \\(\\left\\lceil n/2^{h+1}\\right\\rceil\\geq 1/2\\) for \\(0\\leq h\\leq\\lfloor\\lg n\\rfloor\\).\n\n#### 6.3-3\n\nWhy does the loop index \\(i\\) in line 2 of Build-Max-Heap decrease from \\(\\lfloor n/2\\rfloor\\) to 1 rather than increase from 1 to \\(\\lfloor n/2\\rfloor\\)?\n\n#### 6.3-4\n\nShow that there are at most \\(\\left\\lceil n/2^{h+1}\\right\\rceil\\) nodes of height \\(h\\) in any \\(n\\)-element heap.\n\n### The heapsort algorithm\n\nThe heapsort algorithm, given by the procedure Heapsort, starts by calling the Build-Max-Heap procedure to build a max-heap on the input array \\(A[1\\!:\\!n]\\). Since the maximum element of the array is stored at the root \\(A[1]\\), Heapsort can place it into its correct final position by exchanging it with \\(A[n]\\). If the procedure then discards node \\(n\\) from the heap--and it can do so by simply decrementing \\(A\\)._heap-size_--the children of the root remain max-heaps, but the new root element might violate the max-heap property. To restore the max-heap property, the procedure just calls Max-Heapify\\((A,1)\\), which leaves a max-heap in \\(A[1\\!:\\!n-1]\\). The Heapsort procedure then repeats this process for the max-heap of size \\(n-1\\) down to a heap of size 2. (See Exercise 6.4-2 for a precise loop invariant.)\n\nHeapsort(\\(A,n\\))\n\nBuild-Max-Heap(\\(A,n\\))\n\n**for \\(i\\ =\\ n\\) downto 2**\n\nexchange \\(A[1]\\) with \\(A[i]\\)\n\n\\(A\\)._heap-size \\(=\\ A\\).heap-size \\(-1\\)_\n\nMax-Heapify(\\(A,1\\))\n\nFigure 6.4 shows an example of the operation of Heapsort after line 1 has built the initial max-heap. The figure shows the max-heap before the first iteration of the **for** loop of lines 2-5 and after each iteration.",
      "metadata": {
        "content": "### Building a heap\n\nThe procedure Build-Max-Heap converts an array \\(A[1:n]\\) into a max-heap by calling Max-Heapify in a bottom-up manner. Exercise 6.1-8 says that the elements in the subarray \\(A[\\lfloor n/2\\rfloor+1:n]\\) are all leaves of the tree, and so each is a 1-element heap to begin with. Build-Max-Heap goes through the remaining nodes of the tree and runs Max-Heapify on each one. Figure 6.3 shows an example of the action of Build-Max-Heap.\n\nBuild-Max-Heap(\\(A,n\\))\n\n\\(A.\\)_heap-size \\(=n\\)_\n\n\\(i\\ =\\lfloor n/2\\rfloor\\)**downto \\(1\\)**\n\n\\(\\\nFigure 6.3: The operation of Build-Max-Heap, showing the data structure before the call to Max-Heapify in line 3 of Build-Max-Heap. The node indexed by \\(i\\) in each iteration is shown in blue. **(a)** A 10-element input array \\(A\\) and the binary tree it represents. The loop index \\(i\\) refers to node 5 before the call Max-Heapify\\((A,i)\\). **(b)** The data structure that results. The loop index \\(i\\) for the next iteration refers to node 4. **(c)–(e)** Subsequent iterations of the **for** loop in Build-Max-Heap. Observe that whenever Max-Heapify is called on a node, the two subtrees of that node are both max-heaps. **(f)** The max-heap after Build-Max-Heap finishes.\n\n**Termination:**: The loop makes exactly \\(\\lfloor n/2\\rfloor\\) iterations, and so it terminates. At termination, \\(i=0\\). By the loop invariant, each node \\(1,2,\\ldots,n\\) is the root of a max-heap. In particular, node 1 is.\n\nWe can compute a simple upper bound on the running time of Build-Max-Heap as follows. Each call to Max-Heapify costs \\(O(\\lg n)\\) time, and Build-Max-Heap makes \\(O(n)\\) such calls. Thus, the running time is \\(O(n\\lg n)\\). This upper bound, though correct, is not as tight as it can be.\n\nWe can derive a tighter asymptotic bound by observing that the time for Max-Heapify to run at a node varies with the height of the node in the tree, and that the heights of most nodes are small. Our tighter analysis relies on the properties that an \\(n\\)-element heap has height \\(\\lfloor\\lg n\\rfloor\\) (see Exercise 6.1-2) and at most \\(\\left\\lceil n/2^{h+1}\\right\\rceil\\) nodes of any height \\(h\\) (see Exercise 6.3-4).\n\nThe time required by Max-Heapify when called on a node of height \\(h\\) is \\(O(h)\\). Letting \\(c\\) be the constant implicit in the asymptotic notation, we can express the total cost of Build-Max-Heap as being bounded from above by \\(\\sum_{h=0}^{\\lfloor\\lg n\\rfloor}\\left\\lceil n/2^{h+1}\\right\\rceil c\\,h\\). As Exercise 6.3-2 shows, we have \\(\\left\\lceil n/2^{h+1}\\right\\rceil\\geq 1/2\\) for \\(0\\leq h\\leq\\lfloor\\lg n\\rfloor\\). Since \\(\\left\\lceil x\\right\\rceil\\leq 2x\\) for any \\(x\\geq 1/2\\), we have \\(\\left\\lceil n/2^{h+1}\\right\\rceil\\leq n/2^{h}\\). We thus obtain\n\n\\[\\sum_{h=0}^{\\lfloor\\lg n\\rfloor}\\left\\lceil\\frac{n}{2^{h+1}} \\right\\rceil c\\,h\\] \\[\\leq\\,\\sum_{h=0}^{\\lfloor\\lg n\\rfloor}\\frac{n}{2^{h}}ch\\] \\[=\\,cn\\,\\sum_{h=0}^{\\lfloor\\lg n\\rfloor}\\frac{h}{2^{h}}\\] \\[\\leq\\,cn\\,\\sum_{h=0}^{\\infty}\\frac{h}{2^{h}}\\] \\[\\leq\\,cn\\cdot\\frac{1/2}{(1-1/2)^{2}}\\quad\\text{(by equation (A.11) on page \\ref{eq:H11} with $x=1/2$)}\\] \\[=\\,O(n)\\.\\]\n\nHence, we can build a max-heap from an unordered array in linear time.\n\nTo build a min-heap, use the procedure Build-Min-Heap, which is the same as Build-Max-Heap but with the call to Max-Heapify in line 3 replaced by a call to Min-Heapify (see Exercise 6.2-3). Build-Min-Heap produces a min-heap from an unordered linear array in linear time.\n\n### Exercises\n\n#### 6.3-1\n\nUsing Figure 6.3 as a model, illustrate the operation of Build-Max-Heap on the array \\(A=\\langle 5,3,17,10,84,19,6,22,9\\rangle\\).\n\n#### 6.3-2\n\nShow that \\(\\left\\lceil n/2^{h+1}\\right\\rceil\\geq 1/2\\) for \\(0\\leq h\\leq\\lfloor\\lg n\\rfloor\\).\n\n#### 6.3-3\n\nWhy does the loop index \\(i\\) in line 2 of Build-Max-Heap decrease from \\(\\lfloor n/2\\rfloor\\) to 1 rather than increase from 1 to \\(\\lfloor n/2\\rfloor\\)?\n\n#### 6.3-4\n\nShow that there are at most \\(\\left\\lceil n/2^{h+1}\\right\\rceil\\) nodes of height \\(h\\) in any \\(n\\)-element heap.\n\n### The heapsort algorithm\n\nThe heapsort algorithm, given by the procedure Heapsort, starts by calling the Build-Max-Heap procedure to build a max-heap on the input array \\(A[1\\!:\\!n]\\). Since the maximum element of the array is stored at the root \\(A[1]\\), Heapsort can place it into its correct final position by exchanging it with \\(A[n]\\). If the procedure then discards node \\(n\\) from the heap--and it can do so by simply decrementing \\(A\\)._heap-size_--the children of the root remain max-heaps, but the new root element might violate the max-heap property. To restore the max-heap property, the procedure just calls Max-Heapify\\((A,1)\\), which leaves a max-heap in \\(A[1\\!:\\!n-1]\\). The Heapsort procedure then repeats this process for the max-heap of size \\(n-1\\) down to a heap of size 2. (See Exercise 6.4-2 for a precise loop invariant.)\n\nHeapsort(\\(A,n\\))\n\nBuild-Max-Heap(\\(A,n\\))\n\n**for \\(i\\ =\\ n\\) downto 2**\n\nexchange \\(A[1]\\) with \\(A[i]\\)\n\n\\(A\\)._heap-size \\(=\\ A\\).heap-size \\(-1\\)_\n\nMax-Heapify(\\(A,1\\))\n\nFigure 6.4 shows an example of the operation of Heapsort after line 1 has built the initial max-heap. The figure shows the max-heap before the first iteration of the **for** loop of lines 2-5 and after each iteration.",
        "chapter": "Part II Sorting and Order Statistics",
        "section": "6 Heapsort",
        "subsection": "6.3 Building a heap",
        "subsubsection": "N/A",
        "textbook_id": 15,
        "node_index": 33
      }
    },
    {
      "node_id": "tb13_node64",
      "content": "$5 , 3$ ： r $4 , 2$ 4, 3 R 3, 1 3, 2 3, 2 3, 3 B C C ℃   \n2,  0 2, 1 2,  1 2, 2 2,  1 2, 2 2,  2 2, 3   \n1, –1 1, 0 1, 0 1, 1 1, 0 1, 1 1, 1 1, 2 1, 0 1, 1 1, 1 1, 2 1, 1 1, 2 1, 2 1, 3 3 3 3 0 8 \nThe recursion depth corresponds to the group members: The node on the top row represents group member A, the two nodes on the next row represent group member B, and so on. If there are 5 group members, you’ll have 5 levels. \nAs you descend the tree you need to remember the sequence of members you visit. Here’s how to do that: Whenever you make a call to a left term, you record the node you’re leaving by adding its letter to a sequence. These left calls and the letters to add to the sequence are shown by the darker lines in the figure. You’ll need to role the sequence back up as you return. \nTo record all the combinations, you can display them as you go along. You don’t display anything when making left calls. However, when you make calls to the right, you check the sequence; if you’re at a valid node, and adding one member will complete the team, then add the node to the sequence and display the complete team. \nSummary \n• A recursive method calls itself repeatedly, with different argument values each time.   \n• Some value of its arguments causes a recursive method to return without calling itself. This is called the base case.   \n• When the innermost instance of a recursive method returns, the process “unwinds” by completing pending instances of the method, going from the latest back to the original call.   \n• A triangular number is the sum of itself and all numbers smaller than itself. (Number means integer in this context.) For example, the triangular number of 4 is 10, because $4 { + } 3 { + } 2 { + } 1 = 1 0$ .   \n• The factorial of a number is the product of itself and all numbers smaller than itself. For example, the factorial of 4 is $4 ^ { star } 3 ^ { star } 2 ^ { star } 1 = 2 4$ .   \n• Both triangular numbers and factorials can be calculated using either a recursive method or a simple loop.   \n• The anagram of a word (all possible combinations of its n letters) can be found recursively by repeatedly rotating all its letters and anagramming the rightmost $mathsf { n } cdot 1$ of them.   \n• A binary search can be carried out recursively by checking which half of a sorted range the search key is in, and then doing the same thing with that half.   \n• The Towers of Hanoi puzzle consists of three towers and an arbitrary number of rings.   \n• The Towers of Hanoi puzzle can be solved recursively by moving all but the bottom disk of a subtree to an intermediate tower, moving the bottom disk to the destination tower, and finally moving the subtree to the destination.   \n• Merging two sorted arrays means to create a third array that contains all the elements from both arrays in sorted order.   \n• In mergesort, 1-element subarrays of a larger array are merged into 2-element subarrays, 2-element subarrays are merged into 4-element subarrays, and so on until the entire array is sorted.   \n• mergesort requires $mathrm { O ( N ^ { star } l o g N ) }$ time.   \n• mergesort requires a workspace equal in size to the original array.   \n• For triangular numbers, factorials, anagrams, and the binary search, the recursive method contains only one call to itself. (There are two shown in the code for the binary search, but only one is used on any given pass through the method’s code.)   \n• For the Towers of Hanoi and mergesort, the recursive method contains two calls to itself.   \n• Any operation that can be carried out with recursion can be carried out with a stack.   \n• A recursive approach may be inefficient. If so, it can sometimes be replaced with a simple loop or a stack-based approach. \n\nQuestions \nThese questions are intended as a self-test for readers. Answers may be found in Appendix C. \n1. If the user enters 10 in the triangle.java program (Listing 6.1), what is the maximum number of “copies” of the triangle() method (actually just copies of its argument) that exist at any one time? \n2. Where are the copies of the argument, mentioned in question 1, stored? \na. in a variable in the triangle() method \nb. in a field of the TriangleApp class c. in a variable of the getString() method d. on a stack   \n3. Assume the user enters 10 as in question 1. What is the value of n when the triangle() method first returns a value other than 1?   \n4. Assume the same situation as in question 1. What is the value of n when the triangle() method is about to return to main()?   \n5. True or false: In the triangle() method, the return values are stored on the stack.   \n6. In the anagram.java program (Listing 6.2), at a certain depth of recursion, a version of the doAnagram() method is working with the string “led”. When this method calls a new version of itself, what letters will the new version be working with?   \n7. We’ve seen that recursion can take the place of a loop, as in the loop-oriented orderedArray.java program (Listing 2.4) and the recursive binarySearch.java program (Listing 6.3). Which of the following is not true? a. Both programs divide the range repeatedly in half. b. If the key is not found, the loop version returns because the range bounds cross, but the recursive version occurs because it reaches the bottom recursion level. c. If the key is found, the loop version returns from the entire method, whereas the recursive version returns from only one level of recursion. d. In the recursive version the range to be searched must be specified in the arguments, while in the loop version it need not be.",
      "metadata": {
        "content": "$5 , 3$ ： r $4 , 2$ 4, 3 R 3, 1 3, 2 3, 2 3, 3 B C C ℃   \n2,  0 2, 1 2,  1 2, 2 2,  1 2, 2 2,  2 2, 3   \n1, –1 1, 0 1, 0 1, 1 1, 0 1, 1 1, 1 1, 2 1, 0 1, 1 1, 1 1, 2 1, 1 1, 2 1, 2 1, 3 3 3 3 0 8 \nThe recursion depth corresponds to the group members: The node on the top row represents group member A, the two nodes on the next row represent group member B, and so on. If there are 5 group members, you’ll have 5 levels. \nAs you descend the tree you need to remember the sequence of members you visit. Here’s how to do that: Whenever you make a call to a left term, you record the node you’re leaving by adding its letter to a sequence. These left calls and the letters to add to the sequence are shown by the darker lines in the figure. You’ll need to role the sequence back up as you return. \nTo record all the combinations, you can display them as you go along. You don’t display anything when making left calls. However, when you make calls to the right, you check the sequence; if you’re at a valid node, and adding one member will complete the team, then add the node to the sequence and display the complete team. \nSummary \n• A recursive method calls itself repeatedly, with different argument values each time.   \n• Some value of its arguments causes a recursive method to return without calling itself. This is called the base case.   \n• When the innermost instance of a recursive method returns, the process “unwinds” by completing pending instances of the method, going from the latest back to the original call.   \n• A triangular number is the sum of itself and all numbers smaller than itself. (Number means integer in this context.) For example, the triangular number of 4 is 10, because $4 { + } 3 { + } 2 { + } 1 = 1 0$ .   \n• The factorial of a number is the product of itself and all numbers smaller than itself. For example, the factorial of 4 is $4 ^ { star } 3 ^ { star } 2 ^ { star } 1 = 2 4$ .   \n• Both triangular numbers and factorials can be calculated using either a recursive method or a simple loop.   \n• The anagram of a word (all possible combinations of its n letters) can be found recursively by repeatedly rotating all its letters and anagramming the rightmost $mathsf { n } cdot 1$ of them.   \n• A binary search can be carried out recursively by checking which half of a sorted range the search key is in, and then doing the same thing with that half.   \n• The Towers of Hanoi puzzle consists of three towers and an arbitrary number of rings.   \n• The Towers of Hanoi puzzle can be solved recursively by moving all but the bottom disk of a subtree to an intermediate tower, moving the bottom disk to the destination tower, and finally moving the subtree to the destination.   \n• Merging two sorted arrays means to create a third array that contains all the elements from both arrays in sorted order.   \n• In mergesort, 1-element subarrays of a larger array are merged into 2-element subarrays, 2-element subarrays are merged into 4-element subarrays, and so on until the entire array is sorted.   \n• mergesort requires $mathrm { O ( N ^ { star } l o g N ) }$ time.   \n• mergesort requires a workspace equal in size to the original array.   \n• For triangular numbers, factorials, anagrams, and the binary search, the recursive method contains only one call to itself. (There are two shown in the code for the binary search, but only one is used on any given pass through the method’s code.)   \n• For the Towers of Hanoi and mergesort, the recursive method contains two calls to itself.   \n• Any operation that can be carried out with recursion can be carried out with a stack.   \n• A recursive approach may be inefficient. If so, it can sometimes be replaced with a simple loop or a stack-based approach. \n\nQuestions \nThese questions are intended as a self-test for readers. Answers may be found in Appendix C. \n1. If the user enters 10 in the triangle.java program (Listing 6.1), what is the maximum number of “copies” of the triangle() method (actually just copies of its argument) that exist at any one time? \n2. Where are the copies of the argument, mentioned in question 1, stored? \na. in a variable in the triangle() method \nb. in a field of the TriangleApp class c. in a variable of the getString() method d. on a stack   \n3. Assume the user enters 10 as in question 1. What is the value of n when the triangle() method first returns a value other than 1?   \n4. Assume the same situation as in question 1. What is the value of n when the triangle() method is about to return to main()?   \n5. True or false: In the triangle() method, the return values are stored on the stack.   \n6. In the anagram.java program (Listing 6.2), at a certain depth of recursion, a version of the doAnagram() method is working with the string “led”. When this method calls a new version of itself, what letters will the new version be working with?   \n7. We’ve seen that recursion can take the place of a loop, as in the loop-oriented orderedArray.java program (Listing 2.4) and the recursive binarySearch.java program (Listing 6.3). Which of the following is not true? a. Both programs divide the range repeatedly in half. b. If the key is not found, the loop version returns because the range bounds cross, but the recursive version occurs because it reaches the bottom recursion level. c. If the key is found, the loop version returns from the entire method, whereas the recursive version returns from only one level of recursion. d. In the recursive version the range to be searched must be specified in the arguments, while in the loop version it need not be.",
        "chapter": "CHAPTER 6 Recursion",
        "section": "Summary",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 13,
        "node_index": 64
      }
    },
    {
      "node_id": "tb13_node126",
      "content": "Programming Projects \nWriting programs to solve the Programming Projects helps to solidify your understanding of the material and demonstrates how the chapter’s concepts are applied. (As noted in the Introduction, qualified instructors may obtain completed solutions to the Programming Projects on the publisher’s Web site.) \n11.1 Modify the hash.java program (Listing 11.1) to use quadratic probing.   \n11.2 Implement a linear probe hash table that stores strings. You’ll need a hash function that converts a string to an index number; see the section “Hashing Strings” in this chapter. Assume the strings will be lowercase words, so 26 characters will suffice.   \n11.3 Write a hash function to implement a digit-folding approach in the hash function (as described in the “Hash Functions” section of this chapter). Your program should work for any array size and any key length. Use linear probing. Accessing a group of digits in a number may be easier than you think. Does it matter if the array size is not a multiple of 10?   \n11.4 Write a rehash() method for the hash.java program. It should be called by insert() to move the entire hash table to an array about twice as large whenever the load factor exceeds 0.5. The new array size should be a prime number. Refer to the section “Expanding the Array” in this chapter. Don’t forget you’ll need to handle items that have been “deleted,” that is, written over with $^ { - 1 }$ .   \n11.5 Instead of using a linked list to resolve collisions, as in separate chaining, use a binary search tree. That is, create a hash table that is an array of trees. You can use the hashChain.java program (Listing 11.3) as a starting point and the Tree class from the tree.java program (Listing 8.1) in Chapter 8. To display a small tree-based hash table, you could use an inorder traversal of each tree. The advantage of a tree over a linked list is that it can be searched in $mathrm { O } ( log mathrm { N } )$ \ninstead of $mathrm { O ( N ) }$ time. This time savings can be a significant advantage if very high load factors are encountered. Checking 15 items takes a maximum of 15 comparisons in a list but only 4 in a tree. \nDuplicates can present problems in both trees and hash tables, so add some code that prevents a duplicate key from being inserted in the hash table. (Beware: The find() method in Tree assumes a non-empty tree.) To shorten the listing for this program, you can forget about deletion, which for trees requires a lot of code. \n12 Heaps \nIN THIS CHAPTER \n• Introduction to Heaps • Java Code for Heaps • A Tree-based Heap • Heapsort \nWe saw in Chapter 4, “Stacks and Queues,” that a priority queue is a data structure that offers convenient access to the data item with the smallest (or largest) key. \nPriority queues may be used for task scheduling in computers, where some programs and activities should be executed sooner than others and are therefore given a higher priority. \nAnother example is in weapons systems, say in a navy cruiser. Numerous threats—airplanes, missiles, submarines, and so on—are detected and must be prioritized. For example, a missile that’s a short distance from the cruiser is assigned a higher priority than an aircraft a long distance away so that countermeasures (surface-to-air missiles, for example) can deal with it first. \nPriority queues are also used internally in other computer algorithms. In Chapter 14, “Weighted Graphs,” we’ll see priority queues used in graph algorithms, such as Dijkstra’s algorithm. \nA priority queue is an Abstract Data Type (ADT) offering methods that allow removal of the item with the maximum (or minimum) key value, insertion, and sometimes other operations. As with other ADTs, priority queues can be implemented using a variety of underlying structures. In Chapter 4 we saw a priority queue implemented as an ordered array. The trouble with that approach is that, even though removal of the largest item is accomplished in fast O(1) time, insertion requires slow O(N) time, because an average of half the items in the array must be moved to insert the new one in order.",
      "metadata": {
        "content": "Programming Projects \nWriting programs to solve the Programming Projects helps to solidify your understanding of the material and demonstrates how the chapter’s concepts are applied. (As noted in the Introduction, qualified instructors may obtain completed solutions to the Programming Projects on the publisher’s Web site.) \n11.1 Modify the hash.java program (Listing 11.1) to use quadratic probing.   \n11.2 Implement a linear probe hash table that stores strings. You’ll need a hash function that converts a string to an index number; see the section “Hashing Strings” in this chapter. Assume the strings will be lowercase words, so 26 characters will suffice.   \n11.3 Write a hash function to implement a digit-folding approach in the hash function (as described in the “Hash Functions” section of this chapter). Your program should work for any array size and any key length. Use linear probing. Accessing a group of digits in a number may be easier than you think. Does it matter if the array size is not a multiple of 10?   \n11.4 Write a rehash() method for the hash.java program. It should be called by insert() to move the entire hash table to an array about twice as large whenever the load factor exceeds 0.5. The new array size should be a prime number. Refer to the section “Expanding the Array” in this chapter. Don’t forget you’ll need to handle items that have been “deleted,” that is, written over with $^ { - 1 }$ .   \n11.5 Instead of using a linked list to resolve collisions, as in separate chaining, use a binary search tree. That is, create a hash table that is an array of trees. You can use the hashChain.java program (Listing 11.3) as a starting point and the Tree class from the tree.java program (Listing 8.1) in Chapter 8. To display a small tree-based hash table, you could use an inorder traversal of each tree. The advantage of a tree over a linked list is that it can be searched in $mathrm { O } ( log mathrm { N } )$ \ninstead of $mathrm { O ( N ) }$ time. This time savings can be a significant advantage if very high load factors are encountered. Checking 15 items takes a maximum of 15 comparisons in a list but only 4 in a tree. \nDuplicates can present problems in both trees and hash tables, so add some code that prevents a duplicate key from being inserted in the hash table. (Beware: The find() method in Tree assumes a non-empty tree.) To shorten the listing for this program, you can forget about deletion, which for trees requires a lot of code. \n12 Heaps \nIN THIS CHAPTER \n• Introduction to Heaps • Java Code for Heaps • A Tree-based Heap • Heapsort \nWe saw in Chapter 4, “Stacks and Queues,” that a priority queue is a data structure that offers convenient access to the data item with the smallest (or largest) key. \nPriority queues may be used for task scheduling in computers, where some programs and activities should be executed sooner than others and are therefore given a higher priority. \nAnother example is in weapons systems, say in a navy cruiser. Numerous threats—airplanes, missiles, submarines, and so on—are detected and must be prioritized. For example, a missile that’s a short distance from the cruiser is assigned a higher priority than an aircraft a long distance away so that countermeasures (surface-to-air missiles, for example) can deal with it first. \nPriority queues are also used internally in other computer algorithms. In Chapter 14, “Weighted Graphs,” we’ll see priority queues used in graph algorithms, such as Dijkstra’s algorithm. \nA priority queue is an Abstract Data Type (ADT) offering methods that allow removal of the item with the maximum (or minimum) key value, insertion, and sometimes other operations. As with other ADTs, priority queues can be implemented using a variety of underlying structures. In Chapter 4 we saw a priority queue implemented as an ordered array. The trouble with that approach is that, even though removal of the largest item is accomplished in fast O(1) time, insertion requires slow O(N) time, because an average of half the items in the array must be moved to insert the new one in order.",
        "chapter": "CHAPTER 11 Hash Tables",
        "section": "Programming Projects",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 13,
        "node_index": 126
      }
    },
    {
      "node_id": "tb1_node15",
      "content": "Proof: Let $A [ 1 ldots n ]$ be any array and $m$ any integer such that the subarrays $A [ 1 ldots m ]$ and $A [ m + 1 ldots n ]$ are sorted. We prove that for all $k$ from 0 to $n$ , the last $n - k - 1$ iterations of the main loop correctly merge $A [ i ldots m ]$ and $A [ j ldots n ]$ into $B [ k . . n ]$ . The proof proceeds by induction on $n - k + 1$ , the number of elements remaining to be merged. \nIf $k > n$ , the algorithm correctly merges the two empty subarrays by doing absolutely nothing. (This is the base case of the inductive proof.) Otherwise, there are four cases to consider for the kth iteration of the main loop. \n• If $j > n$ , then subarray $A [ j ldots n ]$ is empty, so $operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ i ]$ .   \n• If $i > m$ , then subarray $A [ i ldots m ]$ is empty, $operatorname { s o } operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ j ]$ .   \n• Otherwise, if $A [ i ] < A [ j ]$ , then $operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ i ]$ .   \n• Otherwise, we must have $A [ i ] geq A [ j ]$ , and $operatorname* { m i n } { big ( } A [ i ldots m ] cup A [ j ldots n ] { big ) } = A [ j ]$ . \nIn all four cases, $B [ k ]$ is correctly assigned the smallest element of $A [ i ldots m ] cup$ $A [ j ldots n ]$ . In the two cases with the assignment $B [ k ]  A [ i ]$ , the Recursion Fairy correctly merges—sorry, I mean the Induction Hypothesis implies that the last $n - k$ iterations of the main loop correctly merge $A [ i + 1 ldots m ]$ and $A [ j ldots n ]$ into $B [ k + 1 . . n ]$ . Similarly, in the other two cases, the Recursion Fairy also correctly merges the rest of the subarrays. □ \nTheorem 1.2. MergeSort correctly sorts any input array $A [ 1 ldots n ]$ . \nProof: We prove the theorem by induction on $n$ . If $n leq 1$ , the algorithm correctly does nothing. Otherwise, the Recursion Fairy correctly sorts—sorry, I mean the induction hypothesis implies that our algorithm correctly sorts the two smaller subarrays $A [ 1 ldots m ]$ and $A [ m + 1 ldots n ]$ , after which they are correctly Merged into a single sorted array (by Lemma 1.1). □ \nAnalysis \nBecause the MergeSort algorithm is recursive, its running time is naturally expressed as a recurrence. Merge clearly takes $O ( n )$ time, because it’s a simple for-loop with constant work per iteration. We immediately obtain the following recurrence for MergeSort: \nAs in most divide-and-conquer recurrences, we can safely strip out the floors and ceilings (using a technique called domain transformations described later in this chapter), giving us the simpler recurrence $T ( n ) = 2 T ( n / 2 ) + O ( n )$ . The “all levels equal” case of the recursion tree method (also described later in this chapter) immediately implies the closed-form solution $T ( n ) = O ( n log n )$ . Even if you are not (yet) familiar with recursion trees, you can verify the solution $T ( n ) = O ( n log n )$ by induction. \n1.5 Quicksort \nQuicksort is another recursive sorting algorithm, discovered by Tony Hoare in 1959 and first published in 1961. In this algorithm, the hard work is splitting the array into smaller subarrays before recursion, so that merging the sorted subarrays is trivial. \n1. Choose a pivot element from the array.   \n2. Partition the array into three subarrays containing the elements smaller than the pivot, the pivot element itself, and the elements larger than the pivot.   \n3. Recursively quicksort the first and last subarrays. Input: S O R T I N G E X A M P L   \nChoose a pivot: S O R T I N G E X A M P L Partition: A G O E I N L M P T X S R Recurse Left: A E G I L M N O P T X S R   \nRecurse Right: A E G I L M N O P R S T X \n\nMore detailed pseudocode is given in Figure 1.8. In the Partition subroutine, the input parameter $p$ is the index of the pivot element in the unsorted array; the subroutine partitions the array and returns the new index of the pivot element. There are many different efficient partitioning algorithms; the one I’m presenting here is attributed to Nico Lomuto.6 The variable $ell$ counts the number of items in the array that are ℓess than the pivot element. \nPartition(A[1 .. n], p): swap A[p] A[n] QuickSort(A[1 .. n]): ℓ 0 #items < pivot if $( n > 1 )$ 0 for $i gets 1$ to $n - 1$ Choose a pivot element A[p] if A[i] < A[n] r Partition(A, p) ℓ ℓ + 1 QuickSort(A[1 .. r 1]) Recurse! QuickSort(A[r + 1 .. n]) Recurse! swap $A [ ell ] longleftrightarrow A [ i ]$ $s mathrm { w a p } A [ n ] longleftrightarrow A [ ell + 1 ]$ return ℓ + 1 \nCorrectness \nJust like mergesort, proving that QuickSort is correct requires two separate induction proofs: one to prove that Partition correctly partitions the array, and the other to prove that QuickSort correctly sorts assuming Partition is correct. To prove Partition is correct, we need to prove the following loop invariant: At the end of each iteration of the main loop, everything in the subarray $A [ 1 ldots ell ]$ is ℓess than $A [ n ]$ , and nothing in the subarray $A [ ell + 1 ldots i ]$ is less than $boldsymbol { A } [ n ]$ . I’ll leave the remaining straightforward but tedious details as exercises for the reader.",
      "metadata": {
        "content": "Proof: Let $A [ 1 ldots n ]$ be any array and $m$ any integer such that the subarrays $A [ 1 ldots m ]$ and $A [ m + 1 ldots n ]$ are sorted. We prove that for all $k$ from 0 to $n$ , the last $n - k - 1$ iterations of the main loop correctly merge $A [ i ldots m ]$ and $A [ j ldots n ]$ into $B [ k . . n ]$ . The proof proceeds by induction on $n - k + 1$ , the number of elements remaining to be merged. \nIf $k > n$ , the algorithm correctly merges the two empty subarrays by doing absolutely nothing. (This is the base case of the inductive proof.) Otherwise, there are four cases to consider for the kth iteration of the main loop. \n• If $j > n$ , then subarray $A [ j ldots n ]$ is empty, so $operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ i ]$ .   \n• If $i > m$ , then subarray $A [ i ldots m ]$ is empty, $operatorname { s o } operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ j ]$ .   \n• Otherwise, if $A [ i ] < A [ j ]$ , then $operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ i ]$ .   \n• Otherwise, we must have $A [ i ] geq A [ j ]$ , and $operatorname* { m i n } { big ( } A [ i ldots m ] cup A [ j ldots n ] { big ) } = A [ j ]$ . \nIn all four cases, $B [ k ]$ is correctly assigned the smallest element of $A [ i ldots m ] cup$ $A [ j ldots n ]$ . In the two cases with the assignment $B [ k ]  A [ i ]$ , the Recursion Fairy correctly merges—sorry, I mean the Induction Hypothesis implies that the last $n - k$ iterations of the main loop correctly merge $A [ i + 1 ldots m ]$ and $A [ j ldots n ]$ into $B [ k + 1 . . n ]$ . Similarly, in the other two cases, the Recursion Fairy also correctly merges the rest of the subarrays. □ \nTheorem 1.2. MergeSort correctly sorts any input array $A [ 1 ldots n ]$ . \nProof: We prove the theorem by induction on $n$ . If $n leq 1$ , the algorithm correctly does nothing. Otherwise, the Recursion Fairy correctly sorts—sorry, I mean the induction hypothesis implies that our algorithm correctly sorts the two smaller subarrays $A [ 1 ldots m ]$ and $A [ m + 1 ldots n ]$ , after which they are correctly Merged into a single sorted array (by Lemma 1.1). □ \nAnalysis \nBecause the MergeSort algorithm is recursive, its running time is naturally expressed as a recurrence. Merge clearly takes $O ( n )$ time, because it’s a simple for-loop with constant work per iteration. We immediately obtain the following recurrence for MergeSort: \nAs in most divide-and-conquer recurrences, we can safely strip out the floors and ceilings (using a technique called domain transformations described later in this chapter), giving us the simpler recurrence $T ( n ) = 2 T ( n / 2 ) + O ( n )$ . The “all levels equal” case of the recursion tree method (also described later in this chapter) immediately implies the closed-form solution $T ( n ) = O ( n log n )$ . Even if you are not (yet) familiar with recursion trees, you can verify the solution $T ( n ) = O ( n log n )$ by induction. \n1.5 Quicksort \nQuicksort is another recursive sorting algorithm, discovered by Tony Hoare in 1959 and first published in 1961. In this algorithm, the hard work is splitting the array into smaller subarrays before recursion, so that merging the sorted subarrays is trivial. \n1. Choose a pivot element from the array.   \n2. Partition the array into three subarrays containing the elements smaller than the pivot, the pivot element itself, and the elements larger than the pivot.   \n3. Recursively quicksort the first and last subarrays. Input: S O R T I N G E X A M P L   \nChoose a pivot: S O R T I N G E X A M P L Partition: A G O E I N L M P T X S R Recurse Left: A E G I L M N O P T X S R   \nRecurse Right: A E G I L M N O P R S T X \n\nMore detailed pseudocode is given in Figure 1.8. In the Partition subroutine, the input parameter $p$ is the index of the pivot element in the unsorted array; the subroutine partitions the array and returns the new index of the pivot element. There are many different efficient partitioning algorithms; the one I’m presenting here is attributed to Nico Lomuto.6 The variable $ell$ counts the number of items in the array that are ℓess than the pivot element. \nPartition(A[1 .. n], p): swap A[p] A[n] QuickSort(A[1 .. n]): ℓ 0 #items < pivot if $( n > 1 )$ 0 for $i gets 1$ to $n - 1$ Choose a pivot element A[p] if A[i] < A[n] r Partition(A, p) ℓ ℓ + 1 QuickSort(A[1 .. r 1]) Recurse! QuickSort(A[r + 1 .. n]) Recurse! swap $A [ ell ] longleftrightarrow A [ i ]$ $s mathrm { w a p } A [ n ] longleftrightarrow A [ ell + 1 ]$ return ℓ + 1 \nCorrectness \nJust like mergesort, proving that QuickSort is correct requires two separate induction proofs: one to prove that Partition correctly partitions the array, and the other to prove that QuickSort correctly sorts assuming Partition is correct. To prove Partition is correct, we need to prove the following loop invariant: At the end of each iteration of the main loop, everything in the subarray $A [ 1 ldots ell ]$ is ℓess than $A [ n ]$ , and nothing in the subarray $A [ ell + 1 ldots i ]$ is less than $boldsymbol { A } [ n ]$ . I’ll leave the remaining straightforward but tedious details as exercises for the reader.",
        "chapter": "Recursion",
        "section": "Mergesort",
        "subsection": "Analysis",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 15
      }
    },
    {
      "node_id": "tb1_node17",
      "content": "Analysis \nThe analysis of quicksort is also similar to that of mergesort. Partition clearly runs in $O ( n )$ time, because it’s a simple for-loop with constant work per iteration. For QuickSort, we get a recurrence that depends on $r$ , the rank of the chosen pivot element: \nIf we could somehow always magically choose the pivot to be the median element of the array $A$ , we would have $r = lceil n / 2 rceil$ , the two subproblems would be as close to the same size as possible, the recurrence would become \nand we’d have $T ( n ) = O ( n log n )$ using either the recursion tree method or the even simpler “Oh yeah, we already solved that recurrence for mergesort” method. \nIn fact, as we will see later in this chapter, we can actually locate the median element in an unsorted array in linear time, but the algorithm is fairly complicated, and the hidden constant in the $O ( cdot )$ notation is large enough to make the resulting sorting algorithm impractical. In practice, most programmers settle for something simple, like choosing the first or last element of the array. In this case, $r$ can take any value between 1 and $n$ , so we have \nIn the worst case, the two subproblems are completely unbalanced—either $r = 1$ or $r = n$ —and the recurrence becomes $T ( n ) leq T ( n - 1 ) + O ( n )$ . The solution is $T ( n ) = O ( n ^ { 2 } )$ . \nAnother common heuristic is called “median of three”—choose three elements (usually at the beginning, middle, and end of the array), and take the median of those three elements as the pivot. Although this heuristic is somewhat more efficient in practice than just choosing one element, especially when the array is already (nearly) sorted, we can still have $r = 2$ or $r = n - 1$ in the worst case. With the median-of-three heuristic, the recurrence becomes $T ( n ) leq T ( 1 ) + T ( n - 2 ) + O ( n )$ , whose solution is still $T ( n ) = O ( n ^ { 2 } )$ . \nIntuitively, the pivot element should “usually” fall somewhere in the middle of the array, say with rank between $n / 1 0$ and $9 n / 1 0$ . This observation suggests that the “average-case” running time should be $O ( n log { n } )$ . Although this intuition can be formalized, the most common formalization makes the completely unrealistic assumption that all permutations of the input array are equally likely. Real world data may be random, but it is not random in any way that we can predict in advance, and it is certainly not uniform!7 \nOccasionally people also consider “best case” running time for some reason. We won’t. \n1.6 The Pattern \nBoth mergesort and quicksort follow a general three-step pattern called divide and conquer: \n1. Divide the given instance of the problem into several independent smaller instances of exactly the same problem.   \n2. Delegate each smaller instance to the Recursion Fairy.   \n3. Combine the solutions for the smaller instances into the final solution for the given instance. \nIf the size of any instance falls below some constant threshold, we abandon recursion and solve the problem directly, by brute force, in constant time. \nProving a divide-and-conquer algorithm correct almost always requires induction. Analyzing the running time requires setting up and solving a recurrence, which usually (but unfortunately not always!) can be solved using recursion trees. \n1.7 Recursion Trees \nSo what are these “recursion trees” I keep talking about? Recursion trees are a simple, general, pictorial tool for solving divide-and-conquer recurrences. A recursion tree is a rooted tree with one node for each recursive subproblem. The value of each node is the amount of time spent on the corresponding subproblem excluding recursive calls. Thus, the overall running time of the algorithm is the sum of the values of all nodes in the tree. \nTo make this idea more concrete, imagine a divide-and-conquer algorithm that spends $O ( f ( n ) )$ time on non-recursive work, and then makes $r$ recursive calls, each on a problem of size $n / c$ . Up to constant factors (which we can hide in the $O ( )$ notation), the running time of this algorithm is governed by the recurrence",
      "metadata": {
        "content": "Analysis \nThe analysis of quicksort is also similar to that of mergesort. Partition clearly runs in $O ( n )$ time, because it’s a simple for-loop with constant work per iteration. For QuickSort, we get a recurrence that depends on $r$ , the rank of the chosen pivot element: \nIf we could somehow always magically choose the pivot to be the median element of the array $A$ , we would have $r = lceil n / 2 rceil$ , the two subproblems would be as close to the same size as possible, the recurrence would become \nand we’d have $T ( n ) = O ( n log n )$ using either the recursion tree method or the even simpler “Oh yeah, we already solved that recurrence for mergesort” method. \nIn fact, as we will see later in this chapter, we can actually locate the median element in an unsorted array in linear time, but the algorithm is fairly complicated, and the hidden constant in the $O ( cdot )$ notation is large enough to make the resulting sorting algorithm impractical. In practice, most programmers settle for something simple, like choosing the first or last element of the array. In this case, $r$ can take any value between 1 and $n$ , so we have \nIn the worst case, the two subproblems are completely unbalanced—either $r = 1$ or $r = n$ —and the recurrence becomes $T ( n ) leq T ( n - 1 ) + O ( n )$ . The solution is $T ( n ) = O ( n ^ { 2 } )$ . \nAnother common heuristic is called “median of three”—choose three elements (usually at the beginning, middle, and end of the array), and take the median of those three elements as the pivot. Although this heuristic is somewhat more efficient in practice than just choosing one element, especially when the array is already (nearly) sorted, we can still have $r = 2$ or $r = n - 1$ in the worst case. With the median-of-three heuristic, the recurrence becomes $T ( n ) leq T ( 1 ) + T ( n - 2 ) + O ( n )$ , whose solution is still $T ( n ) = O ( n ^ { 2 } )$ . \nIntuitively, the pivot element should “usually” fall somewhere in the middle of the array, say with rank between $n / 1 0$ and $9 n / 1 0$ . This observation suggests that the “average-case” running time should be $O ( n log { n } )$ . Although this intuition can be formalized, the most common formalization makes the completely unrealistic assumption that all permutations of the input array are equally likely. Real world data may be random, but it is not random in any way that we can predict in advance, and it is certainly not uniform!7 \nOccasionally people also consider “best case” running time for some reason. We won’t. \n1.6 The Pattern \nBoth mergesort and quicksort follow a general three-step pattern called divide and conquer: \n1. Divide the given instance of the problem into several independent smaller instances of exactly the same problem.   \n2. Delegate each smaller instance to the Recursion Fairy.   \n3. Combine the solutions for the smaller instances into the final solution for the given instance. \nIf the size of any instance falls below some constant threshold, we abandon recursion and solve the problem directly, by brute force, in constant time. \nProving a divide-and-conquer algorithm correct almost always requires induction. Analyzing the running time requires setting up and solving a recurrence, which usually (but unfortunately not always!) can be solved using recursion trees. \n1.7 Recursion Trees \nSo what are these “recursion trees” I keep talking about? Recursion trees are a simple, general, pictorial tool for solving divide-and-conquer recurrences. A recursion tree is a rooted tree with one node for each recursive subproblem. The value of each node is the amount of time spent on the corresponding subproblem excluding recursive calls. Thus, the overall running time of the algorithm is the sum of the values of all nodes in the tree. \nTo make this idea more concrete, imagine a divide-and-conquer algorithm that spends $O ( f ( n ) )$ time on non-recursive work, and then makes $r$ recursive calls, each on a problem of size $n / c$ . Up to constant factors (which we can hide in the $O ( )$ notation), the running time of this algorithm is governed by the recurrence",
        "chapter": "Recursion",
        "section": "Quicksort",
        "subsection": "Analysis",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 17
      }
    },
    {
      "node_id": "tb13_node67",
      "content": "Experiments \nCarrying out these experiments will help to provide insights into the topics covered in the chapter. No programming is involved. \n1. In the triangle.java program (Listing 6.1), remove the code for the base case (the if $scriptstyle left[  n = = 1 right]$ ), the return 1;, and the else). Then run the program and see what happens.   \n2. Use the Towers Workshop applet in manual mode to solve the puzzle with seven or more disks.   \n3. Rewrite the main() part of mergeSort.java (Listing 6.6) so you can fill the array with hundreds of thousands of random numbers. Run the program to sort these numbers and compare its speed with the sorts in Chapter 3, “Simple Sorting.” \nProgramming Projects \nWriting programs that solve the Programming Projects helps to solidify your understanding of the material and demonstrates how the chapter’s concepts are applied. (As noted in the Introduction, qualified instructors may obtain completed solutions to the Programming Projects on the publisher’s Web site.) \n6.1 Suppose you buy a budget-priced pocket PC and discover that the chip inside can’t do multiplication, only addition. You program your way out of this quandary by writing a recursive method, mult(), that performs multiplication of x and y by adding x to itself y times. Its arguments are x and y and its return value is the product of x and y. Write such a method and a main() program to call it. Does the addition take place when the method calls itself or when it returns? \n6.2 In Chapter 8, “Binary Trees,” we’ll look at binary trees, where every branch has (potentially) exactly two sub-branches. If we draw a binary tree on the screen using characters, we might have 1 branch on the top row, 2 on the next row, then 4, 8, 16, and so on. Here’s what that looks like for a tree 16 characters wide: \n----X---- ---X-----X---X---X---X--X-X-X-X-X-X-X-XXXXXXXXXXXXXXXXX\n(Note that the bottom line should be shifted a half character-width right, but there’s nothing we can do about that with character-mode graphics.) You can draw this tree using a recursive makeBranches() method with arguments left and right, which are the endpoints of a horizontal range. When you first enter the routine, left is 0 and right is the number of characters (including dashes) in all the lines, minus 1. You draw an X in the center of this range. Then the method calls itself twice: once for the left half of the range and once for the right half. Return when the range gets too small. You will probably want to put all the dashes and Xs into an array and display the array all at once, perhaps with a display() method. Write a main() program to draw the tree by calling makeBranches() and display(). Allow main() to determine the line length of the display (32, 64, or whatever). Ensure that the array that holds the characters for display is no larger than it needs to be. What is the relationship of the number of lines (five in the picture here) to the line width? \n6.3 Implement the recursive approach to raising a number to a power, as described in the “Raising a Number to a Power” section near the end of this chapter. Write the recursive power() function and a main() routine to test it. \n6.4 Write a program that solves the knapsack problem for an arbitrary knapsack capacity and series of weights. Assume the weights are stored in an array. Hint: The arguments to the recursive knapsack() function are the target weight and the array index where the remaining items start. \n6.5 Implement a recursive approach to showing all the teams that can be created from a group (n things taken k at a time). Write the recursive showTeams() method and a main() method to prompt the user for the group size and the team size to provide arguments for showTeam(), which then displays all the possible combinations. \n7 Advanced Sorting \nWe discussed simple sorting in the aptly titled Chapter 3, “Simple Sorting.” The sorts described there—the bubble, selection, and insertion sorts—are easy to implement but are rather slow. In Chapter 6, “Recursion,” we described the mergesort. It runs much faster than the simple sorts but requires twice as much space as the original array; this is often a serious drawback. \nThis chapter covers two advanced approaches to sorting: Shellsort and quicksort. These sorts both operate much faster than the simple sorts: the Shellsort in about $mathrm { O } ( mathrm { N } ^ { star } ( log mathrm { N } ) ^ { 2 } )$ time, and quicksort in $mathrm { O ( N ^ { star } l o g N ) }$ time. Neither of these sorts requires a large amount of extra space, as mergesort does. The Shellsort is almost as easy to implement as mergesort, while quicksort is the fastest of all the general-purpose sorts. We’ll conclude the chapter with a brief mention of the radix sort, an unusual and interesting approach to sorting. \nWe’ll examine the Shellsort first. Quicksort is based on the idea of partitioning, so we’ll then examine partitioning separately, before examining quicksort itself. \nShellsort \nThe Shellsort is named for Donald L. Shell, the computer scientist who discovered it in 1959. It’s based on the insertion sort, but adds a new feature that dramatically improves the insertion sort’s performance. \nThe Shellsort is good for medium-sized arrays, perhaps up to a few thousand items, depending on the particular implementation. It’s not quite as fast as quicksort and other $mathrm { O ( N ^ { star } l o g N ) }$ sorts, so it’s not optimum for very large files. However, it’s much faster than the $mathrm { O } ( mathrm { N } ^ { 2 } )$ sorts like the selection sort and the insertion sort, and it’s very easy to implement: The code is short and simple. \nIN THIS CHAPTER \n• Shellsort • Partitioning • Quicksort • Radix Sort",
      "metadata": {
        "content": "Experiments \nCarrying out these experiments will help to provide insights into the topics covered in the chapter. No programming is involved. \n1. In the triangle.java program (Listing 6.1), remove the code for the base case (the if $scriptstyle left[  n = = 1 right]$ ), the return 1;, and the else). Then run the program and see what happens.   \n2. Use the Towers Workshop applet in manual mode to solve the puzzle with seven or more disks.   \n3. Rewrite the main() part of mergeSort.java (Listing 6.6) so you can fill the array with hundreds of thousands of random numbers. Run the program to sort these numbers and compare its speed with the sorts in Chapter 3, “Simple Sorting.” \nProgramming Projects \nWriting programs that solve the Programming Projects helps to solidify your understanding of the material and demonstrates how the chapter’s concepts are applied. (As noted in the Introduction, qualified instructors may obtain completed solutions to the Programming Projects on the publisher’s Web site.) \n6.1 Suppose you buy a budget-priced pocket PC and discover that the chip inside can’t do multiplication, only addition. You program your way out of this quandary by writing a recursive method, mult(), that performs multiplication of x and y by adding x to itself y times. Its arguments are x and y and its return value is the product of x and y. Write such a method and a main() program to call it. Does the addition take place when the method calls itself or when it returns? \n6.2 In Chapter 8, “Binary Trees,” we’ll look at binary trees, where every branch has (potentially) exactly two sub-branches. If we draw a binary tree on the screen using characters, we might have 1 branch on the top row, 2 on the next row, then 4, 8, 16, and so on. Here’s what that looks like for a tree 16 characters wide: \n----X---- ---X-----X---X---X---X--X-X-X-X-X-X-X-XXXXXXXXXXXXXXXXX\n(Note that the bottom line should be shifted a half character-width right, but there’s nothing we can do about that with character-mode graphics.) You can draw this tree using a recursive makeBranches() method with arguments left and right, which are the endpoints of a horizontal range. When you first enter the routine, left is 0 and right is the number of characters (including dashes) in all the lines, minus 1. You draw an X in the center of this range. Then the method calls itself twice: once for the left half of the range and once for the right half. Return when the range gets too small. You will probably want to put all the dashes and Xs into an array and display the array all at once, perhaps with a display() method. Write a main() program to draw the tree by calling makeBranches() and display(). Allow main() to determine the line length of the display (32, 64, or whatever). Ensure that the array that holds the characters for display is no larger than it needs to be. What is the relationship of the number of lines (five in the picture here) to the line width? \n6.3 Implement the recursive approach to raising a number to a power, as described in the “Raising a Number to a Power” section near the end of this chapter. Write the recursive power() function and a main() routine to test it. \n6.4 Write a program that solves the knapsack problem for an arbitrary knapsack capacity and series of weights. Assume the weights are stored in an array. Hint: The arguments to the recursive knapsack() function are the target weight and the array index where the remaining items start. \n6.5 Implement a recursive approach to showing all the teams that can be created from a group (n things taken k at a time). Write the recursive showTeams() method and a main() method to prompt the user for the group size and the team size to provide arguments for showTeam(), which then displays all the possible combinations. \n7 Advanced Sorting \nWe discussed simple sorting in the aptly titled Chapter 3, “Simple Sorting.” The sorts described there—the bubble, selection, and insertion sorts—are easy to implement but are rather slow. In Chapter 6, “Recursion,” we described the mergesort. It runs much faster than the simple sorts but requires twice as much space as the original array; this is often a serious drawback. \nThis chapter covers two advanced approaches to sorting: Shellsort and quicksort. These sorts both operate much faster than the simple sorts: the Shellsort in about $mathrm { O } ( mathrm { N } ^ { star } ( log mathrm { N } ) ^ { 2 } )$ time, and quicksort in $mathrm { O ( N ^ { star } l o g N ) }$ time. Neither of these sorts requires a large amount of extra space, as mergesort does. The Shellsort is almost as easy to implement as mergesort, while quicksort is the fastest of all the general-purpose sorts. We’ll conclude the chapter with a brief mention of the radix sort, an unusual and interesting approach to sorting. \nWe’ll examine the Shellsort first. Quicksort is based on the idea of partitioning, so we’ll then examine partitioning separately, before examining quicksort itself. \nShellsort \nThe Shellsort is named for Donald L. Shell, the computer scientist who discovered it in 1959. It’s based on the insertion sort, but adds a new feature that dramatically improves the insertion sort’s performance. \nThe Shellsort is good for medium-sized arrays, perhaps up to a few thousand items, depending on the particular implementation. It’s not quite as fast as quicksort and other $mathrm { O ( N ^ { star } l o g N ) }$ sorts, so it’s not optimum for very large files. However, it’s much faster than the $mathrm { O } ( mathrm { N } ^ { 2 } )$ sorts like the selection sort and the insertion sort, and it’s very easy to implement: The code is short and simple. \nIN THIS CHAPTER \n• Shellsort • Partitioning • Quicksort • Radix Sort",
        "chapter": "CHAPTER 6 Recursion",
        "section": "Programming Projects",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 13,
        "node_index": 67
      }
    },
    {
      "node_id": "tb13_node156",
      "content": "Use an array or a double-ended linked list if insertion speed is not a problem. The array works when the amount of data to be stored can be predicted in advance; the linked list when the amount of data is unknown. If speed is important, a heap is a better choice. \nComparison of Special-Purpose Structures \nTable 15.2 shows the Big O times for stacks, queues, and priority queues. These structures don’t support searching or traversal. \nSorting \nAs with the choice of data structures, it’s worthwhile initially to try a slow but simple sort, such as the insertion sort. It may be that the fast processing speeds available in modern computers will allow sorting of your data in reasonable time. (As a wild guess, the slow sort might be appropriate for fewer than 1,000 items.) \nInsertion sort is also good for almost-sorted files, operating in about O(N) time if not too many items are out of place. This is typically the case where a few new items are added to an already-sorted file. \nIf the insertion sort proves too slow, then the Shellsort is the next candidate. It’s fairly easy to implement, and not very temperamental. Sedgewick estimates it to be useful up to 5,000 items. \nOnly when the Shellsort proves too slow should you use one of the more complex but faster sorts: mergesort, heapsort, or quicksort. Mergesort requires extra memory, heapsort requires a heap data structure, and both are somewhat slower than quicksort, so quicksort is the usual choice when the fastest sorting time is necessary. \nHowever, quicksort is suspect if there’s a danger that the data may not be random, in which case it may deteriorate to $mathrm { O } ( mathrm { N } ^ { 2 } )$ performance. For potentially non-random \ndata, heapsort is better. Quicksort is also prone to subtle errors if it is not implemented correctly. Small mistakes in coding can make it work poorly for certain arrangements of data, a situation that may be hard to diagnose. \nTable 15.3 summarizes the running time for various sorting algorithms. The column labeled Comparison attempts to estimate the minor speed differences between algorithms with the same average Big O times. (There’s no entry for Shellsort because there are no other algorithms with the same Big O performance.) \nGraphs \nGraphs are unique in the pantheon of data storage structures. They don’t store general-purpose data, and they don’t act as programmer’s tools for use in other algorithms. Instead, they directly model real-world situations. The structure of the graph reflects the structure of the problem. \nWhen you need a graph, nothing else will do, so there’s no decision to be made about when to use one. The primary choice is how to represent the graph: using an adjacency matrix or adjacency lists. Your choice depends on whether the graph is full, when the adjacency matrix is preferred, or sparse, when the adjacency list should be used. \nThe depth-first search and breadth-first search run in $mathrm { O } ( mathrm { V } ^ { 2 } )$ time, where $mathrm { v }$ is the number of vertices, for adjacency matrix representation. They run in $mathrm { { O } ( V + E ) }$ time, where E is the number of edges, for adjacency list representation. Minimum spanning trees and shortest paths run in $mathrm { O } ( mathrm { V } ^ { 2 } )$ time using an adjacency matrix and $mathrm { O } ( ( mathrm { E } { + } mathrm { V } ) mathrm { l o g } mathrm { V } )$ time using adjacency lists. You’ll need to estimate $mathrm { Delta V }$ and E for your graph and do the arithmetic to see which representation is appropriate. \nExternal Storage \nIn the previous discussion we assumed that data was stored in main memory. However, amounts of data too large to store in memory must be stored in externa storage, which generally means disk files. We discussed external storage in the second parts of Chapter 10, “2-3-4 Trees and External Storage,” and Chapter 11, “Hash Tables.”",
      "metadata": {
        "content": "Use an array or a double-ended linked list if insertion speed is not a problem. The array works when the amount of data to be stored can be predicted in advance; the linked list when the amount of data is unknown. If speed is important, a heap is a better choice. \nComparison of Special-Purpose Structures \nTable 15.2 shows the Big O times for stacks, queues, and priority queues. These structures don’t support searching or traversal. \nSorting \nAs with the choice of data structures, it’s worthwhile initially to try a slow but simple sort, such as the insertion sort. It may be that the fast processing speeds available in modern computers will allow sorting of your data in reasonable time. (As a wild guess, the slow sort might be appropriate for fewer than 1,000 items.) \nInsertion sort is also good for almost-sorted files, operating in about O(N) time if not too many items are out of place. This is typically the case where a few new items are added to an already-sorted file. \nIf the insertion sort proves too slow, then the Shellsort is the next candidate. It’s fairly easy to implement, and not very temperamental. Sedgewick estimates it to be useful up to 5,000 items. \nOnly when the Shellsort proves too slow should you use one of the more complex but faster sorts: mergesort, heapsort, or quicksort. Mergesort requires extra memory, heapsort requires a heap data structure, and both are somewhat slower than quicksort, so quicksort is the usual choice when the fastest sorting time is necessary. \nHowever, quicksort is suspect if there’s a danger that the data may not be random, in which case it may deteriorate to $mathrm { O } ( mathrm { N } ^ { 2 } )$ performance. For potentially non-random \ndata, heapsort is better. Quicksort is also prone to subtle errors if it is not implemented correctly. Small mistakes in coding can make it work poorly for certain arrangements of data, a situation that may be hard to diagnose. \nTable 15.3 summarizes the running time for various sorting algorithms. The column labeled Comparison attempts to estimate the minor speed differences between algorithms with the same average Big O times. (There’s no entry for Shellsort because there are no other algorithms with the same Big O performance.) \nGraphs \nGraphs are unique in the pantheon of data storage structures. They don’t store general-purpose data, and they don’t act as programmer’s tools for use in other algorithms. Instead, they directly model real-world situations. The structure of the graph reflects the structure of the problem. \nWhen you need a graph, nothing else will do, so there’s no decision to be made about when to use one. The primary choice is how to represent the graph: using an adjacency matrix or adjacency lists. Your choice depends on whether the graph is full, when the adjacency matrix is preferred, or sparse, when the adjacency list should be used. \nThe depth-first search and breadth-first search run in $mathrm { O } ( mathrm { V } ^ { 2 } )$ time, where $mathrm { v }$ is the number of vertices, for adjacency matrix representation. They run in $mathrm { { O } ( V + E ) }$ time, where E is the number of edges, for adjacency list representation. Minimum spanning trees and shortest paths run in $mathrm { O } ( mathrm { V } ^ { 2 } )$ time using an adjacency matrix and $mathrm { O } ( ( mathrm { E } { + } mathrm { V } ) mathrm { l o g } mathrm { V } )$ time using adjacency lists. You’ll need to estimate $mathrm { Delta V }$ and E for your graph and do the arithmetic to see which representation is appropriate. \nExternal Storage \nIn the previous discussion we assumed that data was stored in main memory. However, amounts of data too large to store in memory must be stored in externa storage, which generally means disk files. We discussed external storage in the second parts of Chapter 10, “2-3-4 Trees and External Storage,” and Chapter 11, “Hash Tables.”",
        "chapter": "CHAPTER 15 When to Use What",
        "section": "Sorting",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 13,
        "node_index": 156
      }
    },
    {
      "node_id": "tb13_node55",
      "content": "15. Which do you think would be a better choice to implement a stack: a singly linked list or an array? \nExperiments \nCarrying out these experiments will help to provide insights into the topics covered in the chapter. No programming is involved. \n1. Use the LinkList Workshop applet to execute insert, find, and delete operations on both sorted and unsorted lists. For the operations demonstrated by this applet, is there any advantage to the sorted list?   \n2. Modify main() in the linkList.java program (Listing 5.1) so that it continuously inserts links into the list until memory is exhausted. After each 1,000 items, have it display the number of items inserted so far. This way, you can learn approximately how many links a list can hold in your particular machine. (Of course, the number will vary depending on what other programs are in memory and many other factors.) Don’t try this experiment if it will crash your institution’s network. \nProgramming Projects \nWriting programs that solve the Programming Projects helps to solidify your understanding of the material and demonstrates how the chapter’s concepts are applied. \n(As noted in the Introduction, qualified instructors may obtain completed solutions to the Programming Projects on the publisher’s Web site.) \n5.1 Implement a priority queue based on a sorted linked list. The remove operation on the priority queue should remove the item with the smallest key.   \n5.2 Implement a deque based on a doubly linked list. (See Programming Project 4.2 in the preceding chapter.) The user should be able to carry out the normal operations on the deque.   \n5.3 A circular list is a linked list in which the last link points back to the first link. There are many ways to design a circular list. Sometimes there is a pointer to the “start” of the list. However, this makes the list less like a real circle and more like an ordinary list that has its end attached to its beginning. Make a class for a singly linked circular list that has no end and no beginning. The only access to the list is a single reference, current, that can point to any link on the list. This reference can move around the list as needed. (See Programming Project 5.5 for a situation in which such a circular list is ideally suited.) Your list should handle insertion, searching, and deletion. You may find it convenient if these operations take place one link downstream of the link pointed to by current. (Because the upstream link is singly linked, you can’t get at it without going all the way around the circle.) You should also be able to display the list (although you’ll need to break the circle at some arbitrary point to print it on the screen). A step() method that moves current along to the next link might come in handy too. \n5.4 Implement a stack class based on the circular list of Programming Project 5.3. This exercise is not too difficult. (However, implementing a queue can be harder, unless you make the circular list doubly linked.) \n5.5 The Josephus Problem is a famous mathematical puzzle that goes back to ancient times. There are many stories to go with the puzzle. One is that Josephus was one of a group of Jews who were about to be captured by the Romans. Rather than be enslaved, they chose to commit suicide. They arranged themselves in a circle and, starting at a certain person, started counting off around the circle. Every $mathrm { { n ^ { t h } } }$ person had to leave the circle and commit suicide. Josephus decided he didn’t want to die, so he arranged the rules so he would be the last person left. If there were (say) 20 people, and he was the seventh person from the start of the circle, what number should he tell them to use for counting off? The problem is made much more complicated because the circle shrinks as the counting continues. \nCreate an application that uses a circular linked list (like that in Programming Project 5.3) to model this problem. Inputs are the number of people in the circle, the number used for counting off, and the number of the person where counting starts (usually 1). The output is the list of persons being eliminated. When a person drops out of the circle, counting starts again from the person who was on his left (assuming you go around clockwise). Here’s an example. There are seven people numbered 1 through 7, and you start at 1 and count off by threes. People will be eliminated in the order 4, 1, 6, 5, 7, 3. Number 2 will be left. \n\n5.6 Let’s try something a little different: a two-dimensional linked list, which we’ll call a matrix. This is the list analogue of a two-dimensional array. It might be useful in applications such as spreadsheet programs. If a spreadsheet is based on an array, and you insert a new row near the top, you must move every cell in the lower rows $mathrm { { N ^ { star } M } }$ cells, which is potentially a slow process. If the spreadsheet is implemented by a matrix, you need only change N pointers. \nFor simplicity, we’ll assume a singly linked approach (although a double-linked approach would probably be more appropriate for a spreadsheet). Each link (except those on the top row and left side) is pointed to by the link directly above it and by the link on its left. You can start at the upper-left link and navigate to, say, the link on the third row and fifth column by following the pointers down two rows and right four columns. Assume your matrix is created with specified dimensions (7 by 10, for example). You should be able to insert values in specified links and display the contents of the matrix. \n6 \nRecursion \nRecursion is a programming technique in which a method (function) calls itself. This may sound like a strange thing to do, or even a catastrophic mistake. Recursion is, however, one of the most interesting, and one of the most surprisingly effective, techniques in programming. Like pulling yourself up by your bootstraps (you do have bootstraps, don’t you?), recursion seems incredible when you first encounter it. However, it not only works, it also provides a unique conceptual framework for solving many problems. \nIn this chapter we’ll examine numerous examples to show the wide variety of situations to which recursion can be applied. We will calculate triangular numbers and factorials, generate anagrams, perform a recursive binary search, solve the Towers of Hanoi puzzle, and investigate a sorting technique called mergesort. Workshop applets are provided to demonstrate the Towers of Hanoi and mergesort. \nWe’ll also discuss the strengths and weaknesses of recursion, and show how a recursive approach can be transformed into a stack-based approach. \nTriangular Numbers \nIt’s said that the Pythagorians, a band of mathematicians in ancient Greece who worked under Pythagoras (of Pythagorian theorem fame), felt a mystical connection with the series of numbers 1, 3, 6, 10, 15, 21, … (where the … means the series continues indefinitely). Can you find the next member of this series? \nThe nth term in the series is obtained by adding n to the previous term. Thus, the second term is found by adding 2 to the first term (which is 1), giving 3. The third term is 3 added to the second term (which is 3) giving 6, and so on. \nIN THIS CHAPTER \n• Triangular Numbers   \n• Factorials   \n• Anagrams   \n• A Recursive Binary Search   \n• The Towers of Hanoi   \n• Mergesort   \n• Eliminating Recursion   \n• Some Interesting Recursive Applications",
      "metadata": {
        "content": "15. Which do you think would be a better choice to implement a stack: a singly linked list or an array? \nExperiments \nCarrying out these experiments will help to provide insights into the topics covered in the chapter. No programming is involved. \n1. Use the LinkList Workshop applet to execute insert, find, and delete operations on both sorted and unsorted lists. For the operations demonstrated by this applet, is there any advantage to the sorted list?   \n2. Modify main() in the linkList.java program (Listing 5.1) so that it continuously inserts links into the list until memory is exhausted. After each 1,000 items, have it display the number of items inserted so far. This way, you can learn approximately how many links a list can hold in your particular machine. (Of course, the number will vary depending on what other programs are in memory and many other factors.) Don’t try this experiment if it will crash your institution’s network. \nProgramming Projects \nWriting programs that solve the Programming Projects helps to solidify your understanding of the material and demonstrates how the chapter’s concepts are applied. \n(As noted in the Introduction, qualified instructors may obtain completed solutions to the Programming Projects on the publisher’s Web site.) \n5.1 Implement a priority queue based on a sorted linked list. The remove operation on the priority queue should remove the item with the smallest key.   \n5.2 Implement a deque based on a doubly linked list. (See Programming Project 4.2 in the preceding chapter.) The user should be able to carry out the normal operations on the deque.   \n5.3 A circular list is a linked list in which the last link points back to the first link. There are many ways to design a circular list. Sometimes there is a pointer to the “start” of the list. However, this makes the list less like a real circle and more like an ordinary list that has its end attached to its beginning. Make a class for a singly linked circular list that has no end and no beginning. The only access to the list is a single reference, current, that can point to any link on the list. This reference can move around the list as needed. (See Programming Project 5.5 for a situation in which such a circular list is ideally suited.) Your list should handle insertion, searching, and deletion. You may find it convenient if these operations take place one link downstream of the link pointed to by current. (Because the upstream link is singly linked, you can’t get at it without going all the way around the circle.) You should also be able to display the list (although you’ll need to break the circle at some arbitrary point to print it on the screen). A step() method that moves current along to the next link might come in handy too. \n5.4 Implement a stack class based on the circular list of Programming Project 5.3. This exercise is not too difficult. (However, implementing a queue can be harder, unless you make the circular list doubly linked.) \n5.5 The Josephus Problem is a famous mathematical puzzle that goes back to ancient times. There are many stories to go with the puzzle. One is that Josephus was one of a group of Jews who were about to be captured by the Romans. Rather than be enslaved, they chose to commit suicide. They arranged themselves in a circle and, starting at a certain person, started counting off around the circle. Every $mathrm { { n ^ { t h } } }$ person had to leave the circle and commit suicide. Josephus decided he didn’t want to die, so he arranged the rules so he would be the last person left. If there were (say) 20 people, and he was the seventh person from the start of the circle, what number should he tell them to use for counting off? The problem is made much more complicated because the circle shrinks as the counting continues. \nCreate an application that uses a circular linked list (like that in Programming Project 5.3) to model this problem. Inputs are the number of people in the circle, the number used for counting off, and the number of the person where counting starts (usually 1). The output is the list of persons being eliminated. When a person drops out of the circle, counting starts again from the person who was on his left (assuming you go around clockwise). Here’s an example. There are seven people numbered 1 through 7, and you start at 1 and count off by threes. People will be eliminated in the order 4, 1, 6, 5, 7, 3. Number 2 will be left. \n\n5.6 Let’s try something a little different: a two-dimensional linked list, which we’ll call a matrix. This is the list analogue of a two-dimensional array. It might be useful in applications such as spreadsheet programs. If a spreadsheet is based on an array, and you insert a new row near the top, you must move every cell in the lower rows $mathrm { { N ^ { star } M } }$ cells, which is potentially a slow process. If the spreadsheet is implemented by a matrix, you need only change N pointers. \nFor simplicity, we’ll assume a singly linked approach (although a double-linked approach would probably be more appropriate for a spreadsheet). Each link (except those on the top row and left side) is pointed to by the link directly above it and by the link on its left. You can start at the upper-left link and navigate to, say, the link on the third row and fifth column by following the pointers down two rows and right four columns. Assume your matrix is created with specified dimensions (7 by 10, for example). You should be able to insert values in specified links and display the contents of the matrix. \n6 \nRecursion \nRecursion is a programming technique in which a method (function) calls itself. This may sound like a strange thing to do, or even a catastrophic mistake. Recursion is, however, one of the most interesting, and one of the most surprisingly effective, techniques in programming. Like pulling yourself up by your bootstraps (you do have bootstraps, don’t you?), recursion seems incredible when you first encounter it. However, it not only works, it also provides a unique conceptual framework for solving many problems. \nIn this chapter we’ll examine numerous examples to show the wide variety of situations to which recursion can be applied. We will calculate triangular numbers and factorials, generate anagrams, perform a recursive binary search, solve the Towers of Hanoi puzzle, and investigate a sorting technique called mergesort. Workshop applets are provided to demonstrate the Towers of Hanoi and mergesort. \nWe’ll also discuss the strengths and weaknesses of recursion, and show how a recursive approach can be transformed into a stack-based approach. \nTriangular Numbers \nIt’s said that the Pythagorians, a band of mathematicians in ancient Greece who worked under Pythagoras (of Pythagorian theorem fame), felt a mystical connection with the series of numbers 1, 3, 6, 10, 15, 21, … (where the … means the series continues indefinitely). Can you find the next member of this series? \nThe nth term in the series is obtained by adding n to the previous term. Thus, the second term is found by adding 2 to the first term (which is 1), giving 3. The third term is 3 added to the second term (which is 3) giving 6, and so on. \nIN THIS CHAPTER \n• Triangular Numbers   \n• Factorials   \n• Anagrams   \n• A Recursive Binary Search   \n• The Towers of Hanoi   \n• Mergesort   \n• Eliminating Recursion   \n• Some Interesting Recursive Applications",
        "chapter": "CHAPTER 5 Linked Lists",
        "section": "Programming Projects",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 13,
        "node_index": 55
      }
    },
    {
      "node_id": "tb1_node14",
      "content": "1. Divide the input array into two subarrays of roughly equal size. \n2. Recursively mergesort each of the subarrays. \n3. Merge the newly-sorted subarrays into a single sorted array. \nInput: S O R T I N G E X A M P L Divide: S O R T I N G E X A M P L   \nRecurse Left: I N O R S T G E X A M P L   \nRecurse Right: I N O R S T A E G L M P X Merge: A E G I L M N O P R S T X \nThe first step is completely trivial—just divide the array size by two—and we can delegate the second step to the Recursion Fairy. All the real work is done in the final merge step. A complete description of the algorithm is given in Figure 1.6; to keep the recursive structure clear, I’ve extracted the merge step into an independent subroutine. The merge algorithm is also recursive—identify the first element of the output array, and then recursively merge the rest of the input arrays. \nCorrectness \nTo prove that this algorithm is correct, we apply our old friend induction twice, first to the Merge subroutine then to the top-level Mergesort algorithm. \nLemma 1.1. Merge correctly merges the subarrays $A [ 1 ldots m ]$ and $A [ m + 1 ldots n ] ,$ .   \nassuming those subarrays are sorted in the input. \nProof: Let $A [ 1 ldots n ]$ be any array and $m$ any integer such that the subarrays $A [ 1 ldots m ]$ and $A [ m + 1 ldots n ]$ are sorted. We prove that for all $k$ from 0 to $n$ , the last $n - k - 1$ iterations of the main loop correctly merge $A [ i ldots m ]$ and $A [ j ldots n ]$ into $B [ k . . n ]$ . The proof proceeds by induction on $n - k + 1$ , the number of elements remaining to be merged. \nIf $k > n$ , the algorithm correctly merges the two empty subarrays by doing absolutely nothing. (This is the base case of the inductive proof.) Otherwise, there are four cases to consider for the kth iteration of the main loop. \n• If $j > n$ , then subarray $A [ j ldots n ]$ is empty, so $operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ i ]$ .   \n• If $i > m$ , then subarray $A [ i ldots m ]$ is empty, $operatorname { s o } operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ j ]$ .   \n• Otherwise, if $A [ i ] < A [ j ]$ , then $operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ i ]$ .   \n• Otherwise, we must have $A [ i ] geq A [ j ]$ , and $operatorname* { m i n } { big ( } A [ i ldots m ] cup A [ j ldots n ] { big ) } = A [ j ]$ . \nIn all four cases, $B [ k ]$ is correctly assigned the smallest element of $A [ i ldots m ] cup$ $A [ j ldots n ]$ . In the two cases with the assignment $B [ k ]  A [ i ]$ , the Recursion Fairy correctly merges—sorry, I mean the Induction Hypothesis implies that the last $n - k$ iterations of the main loop correctly merge $A [ i + 1 ldots m ]$ and $A [ j ldots n ]$ into $B [ k + 1 . . n ]$ . Similarly, in the other two cases, the Recursion Fairy also correctly merges the rest of the subarrays. □ \nTheorem 1.2. MergeSort correctly sorts any input array $A [ 1 ldots n ]$ . \nProof: We prove the theorem by induction on $n$ . If $n leq 1$ , the algorithm correctly does nothing. Otherwise, the Recursion Fairy correctly sorts—sorry, I mean the induction hypothesis implies that our algorithm correctly sorts the two smaller subarrays $A [ 1 ldots m ]$ and $A [ m + 1 ldots n ]$ , after which they are correctly Merged into a single sorted array (by Lemma 1.1). □ \nAnalysis \nBecause the MergeSort algorithm is recursive, its running time is naturally expressed as a recurrence. Merge clearly takes $O ( n )$ time, because it’s a simple for-loop with constant work per iteration. We immediately obtain the following recurrence for MergeSort: \nAs in most divide-and-conquer recurrences, we can safely strip out the floors and ceilings (using a technique called domain transformations described later in this chapter), giving us the simpler recurrence $T ( n ) = 2 T ( n / 2 ) + O ( n )$ . The “all levels equal” case of the recursion tree method (also described later in this chapter) immediately implies the closed-form solution $T ( n ) = O ( n log n )$ . Even if you are not (yet) familiar with recursion trees, you can verify the solution $T ( n ) = O ( n log n )$ by induction.",
      "metadata": {
        "content": "1. Divide the input array into two subarrays of roughly equal size. \n2. Recursively mergesort each of the subarrays. \n3. Merge the newly-sorted subarrays into a single sorted array. \nInput: S O R T I N G E X A M P L Divide: S O R T I N G E X A M P L   \nRecurse Left: I N O R S T G E X A M P L   \nRecurse Right: I N O R S T A E G L M P X Merge: A E G I L M N O P R S T X \nThe first step is completely trivial—just divide the array size by two—and we can delegate the second step to the Recursion Fairy. All the real work is done in the final merge step. A complete description of the algorithm is given in Figure 1.6; to keep the recursive structure clear, I’ve extracted the merge step into an independent subroutine. The merge algorithm is also recursive—identify the first element of the output array, and then recursively merge the rest of the input arrays. \nCorrectness \nTo prove that this algorithm is correct, we apply our old friend induction twice, first to the Merge subroutine then to the top-level Mergesort algorithm. \nLemma 1.1. Merge correctly merges the subarrays $A [ 1 ldots m ]$ and $A [ m + 1 ldots n ] ,$ .   \nassuming those subarrays are sorted in the input. \nProof: Let $A [ 1 ldots n ]$ be any array and $m$ any integer such that the subarrays $A [ 1 ldots m ]$ and $A [ m + 1 ldots n ]$ are sorted. We prove that for all $k$ from 0 to $n$ , the last $n - k - 1$ iterations of the main loop correctly merge $A [ i ldots m ]$ and $A [ j ldots n ]$ into $B [ k . . n ]$ . The proof proceeds by induction on $n - k + 1$ , the number of elements remaining to be merged. \nIf $k > n$ , the algorithm correctly merges the two empty subarrays by doing absolutely nothing. (This is the base case of the inductive proof.) Otherwise, there are four cases to consider for the kth iteration of the main loop. \n• If $j > n$ , then subarray $A [ j ldots n ]$ is empty, so $operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ i ]$ .   \n• If $i > m$ , then subarray $A [ i ldots m ]$ is empty, $operatorname { s o } operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ j ]$ .   \n• Otherwise, if $A [ i ] < A [ j ]$ , then $operatorname* { m i n } left( A [ i ldots m ] cup A [ j ldots n ] right) = A [ i ]$ .   \n• Otherwise, we must have $A [ i ] geq A [ j ]$ , and $operatorname* { m i n } { big ( } A [ i ldots m ] cup A [ j ldots n ] { big ) } = A [ j ]$ . \nIn all four cases, $B [ k ]$ is correctly assigned the smallest element of $A [ i ldots m ] cup$ $A [ j ldots n ]$ . In the two cases with the assignment $B [ k ]  A [ i ]$ , the Recursion Fairy correctly merges—sorry, I mean the Induction Hypothesis implies that the last $n - k$ iterations of the main loop correctly merge $A [ i + 1 ldots m ]$ and $A [ j ldots n ]$ into $B [ k + 1 . . n ]$ . Similarly, in the other two cases, the Recursion Fairy also correctly merges the rest of the subarrays. □ \nTheorem 1.2. MergeSort correctly sorts any input array $A [ 1 ldots n ]$ . \nProof: We prove the theorem by induction on $n$ . If $n leq 1$ , the algorithm correctly does nothing. Otherwise, the Recursion Fairy correctly sorts—sorry, I mean the induction hypothesis implies that our algorithm correctly sorts the two smaller subarrays $A [ 1 ldots m ]$ and $A [ m + 1 ldots n ]$ , after which they are correctly Merged into a single sorted array (by Lemma 1.1). □ \nAnalysis \nBecause the MergeSort algorithm is recursive, its running time is naturally expressed as a recurrence. Merge clearly takes $O ( n )$ time, because it’s a simple for-loop with constant work per iteration. We immediately obtain the following recurrence for MergeSort: \nAs in most divide-and-conquer recurrences, we can safely strip out the floors and ceilings (using a technique called domain transformations described later in this chapter), giving us the simpler recurrence $T ( n ) = 2 T ( n / 2 ) + O ( n )$ . The “all levels equal” case of the recursion tree method (also described later in this chapter) immediately implies the closed-form solution $T ( n ) = O ( n log n )$ . Even if you are not (yet) familiar with recursion trees, you can verify the solution $T ( n ) = O ( n log n )$ by induction.",
        "chapter": "Recursion",
        "section": "Mergesort",
        "subsection": "Correctness",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 14
      }
    },
    {
      "node_id": "tb13_node68",
      "content": "7 Advanced Sorting \nWe discussed simple sorting in the aptly titled Chapter 3, “Simple Sorting.” The sorts described there—the bubble, selection, and insertion sorts—are easy to implement but are rather slow. In Chapter 6, “Recursion,” we described the mergesort. It runs much faster than the simple sorts but requires twice as much space as the original array; this is often a serious drawback. \nThis chapter covers two advanced approaches to sorting: Shellsort and quicksort. These sorts both operate much faster than the simple sorts: the Shellsort in about $mathrm { O } ( mathrm { N } ^ { star } ( log mathrm { N } ) ^ { 2 } )$ time, and quicksort in $mathrm { O ( N ^ { star } l o g N ) }$ time. Neither of these sorts requires a large amount of extra space, as mergesort does. The Shellsort is almost as easy to implement as mergesort, while quicksort is the fastest of all the general-purpose sorts. We’ll conclude the chapter with a brief mention of the radix sort, an unusual and interesting approach to sorting. \nWe’ll examine the Shellsort first. Quicksort is based on the idea of partitioning, so we’ll then examine partitioning separately, before examining quicksort itself. \nShellsort \nThe Shellsort is named for Donald L. Shell, the computer scientist who discovered it in 1959. It’s based on the insertion sort, but adds a new feature that dramatically improves the insertion sort’s performance. \nThe Shellsort is good for medium-sized arrays, perhaps up to a few thousand items, depending on the particular implementation. It’s not quite as fast as quicksort and other $mathrm { O ( N ^ { star } l o g N ) }$ sorts, so it’s not optimum for very large files. However, it’s much faster than the $mathrm { O } ( mathrm { N } ^ { 2 } )$ sorts like the selection sort and the insertion sort, and it’s very easy to implement: The code is short and simple. \nIN THIS CHAPTER \n• Shellsort • Partitioning • Quicksort • Radix Sort \nThe worst-case performance is not significantly worse than the average performance. (We’ll see later in this chapter that the worst-case performance for quicksort can be much worse unless precautions are taken.) Some experts (see Sedgewick in Appendix B, “Further Reading”) recommend starting with a Shellsort for almost any sorting project and changing to a more advanced sort, like quicksort, only if Shellsort proves too slow in practice. \nInsertion Sort: Too Many Copies \nBecause Shellsort is based on the insertion sort, you might want to review the section titled “Insertion Sort” in Chapter 3. Recall that partway through the insertion sort the items to the left of a marker are internally sorted (sorted among themselves) and items to the right are not. The algorithm removes the item at the marker and stores it in a temporary variable. Then, beginning with the item to the left of the newly vacated cell, it shifts the sorted items right one cell at a time, until the item in the temporary variable can be reinserted in sorted order. \nHere’s the problem with the insertion sort. Suppose a small item is on the far right, where the large items should be. To move this small item to its proper place on the left, all the intervening items (between the place where it is and where it should be) must be shifted one space right. This step takes close to N copies, just for one item. Not all the items must be moved a full N spaces, but the average item must be moved $mathrm { N } / 2$ spaces, which takes $mathrm { Delta N }$ times $_ { mathrm { N } / 2 }$ shifts for a total of $mathrm { N } ^ { 2 } / 2$ copies. Thus, the performance of insertion sort is $mathrm { O } ( mathrm { N } ^ { 2 } )$ . \nThis performance could be improved if we could somehow move a smaller item many spaces to the left without shifting all the intermediate items individually. \nN-Sorting \nThe Shellsort achieves these large shifts by insertion-sorting widely spaced elements. After they are sorted, it sorts somewhat less widely spaced elements, and so on. The spacing between elements for these sorts is called the increment and is traditionally represented by the letter h. Figure 7.1 shows the first step in the process of sorting a 10-element array with an increment of 4. Here the elements 0, 4, and 8 are sorted. \nAfter 0, 4, and 8 are sorted, the algorithm shifts over one cell and sorts 1, 5, and 9. This process continues until all the elements have been 4-sorted, which means that all items spaced four cells apart are sorted among themselves. The process is shown (using a more compact visual metaphor) in Figure 7.2. \nAfter the complete 4-sort, the array can be thought of as comprising four subarrays: (0,4,8), (1,5,9), (2,6), and (3,7), each of which is completely sorted. These subarrays are interleaved but otherwise independent. \n  \n0 1 2 3 4 5 6 7 8 9   \n↑ ↑ Unsorted   \n0 m   \n↑ ↑ ↑ Sorted \nNotice that, in this particular example, at the end of the 4-sort no item is more than two cells from where it would be if the array were completely sorted. This is what is meant by an array being “almost” sorted and is the secret of the Shellsort. By creating interleaved, internally sorted sets of items, we minimize the amount of work that must be done to complete the sort. \nNow, as we noted in Chapter 3, the insertion sort is very efficient when operating on an array that’s almost sorted. If it needs to move items only one or two cells to sort the file, it can operate in almost O(N) time. Thus, after the array has been 4-sorted, we can 1-sort it using the ordinary insertion sort. The combination of the 4-sort and the 1-sort is much faster than simply applying the ordinary insertion sort without the preliminary 4-sort. \nDiminishing Gaps \nWe’ve shown an initial interval—or gap—of 4 cells for sorting a 10-cell array. For larger arrays the interval should start out much larger. The interval is then repeatedly reduced until it becomes 1. \nFor instance, an array of 1,000 items might be 364-sorted, then 121-sorted, then 40- sorted, then 13-sorted, then 4-sorted, and finally 1-sorted. The sequence of numbers used to generate the intervals (in this example, 364, 121, 40, 13, 4, 1) is called the interval sequence or gap sequence. The particular interval sequence shown here, \nattributed to Knuth (see Appendix B), is a popular one. In reversed form, starting from 1, it’s generated by the recursive expression \nwhere the initial value of h is 1. The first two columns of Table 7.1 show how this formula generates the sequence. \nFIGURE 7.2 A complete 4-sort. \nThere are other approaches to generating the interval sequence; we’ll return to this issue later. First, we’ll explore how the Shellsort works using Knuth’s sequence. \nIn the sorting algorithm, the sequence-generating formula is first used in a short loop to figure out the initial gap. A value of 1 is used for the first value of h, and the $scriptstyle mathrm { h = h } ^ { star } 3 + 1$ formula is applied to generate the sequence 1, 4, 13, 40, 121, 364, and so on. This process ends when the gap is larger than the array. For a 1,000-element array, the seventh number in the sequence, 1,093, is too large. Thus, we begin the sorting process with the sixth-largest number, creating a 364-sort. Then, each time through the outer loop of the sorting routine, we reduce the interval using the inverse of the formula previously given: \nThis is shown in the third column of Table 7.1. This inverse formula generates the reverse sequence 364, 121, 40, 13, 4, 1. Starting with 364, each of these numbers is used to n-sort the array. When the array has been 1-sorted, the algorithm is done. \nThe Shellsort Workshop Applet \nYou can use the Shellsort Workshop applet to see how this sort works. Figure 7.3 shows the applet after all the bars have been 4-sorted, just as the 1-sort begins. \nAs you single-step through the algorithm, you’ll notice that the explanation we gave in the preceding discussion is slightly simplified. The sequence for the 4-sort is not actually (0,4,8), (1,5,9), (2,6), and (3,7). Instead, the first two elements of each group of three are sorted first, then the first two elements of the second group, and so on. Once the first two elements of all the groups are sorted, the algorithm returns and sorts three-element groups. The actual sequence is (0,4), (1,5), (2,6), (3,7), (0,4,8), (1,5,9). \nIt might seem more obvious for the algorithm to 4-sort each complete subarray first—(0,4), (0,4,8), (1,5), (1,5,9), (2,6), (3,7)—but the algorithm handles the array indices more efficiently using the first scheme. \nThe Shellsort is actually not very efficient with only 10 items, making almost as many swaps and comparisons as the insertion sort. However, with 100 bars the improvement becomes significant. \nIt’s instructive to run the Workshop applet starting with 100 inversely sorted bars. (Remember that, as in Chapter 3, the first press of New creates a random sequence of bars, while the second press creates an inversely sorted sequence.) Figure 7.4 shows how the bars look after the first pass, when the array has been completely 40-sorted. Figure 7.5 shows the situation after the next pass, when it is 13-sorted. With each new value of $mathbf { h }$ , the array becomes more nearly sorted. \nWhy is the Shellsort so much faster than the insertion sort, on which it’s based? When h is large, the number of items per pass is small, and items move long distances. This is very efficient. As h grows smaller, the number of items per pass increases, but the items are already closer to their final sorted positions, which is more efficient for the insertion sort. It’s the combination of these trends that makes the Shellsort so effective. \n\nNotice that later sorts (small values of h) don’t undo the work of earlier sorts (large values of h). An array that has been 40-sorted remains 40-sorted after a 13-sort, for example. If this wasn’t so, the Shellsort couldn’t work. \nJava Code for the Shellsort \nThe Java code for the Shellsort is scarcely more complicated than for the insertion sort. Starting with the insertion sort, you substitute h for 1 in appropriate places and add the formula to generate the interval sequence. We’ve made shellSort() a method in the ArraySh class, a version of the array classes from Chapter 2, “Arrays.” Listing 7.1 shows the complete shellSort.java program. \nLISTING 7.1 The shellSort.java Program \n// shellSort.java   \n$bigstar / / bigstar$ demonstrates shell sort   \n$bigstar / / bigstar$ to run this program: $complement >$ java ShellSortApp   \n/   \nclass ArraySh { private long[] theArray; // ref to array theArray private int nElems; $bigstar / / bigstar$ number of data items \nLISTING 7.1 Continued \npublic ArraySh(int max) // constructor { theArray $mathbf { tau } = mathbf { tau }$ new long[max]; // create the array nElems $= 0$ ; // no items yet }   \n// public void insert(long value) // put element into array { theArray[nElems] $mathbf { Sigma } = mathbf { Sigma }$ value; $bigstar / / bigstar$ insert it nElems++; $bigstar / / bigstar$ increment size }   \n// public void display() $bigstar / / bigstar$ displays array contents { System.out.print ${ mathrm { ? } }  { mathsf { A } } =  \" mathbf { Gamma }$ ); for(int $scriptstyle mathbf { j } = 0$ ; j<nElems; ${ mathrm { j } } + +$ ) $bigstar / / bigstar$ for each element, System.out.print(theArray[j] $^ +$ “ “);  // display it System.out.println ${ bigl ( } { begin{array} { l } { ^ { mathfrak { u } } }  { } end{array} } { bigr ) }$ ; } public void shellSort() { int inner, outer; long temp; int ${ mathsf { h } }  =  { mathsf { 1 } }$ ; // find initial value of h while(h $< =$ nElems/3) ${ textsf { h } } = { mathsf { h } } ^ { star } 3 { mathsf { Omega } } + { mathsf { Omega } } 1$ ; // (1, 4, 13, 40, 121, ...) while $( n > 0 )$ // decreasing h, until $h = 1$ { // h-sort the file for(outer $cdot = h$ ; outer<nElems; outer $^ { cdot + + }$ ) { temp $mathbf { tau } = mathbf { tau }$ theArray[outer]; inner $mathbf { tau } = mathbf { tau }$ outer; $bigstar / / bigstar$ one subpass (eg 0, 4, 8) while(inner $>$ h-1 && theArray[inner-h] $> =$ temp) { theArray[inner] $mathbf { tau } = mathbf { tau }$ theArray[inner-h]; inner $cdot = mathsf { h }$ ; $}$ theArray[inner] $mathbf { tau } = mathbf { tau }$ temp; }  // end for ${ textsf { h } } = ( { mathsf { h } } - 1 ) / 3 ;$ ; // decrease h } // end while $( n > 0 )$ ) } // end shellSort()   \n//- } // end class ArraySh   \n////////////////////////////////////////////////////////////////   \nclass ShellSortApp { public static void main(String[] args) { int maxSize $= 1 0$ ; // array size ArraySh arr; arr $mathbf { tau } = mathbf { tau }$ new ArraySh(maxSize); // create the array for(int $scriptstyle mathbf { j } = 0$ ; j<maxSize; ${ mathrm { j } } + +$ ) $1 1$ fill array with { // random numbers long ${ mathfrak { n } } =$ (int)(java.lang.Math.random $( ) ^ { star } 9 9 $ ); arr.insert(n); } arr.display(); // display unsorted array arr.shellSort(); // shell sort the array arr.display(); // display sorted array }  // end main() }  // end class ShellSortApp \n\nIn main() we create an object of type ArraySh, able to hold 10 items, fill it with random data, display it, Shellsort it, and display it again. Here’s some sample output: \n$A = 2 0$ 89 6 42 55 59 41 69 75 66   \n$A = 6$ 20 41 42 55 59 66 69 75 89 \nYou can change maxSize to higher numbers, but don’t go too high; 10,000 items take a fraction of a minute to sort. \nThe Shellsort algorithm, although it’s implemented in just a few lines, is not simple to follow. To see the details of its operation, step through a 10-item sort with the Workshop applet, comparing the messages generated by the applet with the code in the shellSort() method. \nOther Interval Sequences \nPicking an interval sequence is a bit of a black art. Our discussion so far used the formula $scriptstyle mathrm { h = h } ^ { star } 3 + 1$ to generate the interval sequence, but other interval sequences have been used with varying degrees of success. The only absolute requirement is that the diminishing sequence ends with 1, so the last pass is a normal insertion sort. \nIn Shell’s original paper, he suggested an initial gap of $mathrm { N } / 2$ , which was simply divided in half for each pass. Thus, the descending sequence for $_ { mathrm { N } = 1 0 0 }$ is 50, 25, 12, 6, 3, 1. This approach has the advantage that you don’t need to calculate the sequence before the sort begins to find the initial gap; you just divide N by 2. However, this turns out not to be the best sequence. Although it’s still better than the insertion sort for most data, it sometimes degenerates to $mathrm { O } ( mathrm { N } ^ { 2 } )$ running time, which is no better than the insertion sort. \nA variation of this approach is to divide each interval by 2.2 instead of 2. For $scriptstyle mathrm { n = 1 0 0 }$ this leads to 45, 20, 9, 4, 1. This is considerably better than dividing by 2, as it avoids some worst-case circumstances that lead to $mathrm { O } ( mathrm { N } ^ { 2 } )$ behavior. Some extra code is needed to ensure that the last value in the sequence is 1, no matter what N is. This gives results comparable to Knuth’s sequence shown in the listing. \nAnother possibility for a descending sequence (from Flamig; see Appendix B) is \n$begin{array} { r } { mathrm { i f ~ ( mathsf { h } ~ leqslant ~ 5 ) ~ } }  { mathsf { h } ~ mathsf { = } ~ 1 ; } end{array}$   \nelse $h = ( 5 times h - 1 ) / 1 1 ;$ \nIt’s generally considered important that the numbers in the interval sequence are relatively prime; that is, they have no common divisors except 1. This constraint makes it more likely that each pass will intermingle all the items sorted on the previous pass. The inefficiency of Shell’s original N/2 sequence is due to its failure to adhere to this rule. \nYou may be able to invent a gap sequence of your own that does just as well (or possibly even better) than those shown. Whatever it is, it should be quick to calculate so as not to slow down the algorithm. \nEfficiency of the Shellsort \nNo one so far has been able to analyze the Shellsort’s efficiency theoretically, except in special cases. Based on experiments, there are various estimates, which range from $0 ( mathrm { N } ^ { 3 / 2 } )$ down to $mathrm { O } ( mathrm { N } ^ { 7 / 6 } )$ . \nTable 7.2 shows some of these estimated O() values, compared with the slower insertion sort and the faster quicksort. The theoretical times corresponding to various values of $mathrm { Delta N }$ are shown. Note that $mathrm { N ^ { x / y } }$ means the yth root of N raised to the x power. Thus, if $mathrm { Delta N }$ is 100, $mathrm { N } ^ { 3 / 2 }$ is the square root of $1 0 0 ^ { 3 }$ , which is 1,000. Also, $( mathrm { l o g N } ) ^ { 2 }$ means the log of $mathrm { N _ { it } }$ , squared. This is often written $log ^ { 2 } mathrm { N } _ { cdot }$ , but that’s easy to confuse with $log _ { 2 } mathrm { N } _ { 1 }$ , the logarithm to the base 2 of N. \nFor most data, the higher estimates, such as $mathrm { N } ^ { 3 / 2 }$ , are probably more realistic. \nPartitioning \nPartitioning is the underlying mechanism of quicksort, which we’ll explore next, but it’s also a useful operation on its own, so we’ll cover it here in its own section. \nTo partition data is to divide it into two groups, so that all the items with a key value higher than a specified amount are in one group, and all the items with a lower key value are in another. \nYou can easily imagine situations in which you would want to partition data. Maybe you want to divide your personnel records into two groups: employees who live within 15 miles of the office and those who live farther away. Or a school administrator might want to divide students into those with grade point averages higher and lower than 3.5, so as to know who deserves to be on the Dean’s list. \nThe Partition Workshop Applet \nOur Partition Workshop applet demonstrates the partitioning process. Figure 7.6 shows 12 bars before partitioning, and Figure 7.7 shows them again after partitioning. \nApplet Viewer:partition.class -□   \nApplet   \nSwaps=0 New Size Draw Run Step   \nComparisons =0 tt Pivot value is 104   \nApplet started \nThe horizontal line represents the pivot value, which is the value used to determine into which of the two groups an item is placed. Items with a key value less than the pivot value go in the left part of the array, and those with a greater (or equal) key go in the right part. (In the section on quicksort, we’ll see that the pivot value can be the key value of an actual data item, called the pivot. For now, it’s just a number.) \nThe arrow labeled partition points to the leftmost item in the right (higher) subarray. This value is returned from the partitioning method, so it can be used by other methods that need to know where the division is. \nFor a more vivid display of the partitioning process, set the Partition Workshop applet to 100 bars and press the Run button. The leftScan and rightScan pointers will zip toward each other, swapping bars as they go. When they meet, the partition is complete. \nYou can choose any value you want for the pivot value, depending on why you’re doing the partition (such as choosing a grade point average of 3.5). For variety, the Workshop applet chooses a random number for the pivot value (the horizontal black line) each time New or Size is pressed, but the value is never too far from the average bar height. \nAfter being partitioned, the data is by no means sorted; it has simply been divided into two groups. However, it’s more sorted than it was before. As we’ll see in the next section, it doesn’t take much more trouble to sort it completely. \nNotice that partitioning is not stable. That is, each group is not in the same order it was originally. In fact, partitioning tends to reverse the order of some of the data in each group. \nThe partition.java Program \nHow is the partitioning process carried out? Let’s look at some example code. Listing 7.2 shows the partition.java program, which includes the partitionIt() method for partitioning an array. \nLISTING 7.2 The partition.java Program \n// partition.java   \n// demonstrates partitioning an array   \n// to run this program: $complement >$ java PartitionApp   \n//////////////////////////////////////////// ////   \nclass ArrayPar { private long[] theArray; // ref to array theArray private int nElems; // number of data items public ArrayPar(int max) // constructor { theArray $mathbf { tau } = mathbf { tau }$ new long[max]; $bigstar / / bigstar$ create the array nElems $mathit { Theta } = mathit { Theta } varnothing$ ; $bigstar / / bigstar$ no items yet } public void insert(long value) // put element into array { theArray[nElems] $mathbf { tau } = mathbf { tau }$ value; // insert it \nLISTING 7.2 Continued \nnElems++; // increment size }   \n// public int size() // return number of items { return nElems; }   \n//- public void display() // displays array contents { System.out.print ${ } ^ { \" } { mathsf { A } } = \"$ ); for(int $scriptstyle mathbf { j } = 0$ ; j<nElems; ${ mathrm { j } } + +$ ) // for each element, System.out.print(theArray[j] $mathbf { Sigma } ^ { } + mathbf { Sigma } ^ { \" } mathbf { Sigma } ^ { } mathbf { Sigma } ^ { } )$ ;  // display it System.out.println ${ bigl ( } { begin{array} { l } { mathfrak { u }  mathfrak { u } }  { mathfrak { u } } end{array} } { bigr ) }$ ; }   \n// public int partitionIt(int left, int right, long pivot) { int leftPtr $mathbf { tau } = mathbf { tau }$ left - 1; // right of first elem int rightPtr $mathbf { tau } = mathbf { tau }$ right $^ +$ 1; // left of pivot while(true) { while(leftPtr $mathit { Theta } <$ right && // find bigger item theArray[++leftPtr] $mathit { Theta } _ { < }$ pivot) ;  // (nop) while(rightPtr $>$ left && // find smaller item theArray[--rightPtr] $>$ pivot) ;  // (nop) if(leftPtr $> =$ rightPtr) // if pointers cross, break; // partition done else // not crossed, so swap(leftPtr, rightPtr); // swap elements }  // end while(true) return leftPtr; // return partition }  // end partitionIt()   \n//-- public void swap(int dex1, int dex2) // swap two elements { long temp; temp $mathbf { tau } = mathbf { tau }$ theArray[dex1]; // A into temp theArray[dex1] $mathbf { tau } = mathbf { tau }$ theArray[dex2]; // B into A theArray[dex2] $mathbf { tau } = mathbf { tau }$ temp; // temp into B } // end swap()   \n//- } // end class ArrayPar   \n//////////////////////////////////////// /////////////////   \nclass PartitionApp { public static void main(String[] args) { int maxSize $mathbf { sigma } = mathbf { sigma }$ 16; // array size ArrayPar arr; $1 1$ reference to array arr $mathbf { tau } = mathbf { tau }$ new ArrayPar(maxSize); $1 1$ create the array for(int $scriptstyle mathbf { j } = 0$ ; j<maxSize; ${ mathrm { j } } + +$ )  // fill array with { // random numbers long ${ mathfrak { n } } =$ (int)(java.lang.Math.random $( ) star 1 9 9 $ ); arr.insert(n); } arr.display(); // display unsorted array long pivot $mathbf { mathfrak { s } } _ { 9 9 }$ ; // pivot value System.out.print(“Pivot is “ $^ +$ pivot); int size $mathbf { tau } = mathbf { tau }$ arr.size(); // partition array int partDex $mathbf { tau } = mathbf { tau }$ arr.partitionIt(0, size-1, pivot); System.out.println(“, Partition is at index “ $^ +$ partDex); arr.display(); // display partitioned array }  // end main() \n\nThe main() routine creates an ArrayPar object that holds 16 items of type long. The pivot value is fixed at 99. The routine inserts 16 random values into ArrayPar, displays them, partitions them by calling the partitionIt() method, and displays them again. Here’s some sample output: \n$A = 1 4 9$ 192 47 152 159 195 61 66 17 167 118 64 27 80 30 105   \nPivot is 99, partition is at index 8   \n$A = 3 0$ 80 47 27 64 17 61 66 195 167 118 159 152 192 149 105 \nYou can see that the partition is successful: The first eight numbers are all smaller than the pivot value of 99; the last eight are all larger. \nNotice that the partitioning process doesn’t necessarily divide the array in half as it does in this example; that depends on the pivot value and key values of the data. There may be many more items in one group than in the other. \nThe Partition Algorithm \nThe partitioning algorithm works by starting with two pointers, one at each end of the array. (We use the term pointers to mean indices that point to array elements, not $mathrm { C } { + } { + }$ pointers.) The pointer on the left, leftPtr, moves toward the right, and the one on the right, rightPtr, moves toward the left. Notice that leftPtr and rightPtr in the partition.java program correspond to leftScan and rightScan in the Partition Workshop applet. \nActually, leftPtr is initialized to one position to the left of the first cell, and rightPtr to one position to the right of the last cell, because they will be incremented and decremented, respectively, before they’re used. \nStopping and Swapping \nWhen leftPtr encounters a data item smaller than the pivot value, it keeps going because that item is already on the correct side of the array. However, when it encounters an item larger than the pivot value, it stops. Similarly, when rightPtr encounters an item larger than the pivot, it keeps going, but when it finds a smaller item, it also stops. Two inner while loops, the first for leftPtr and the second for rightPtr, control the scanning process. A pointer stops because its while loop exits. Here’s a simplified version of the code that scans for out-of-place items: \nwhile( theArray[ $^ { + + }$ leftPtr] $mathbf { Sigma } <$ pivot ) // find bigger item ;  // (nop)   \nwhile( theArray[--rightPtr] $>$ pivot ) // find smaller item ;  // (nop)   \nswap(leftPtr, rightPtr); // swap elements \nThe first while loop exits when an item larger than pivot is found; the second loop exits when an item smaller than pivot is found. When both these loops exit, both leftPtr and rightPtr point to items that are in the wrong sides of the array, so these items are swapped. \nAfter the swap, the two pointers continue on, again stopping at items that are in the wrong side of the array and swapping them. All this activity is nested in an outer while loop, as can be seen in the partitionIt() method in Listing 7.2. When the two pointers eventually meet, the partitioning process is complete and this outer while loop exits. \nYou can watch the pointers in action when you run the Partition Workshop applet with 100 bars. These pointers, represented by blue arrows, start at opposite ends of the array and move toward each other, stopping and swapping as they go. The bars between them are unpartitioned; those they’ve already passed over are partitioned. When they meet, the entire array is partitioned. \n\nHandling Unusual Data \nIf we were sure that there was a data item at the right end of the array that was smaller than the pivot value, and an item at the left end that was larger, the simplified while loops previously shown would work fine. Unfortunately, the algorithm may be called upon to partition data that isn’t so well organized. \nIf all the data is smaller than the pivot value, for example, the leftPtr variable will go all the way across the array, looking in vain for a larger item, and fall off the right end, creating an array index out of bounds exception. A similar fate will befall rightPtr if all the data is larger than the pivot value. \nTo avoid these problems, extra tests must be placed in the while loops to check for the ends of the array: leftPtr<right in the first loop and rightPtr>left in the second. You can see these tests in context in Listing 7.2. \nIn the section on quicksort, we’ll see that a clever pivot-selection process can eliminate these end-of-array tests. Eliminating code from inner loops is always a good idea if you want to make a program run faster. \nDelicate Code \nThe code in the while loops is rather delicate. For example, you might be tempted to remove the increment operators from the inner while loops and use them to replace the nop statements. (Nop refers to a statement consisting only of a semicolon, and means no operation). For example, you might try to change this: \nwhile(leftPtr $mathit { Delta } <$ right && theArray[++leftPtr] $mathit { Theta } _ { < }$ pivot) ;  // (nop) \nto this: \nwhile(leftPtr $angle { mathbf { xi } } < mathbf { xi }$ right && theArray[leftPtr] $angle { mathbf { xi } } < mathbf { xi }$ pivot) ++leftPtr; \nand similarly for the other inner while loop. These changes would make it possible for the initial values of the pointers to be left and right, which is somewhat clearer than left-1 and right $+ 1$ . \nHowever, these changes result in the pointers being incremented only when the condition is satisfied. The pointers must move in any case, so two extra statements within the outer while loop would be required to bump the pointers. The nop version is the most efficient solution. \nEqual Keys \nHere’s another subtle change you might be tempted to make in the partitionIt() code. If you run the partitionIt() method on items that are all equal to the pivot value, you will find that every comparison leads to a swap. Swapping items with equal keys seems like a waste of time. The $<$ and $>$ operators that compare pivot with the array elements in the while loops cause the extra swapping. However, suppose you try to fix this by replacing them with $< =$ and $> =$ operators. This indeed prevents the swapping of equal elements, but it also causes leftPtr and rightPtr to end up at the ends of the array when the algorithm has finished. As we’ll see in the section on quicksort, it’s good for the pointers to end up in the middle of the array, and very bad for them to end up at the ends. So if partitionIt() is going to be used for quicksort, the $<$ and $>$ operators are the right way to go, even if they cause some unnecessary swapping. \nEfficiency of the Partition Algorithm \nThe partition algorithm runs in O(N) time. It’s easy to see why this is so when running the Partition Workshop applet: The two pointers start at opposite ends of the array and move toward each other at a more or less constant rate, stopping and swapping as they go. When they meet, the partition is complete. If there were twice as many items to partition, the pointers would move at the same rate, but they would have twice as many items to compare and swap, so the process would take twice as long. Thus, the running time is proportional to N. \nMore specifically, for each partition there will be $_ { mathrm { N + 1 } }$ or $_ { mathrm { N } + 2 }$ comparisons. Every item will be encountered and used in a comparison by one or the other of the pointers, leading to N comparisons, but the pointers overshoot each other before they find out they’ve “crossed” or gone beyond each other, so there are one or two extra comparisons before the partition is complete. The number of comparisons is independent of how the data is arranged (except for the uncertainty between one or two extra comparisons at the end of the scan). \nThe number of swaps, however, does depend on how the data is arranged. If it’s inversely ordered, and the pivot value divides the items in half, then every pair of values must be swapped, which is $_ { mathrm { N } / 2 }$ swaps. (Remember in the Partition Workshop applet that the pivot value is selected randomly, so that the number of swaps for inversely sorted bars won’t always be exactly $_ { mathrm { N } / 2 }$ .) \nFor random data, there will be fewer than $_ { mathrm { N } / 2 }$ swaps in a partition, even if the pivot value is such that half the bars are shorter and half are taller. This is because some bars will already be in the right place (short bars on the left, tall bars on the right). If the pivot value is higher (or lower) than most of the bars, there will be even fewer swaps because only those few bars that are higher (or lower) than the pivot will need to be swapped. On average, for random data, about half the maximum number of swaps take place. \nAlthough there are fewer swaps than comparisons, they are both proportional to N. Thus, the partitioning process runs in O(N) time. Running the Workshop applet, you can see that for 12 random bars there are about 3 swaps and 14 comparisons, and for 100 random bars there are about 25 swaps and 102 comparisons. \nQuicksort \nQuicksort is undoubtedly the most popular sorting algorithm, and for good reason: In the majority of situations, it’s the fastest, operating in $mathrm { O ( N ^ { star } l o g N ) }$ time. (This is only true for internal or in-memory sorting; for sorting data in disk files, other algorithms may be better.) Quicksort was discovered by C.A.R. Hoare in 1962. \nTo understand quicksort, you should be familiar with the partitioning algorithm described in the preceding section. Basically, the quicksort algorithm operates by partitioning an array into two subarrays and then calling itself recursively to quicksort each of these subarrays. However, there are some embellishments we can make to this basic scheme. They have to do with the selection of the pivot and the sorting of small partitions. We’ll examine these refinements after we’ve looked at a simple version of the main algorithm. \nIt’s difficult to understand what quicksort is doing before you understand how it does it, so we’ll reverse our usual presentation and show the Java code for quicksort before presenting the QuickSort1 Workshop applet. \nThe Quicksort Algorithm \nThe code for a basic recursive quicksort method is fairly simple. Here’s an example: \npublic void recQuickSort(int left, int right) { if(right-left $< = ~ 0$ ) // if size is 1, return; // it’s already sorted else // size is 2 or larger { // partition range int partition $mathbf { tau } = mathbf { tau }$ partitionIt(left, right); recQuickSort(left, partition-1); // sort left side recQuickSort(partition+1, right);  // sort right side } } \nAs you can see, there are three basic steps: \n1. Partition the array or subarray into left (smaller keys) and right (larger keys) groups.",
      "metadata": {
        "content": "7 Advanced Sorting \nWe discussed simple sorting in the aptly titled Chapter 3, “Simple Sorting.” The sorts described there—the bubble, selection, and insertion sorts—are easy to implement but are rather slow. In Chapter 6, “Recursion,” we described the mergesort. It runs much faster than the simple sorts but requires twice as much space as the original array; this is often a serious drawback. \nThis chapter covers two advanced approaches to sorting: Shellsort and quicksort. These sorts both operate much faster than the simple sorts: the Shellsort in about $mathrm { O } ( mathrm { N } ^ { star } ( log mathrm { N } ) ^ { 2 } )$ time, and quicksort in $mathrm { O ( N ^ { star } l o g N ) }$ time. Neither of these sorts requires a large amount of extra space, as mergesort does. The Shellsort is almost as easy to implement as mergesort, while quicksort is the fastest of all the general-purpose sorts. We’ll conclude the chapter with a brief mention of the radix sort, an unusual and interesting approach to sorting. \nWe’ll examine the Shellsort first. Quicksort is based on the idea of partitioning, so we’ll then examine partitioning separately, before examining quicksort itself. \nShellsort \nThe Shellsort is named for Donald L. Shell, the computer scientist who discovered it in 1959. It’s based on the insertion sort, but adds a new feature that dramatically improves the insertion sort’s performance. \nThe Shellsort is good for medium-sized arrays, perhaps up to a few thousand items, depending on the particular implementation. It’s not quite as fast as quicksort and other $mathrm { O ( N ^ { star } l o g N ) }$ sorts, so it’s not optimum for very large files. However, it’s much faster than the $mathrm { O } ( mathrm { N } ^ { 2 } )$ sorts like the selection sort and the insertion sort, and it’s very easy to implement: The code is short and simple. \nIN THIS CHAPTER \n• Shellsort • Partitioning • Quicksort • Radix Sort \nThe worst-case performance is not significantly worse than the average performance. (We’ll see later in this chapter that the worst-case performance for quicksort can be much worse unless precautions are taken.) Some experts (see Sedgewick in Appendix B, “Further Reading”) recommend starting with a Shellsort for almost any sorting project and changing to a more advanced sort, like quicksort, only if Shellsort proves too slow in practice. \nInsertion Sort: Too Many Copies \nBecause Shellsort is based on the insertion sort, you might want to review the section titled “Insertion Sort” in Chapter 3. Recall that partway through the insertion sort the items to the left of a marker are internally sorted (sorted among themselves) and items to the right are not. The algorithm removes the item at the marker and stores it in a temporary variable. Then, beginning with the item to the left of the newly vacated cell, it shifts the sorted items right one cell at a time, until the item in the temporary variable can be reinserted in sorted order. \nHere’s the problem with the insertion sort. Suppose a small item is on the far right, where the large items should be. To move this small item to its proper place on the left, all the intervening items (between the place where it is and where it should be) must be shifted one space right. This step takes close to N copies, just for one item. Not all the items must be moved a full N spaces, but the average item must be moved $mathrm { N } / 2$ spaces, which takes $mathrm { Delta N }$ times $_ { mathrm { N } / 2 }$ shifts for a total of $mathrm { N } ^ { 2 } / 2$ copies. Thus, the performance of insertion sort is $mathrm { O } ( mathrm { N } ^ { 2 } )$ . \nThis performance could be improved if we could somehow move a smaller item many spaces to the left without shifting all the intermediate items individually. \nN-Sorting \nThe Shellsort achieves these large shifts by insertion-sorting widely spaced elements. After they are sorted, it sorts somewhat less widely spaced elements, and so on. The spacing between elements for these sorts is called the increment and is traditionally represented by the letter h. Figure 7.1 shows the first step in the process of sorting a 10-element array with an increment of 4. Here the elements 0, 4, and 8 are sorted. \nAfter 0, 4, and 8 are sorted, the algorithm shifts over one cell and sorts 1, 5, and 9. This process continues until all the elements have been 4-sorted, which means that all items spaced four cells apart are sorted among themselves. The process is shown (using a more compact visual metaphor) in Figure 7.2. \nAfter the complete 4-sort, the array can be thought of as comprising four subarrays: (0,4,8), (1,5,9), (2,6), and (3,7), each of which is completely sorted. These subarrays are interleaved but otherwise independent. \n  \n0 1 2 3 4 5 6 7 8 9   \n↑ ↑ Unsorted   \n0 m   \n↑ ↑ ↑ Sorted \nNotice that, in this particular example, at the end of the 4-sort no item is more than two cells from where it would be if the array were completely sorted. This is what is meant by an array being “almost” sorted and is the secret of the Shellsort. By creating interleaved, internally sorted sets of items, we minimize the amount of work that must be done to complete the sort. \nNow, as we noted in Chapter 3, the insertion sort is very efficient when operating on an array that’s almost sorted. If it needs to move items only one or two cells to sort the file, it can operate in almost O(N) time. Thus, after the array has been 4-sorted, we can 1-sort it using the ordinary insertion sort. The combination of the 4-sort and the 1-sort is much faster than simply applying the ordinary insertion sort without the preliminary 4-sort. \nDiminishing Gaps \nWe’ve shown an initial interval—or gap—of 4 cells for sorting a 10-cell array. For larger arrays the interval should start out much larger. The interval is then repeatedly reduced until it becomes 1. \nFor instance, an array of 1,000 items might be 364-sorted, then 121-sorted, then 40- sorted, then 13-sorted, then 4-sorted, and finally 1-sorted. The sequence of numbers used to generate the intervals (in this example, 364, 121, 40, 13, 4, 1) is called the interval sequence or gap sequence. The particular interval sequence shown here, \nattributed to Knuth (see Appendix B), is a popular one. In reversed form, starting from 1, it’s generated by the recursive expression \nwhere the initial value of h is 1. The first two columns of Table 7.1 show how this formula generates the sequence. \nFIGURE 7.2 A complete 4-sort. \nThere are other approaches to generating the interval sequence; we’ll return to this issue later. First, we’ll explore how the Shellsort works using Knuth’s sequence. \nIn the sorting algorithm, the sequence-generating formula is first used in a short loop to figure out the initial gap. A value of 1 is used for the first value of h, and the $scriptstyle mathrm { h = h } ^ { star } 3 + 1$ formula is applied to generate the sequence 1, 4, 13, 40, 121, 364, and so on. This process ends when the gap is larger than the array. For a 1,000-element array, the seventh number in the sequence, 1,093, is too large. Thus, we begin the sorting process with the sixth-largest number, creating a 364-sort. Then, each time through the outer loop of the sorting routine, we reduce the interval using the inverse of the formula previously given: \nThis is shown in the third column of Table 7.1. This inverse formula generates the reverse sequence 364, 121, 40, 13, 4, 1. Starting with 364, each of these numbers is used to n-sort the array. When the array has been 1-sorted, the algorithm is done. \nThe Shellsort Workshop Applet \nYou can use the Shellsort Workshop applet to see how this sort works. Figure 7.3 shows the applet after all the bars have been 4-sorted, just as the 1-sort begins. \nAs you single-step through the algorithm, you’ll notice that the explanation we gave in the preceding discussion is slightly simplified. The sequence for the 4-sort is not actually (0,4,8), (1,5,9), (2,6), and (3,7). Instead, the first two elements of each group of three are sorted first, then the first two elements of the second group, and so on. Once the first two elements of all the groups are sorted, the algorithm returns and sorts three-element groups. The actual sequence is (0,4), (1,5), (2,6), (3,7), (0,4,8), (1,5,9). \nIt might seem more obvious for the algorithm to 4-sort each complete subarray first—(0,4), (0,4,8), (1,5), (1,5,9), (2,6), (3,7)—but the algorithm handles the array indices more efficiently using the first scheme. \nThe Shellsort is actually not very efficient with only 10 items, making almost as many swaps and comparisons as the insertion sort. However, with 100 bars the improvement becomes significant. \nIt’s instructive to run the Workshop applet starting with 100 inversely sorted bars. (Remember that, as in Chapter 3, the first press of New creates a random sequence of bars, while the second press creates an inversely sorted sequence.) Figure 7.4 shows how the bars look after the first pass, when the array has been completely 40-sorted. Figure 7.5 shows the situation after the next pass, when it is 13-sorted. With each new value of $mathbf { h }$ , the array becomes more nearly sorted. \nWhy is the Shellsort so much faster than the insertion sort, on which it’s based? When h is large, the number of items per pass is small, and items move long distances. This is very efficient. As h grows smaller, the number of items per pass increases, but the items are already closer to their final sorted positions, which is more efficient for the insertion sort. It’s the combination of these trends that makes the Shellsort so effective. \n\nNotice that later sorts (small values of h) don’t undo the work of earlier sorts (large values of h). An array that has been 40-sorted remains 40-sorted after a 13-sort, for example. If this wasn’t so, the Shellsort couldn’t work. \nJava Code for the Shellsort \nThe Java code for the Shellsort is scarcely more complicated than for the insertion sort. Starting with the insertion sort, you substitute h for 1 in appropriate places and add the formula to generate the interval sequence. We’ve made shellSort() a method in the ArraySh class, a version of the array classes from Chapter 2, “Arrays.” Listing 7.1 shows the complete shellSort.java program. \nLISTING 7.1 The shellSort.java Program \n// shellSort.java   \n$bigstar / / bigstar$ demonstrates shell sort   \n$bigstar / / bigstar$ to run this program: $complement >$ java ShellSortApp   \n/   \nclass ArraySh { private long[] theArray; // ref to array theArray private int nElems; $bigstar / / bigstar$ number of data items \nLISTING 7.1 Continued \npublic ArraySh(int max) // constructor { theArray $mathbf { tau } = mathbf { tau }$ new long[max]; // create the array nElems $= 0$ ; // no items yet }   \n// public void insert(long value) // put element into array { theArray[nElems] $mathbf { Sigma } = mathbf { Sigma }$ value; $bigstar / / bigstar$ insert it nElems++; $bigstar / / bigstar$ increment size }   \n// public void display() $bigstar / / bigstar$ displays array contents { System.out.print ${ mathrm { ? } }  { mathsf { A } } =  \" mathbf { Gamma }$ ); for(int $scriptstyle mathbf { j } = 0$ ; j<nElems; ${ mathrm { j } } + +$ ) $bigstar / / bigstar$ for each element, System.out.print(theArray[j] $^ +$ “ “);  // display it System.out.println ${ bigl ( } { begin{array} { l } { ^ { mathfrak { u } } }  { } end{array} } { bigr ) }$ ; } public void shellSort() { int inner, outer; long temp; int ${ mathsf { h } }  =  { mathsf { 1 } }$ ; // find initial value of h while(h $< =$ nElems/3) ${ textsf { h } } = { mathsf { h } } ^ { star } 3 { mathsf { Omega } } + { mathsf { Omega } } 1$ ; // (1, 4, 13, 40, 121, ...) while $( n > 0 )$ // decreasing h, until $h = 1$ { // h-sort the file for(outer $cdot = h$ ; outer<nElems; outer $^ { cdot + + }$ ) { temp $mathbf { tau } = mathbf { tau }$ theArray[outer]; inner $mathbf { tau } = mathbf { tau }$ outer; $bigstar / / bigstar$ one subpass (eg 0, 4, 8) while(inner $>$ h-1 && theArray[inner-h] $> =$ temp) { theArray[inner] $mathbf { tau } = mathbf { tau }$ theArray[inner-h]; inner $cdot = mathsf { h }$ ; $}$ theArray[inner] $mathbf { tau } = mathbf { tau }$ temp; }  // end for ${ textsf { h } } = ( { mathsf { h } } - 1 ) / 3 ;$ ; // decrease h } // end while $( n > 0 )$ ) } // end shellSort()   \n//- } // end class ArraySh   \n////////////////////////////////////////////////////////////////   \nclass ShellSortApp { public static void main(String[] args) { int maxSize $= 1 0$ ; // array size ArraySh arr; arr $mathbf { tau } = mathbf { tau }$ new ArraySh(maxSize); // create the array for(int $scriptstyle mathbf { j } = 0$ ; j<maxSize; ${ mathrm { j } } + +$ ) $1 1$ fill array with { // random numbers long ${ mathfrak { n } } =$ (int)(java.lang.Math.random $( ) ^ { star } 9 9 $ ); arr.insert(n); } arr.display(); // display unsorted array arr.shellSort(); // shell sort the array arr.display(); // display sorted array }  // end main() }  // end class ShellSortApp \n\nIn main() we create an object of type ArraySh, able to hold 10 items, fill it with random data, display it, Shellsort it, and display it again. Here’s some sample output: \n$A = 2 0$ 89 6 42 55 59 41 69 75 66   \n$A = 6$ 20 41 42 55 59 66 69 75 89 \nYou can change maxSize to higher numbers, but don’t go too high; 10,000 items take a fraction of a minute to sort. \nThe Shellsort algorithm, although it’s implemented in just a few lines, is not simple to follow. To see the details of its operation, step through a 10-item sort with the Workshop applet, comparing the messages generated by the applet with the code in the shellSort() method. \nOther Interval Sequences \nPicking an interval sequence is a bit of a black art. Our discussion so far used the formula $scriptstyle mathrm { h = h } ^ { star } 3 + 1$ to generate the interval sequence, but other interval sequences have been used with varying degrees of success. The only absolute requirement is that the diminishing sequence ends with 1, so the last pass is a normal insertion sort. \nIn Shell’s original paper, he suggested an initial gap of $mathrm { N } / 2$ , which was simply divided in half for each pass. Thus, the descending sequence for $_ { mathrm { N } = 1 0 0 }$ is 50, 25, 12, 6, 3, 1. This approach has the advantage that you don’t need to calculate the sequence before the sort begins to find the initial gap; you just divide N by 2. However, this turns out not to be the best sequence. Although it’s still better than the insertion sort for most data, it sometimes degenerates to $mathrm { O } ( mathrm { N } ^ { 2 } )$ running time, which is no better than the insertion sort. \nA variation of this approach is to divide each interval by 2.2 instead of 2. For $scriptstyle mathrm { n = 1 0 0 }$ this leads to 45, 20, 9, 4, 1. This is considerably better than dividing by 2, as it avoids some worst-case circumstances that lead to $mathrm { O } ( mathrm { N } ^ { 2 } )$ behavior. Some extra code is needed to ensure that the last value in the sequence is 1, no matter what N is. This gives results comparable to Knuth’s sequence shown in the listing. \nAnother possibility for a descending sequence (from Flamig; see Appendix B) is \n$begin{array} { r } { mathrm { i f ~ ( mathsf { h } ~ leqslant ~ 5 ) ~ } }  { mathsf { h } ~ mathsf { = } ~ 1 ; } end{array}$   \nelse $h = ( 5 times h - 1 ) / 1 1 ;$ \nIt’s generally considered important that the numbers in the interval sequence are relatively prime; that is, they have no common divisors except 1. This constraint makes it more likely that each pass will intermingle all the items sorted on the previous pass. The inefficiency of Shell’s original N/2 sequence is due to its failure to adhere to this rule. \nYou may be able to invent a gap sequence of your own that does just as well (or possibly even better) than those shown. Whatever it is, it should be quick to calculate so as not to slow down the algorithm. \nEfficiency of the Shellsort \nNo one so far has been able to analyze the Shellsort’s efficiency theoretically, except in special cases. Based on experiments, there are various estimates, which range from $0 ( mathrm { N } ^ { 3 / 2 } )$ down to $mathrm { O } ( mathrm { N } ^ { 7 / 6 } )$ . \nTable 7.2 shows some of these estimated O() values, compared with the slower insertion sort and the faster quicksort. The theoretical times corresponding to various values of $mathrm { Delta N }$ are shown. Note that $mathrm { N ^ { x / y } }$ means the yth root of N raised to the x power. Thus, if $mathrm { Delta N }$ is 100, $mathrm { N } ^ { 3 / 2 }$ is the square root of $1 0 0 ^ { 3 }$ , which is 1,000. Also, $( mathrm { l o g N } ) ^ { 2 }$ means the log of $mathrm { N _ { it } }$ , squared. This is often written $log ^ { 2 } mathrm { N } _ { cdot }$ , but that’s easy to confuse with $log _ { 2 } mathrm { N } _ { 1 }$ , the logarithm to the base 2 of N. \nFor most data, the higher estimates, such as $mathrm { N } ^ { 3 / 2 }$ , are probably more realistic. \nPartitioning \nPartitioning is the underlying mechanism of quicksort, which we’ll explore next, but it’s also a useful operation on its own, so we’ll cover it here in its own section. \nTo partition data is to divide it into two groups, so that all the items with a key value higher than a specified amount are in one group, and all the items with a lower key value are in another. \nYou can easily imagine situations in which you would want to partition data. Maybe you want to divide your personnel records into two groups: employees who live within 15 miles of the office and those who live farther away. Or a school administrator might want to divide students into those with grade point averages higher and lower than 3.5, so as to know who deserves to be on the Dean’s list. \nThe Partition Workshop Applet \nOur Partition Workshop applet demonstrates the partitioning process. Figure 7.6 shows 12 bars before partitioning, and Figure 7.7 shows them again after partitioning. \nApplet Viewer:partition.class -□   \nApplet   \nSwaps=0 New Size Draw Run Step   \nComparisons =0 tt Pivot value is 104   \nApplet started \nThe horizontal line represents the pivot value, which is the value used to determine into which of the two groups an item is placed. Items with a key value less than the pivot value go in the left part of the array, and those with a greater (or equal) key go in the right part. (In the section on quicksort, we’ll see that the pivot value can be the key value of an actual data item, called the pivot. For now, it’s just a number.) \nThe arrow labeled partition points to the leftmost item in the right (higher) subarray. This value is returned from the partitioning method, so it can be used by other methods that need to know where the division is. \nFor a more vivid display of the partitioning process, set the Partition Workshop applet to 100 bars and press the Run button. The leftScan and rightScan pointers will zip toward each other, swapping bars as they go. When they meet, the partition is complete. \nYou can choose any value you want for the pivot value, depending on why you’re doing the partition (such as choosing a grade point average of 3.5). For variety, the Workshop applet chooses a random number for the pivot value (the horizontal black line) each time New or Size is pressed, but the value is never too far from the average bar height. \nAfter being partitioned, the data is by no means sorted; it has simply been divided into two groups. However, it’s more sorted than it was before. As we’ll see in the next section, it doesn’t take much more trouble to sort it completely. \nNotice that partitioning is not stable. That is, each group is not in the same order it was originally. In fact, partitioning tends to reverse the order of some of the data in each group. \nThe partition.java Program \nHow is the partitioning process carried out? Let’s look at some example code. Listing 7.2 shows the partition.java program, which includes the partitionIt() method for partitioning an array. \nLISTING 7.2 The partition.java Program \n// partition.java   \n// demonstrates partitioning an array   \n// to run this program: $complement >$ java PartitionApp   \n//////////////////////////////////////////// ////   \nclass ArrayPar { private long[] theArray; // ref to array theArray private int nElems; // number of data items public ArrayPar(int max) // constructor { theArray $mathbf { tau } = mathbf { tau }$ new long[max]; $bigstar / / bigstar$ create the array nElems $mathit { Theta } = mathit { Theta } varnothing$ ; $bigstar / / bigstar$ no items yet } public void insert(long value) // put element into array { theArray[nElems] $mathbf { tau } = mathbf { tau }$ value; // insert it \nLISTING 7.2 Continued \nnElems++; // increment size }   \n// public int size() // return number of items { return nElems; }   \n//- public void display() // displays array contents { System.out.print ${ } ^ { \" } { mathsf { A } } = \"$ ); for(int $scriptstyle mathbf { j } = 0$ ; j<nElems; ${ mathrm { j } } + +$ ) // for each element, System.out.print(theArray[j] $mathbf { Sigma } ^ { } + mathbf { Sigma } ^ { \" } mathbf { Sigma } ^ { } mathbf { Sigma } ^ { } )$ ;  // display it System.out.println ${ bigl ( } { begin{array} { l } { mathfrak { u }  mathfrak { u } }  { mathfrak { u } } end{array} } { bigr ) }$ ; }   \n// public int partitionIt(int left, int right, long pivot) { int leftPtr $mathbf { tau } = mathbf { tau }$ left - 1; // right of first elem int rightPtr $mathbf { tau } = mathbf { tau }$ right $^ +$ 1; // left of pivot while(true) { while(leftPtr $mathit { Theta } <$ right && // find bigger item theArray[++leftPtr] $mathit { Theta } _ { < }$ pivot) ;  // (nop) while(rightPtr $>$ left && // find smaller item theArray[--rightPtr] $>$ pivot) ;  // (nop) if(leftPtr $> =$ rightPtr) // if pointers cross, break; // partition done else // not crossed, so swap(leftPtr, rightPtr); // swap elements }  // end while(true) return leftPtr; // return partition }  // end partitionIt()   \n//-- public void swap(int dex1, int dex2) // swap two elements { long temp; temp $mathbf { tau } = mathbf { tau }$ theArray[dex1]; // A into temp theArray[dex1] $mathbf { tau } = mathbf { tau }$ theArray[dex2]; // B into A theArray[dex2] $mathbf { tau } = mathbf { tau }$ temp; // temp into B } // end swap()   \n//- } // end class ArrayPar   \n//////////////////////////////////////// /////////////////   \nclass PartitionApp { public static void main(String[] args) { int maxSize $mathbf { sigma } = mathbf { sigma }$ 16; // array size ArrayPar arr; $1 1$ reference to array arr $mathbf { tau } = mathbf { tau }$ new ArrayPar(maxSize); $1 1$ create the array for(int $scriptstyle mathbf { j } = 0$ ; j<maxSize; ${ mathrm { j } } + +$ )  // fill array with { // random numbers long ${ mathfrak { n } } =$ (int)(java.lang.Math.random $( ) star 1 9 9 $ ); arr.insert(n); } arr.display(); // display unsorted array long pivot $mathbf { mathfrak { s } } _ { 9 9 }$ ; // pivot value System.out.print(“Pivot is “ $^ +$ pivot); int size $mathbf { tau } = mathbf { tau }$ arr.size(); // partition array int partDex $mathbf { tau } = mathbf { tau }$ arr.partitionIt(0, size-1, pivot); System.out.println(“, Partition is at index “ $^ +$ partDex); arr.display(); // display partitioned array }  // end main() \n\nThe main() routine creates an ArrayPar object that holds 16 items of type long. The pivot value is fixed at 99. The routine inserts 16 random values into ArrayPar, displays them, partitions them by calling the partitionIt() method, and displays them again. Here’s some sample output: \n$A = 1 4 9$ 192 47 152 159 195 61 66 17 167 118 64 27 80 30 105   \nPivot is 99, partition is at index 8   \n$A = 3 0$ 80 47 27 64 17 61 66 195 167 118 159 152 192 149 105 \nYou can see that the partition is successful: The first eight numbers are all smaller than the pivot value of 99; the last eight are all larger. \nNotice that the partitioning process doesn’t necessarily divide the array in half as it does in this example; that depends on the pivot value and key values of the data. There may be many more items in one group than in the other. \nThe Partition Algorithm \nThe partitioning algorithm works by starting with two pointers, one at each end of the array. (We use the term pointers to mean indices that point to array elements, not $mathrm { C } { + } { + }$ pointers.) The pointer on the left, leftPtr, moves toward the right, and the one on the right, rightPtr, moves toward the left. Notice that leftPtr and rightPtr in the partition.java program correspond to leftScan and rightScan in the Partition Workshop applet. \nActually, leftPtr is initialized to one position to the left of the first cell, and rightPtr to one position to the right of the last cell, because they will be incremented and decremented, respectively, before they’re used. \nStopping and Swapping \nWhen leftPtr encounters a data item smaller than the pivot value, it keeps going because that item is already on the correct side of the array. However, when it encounters an item larger than the pivot value, it stops. Similarly, when rightPtr encounters an item larger than the pivot, it keeps going, but when it finds a smaller item, it also stops. Two inner while loops, the first for leftPtr and the second for rightPtr, control the scanning process. A pointer stops because its while loop exits. Here’s a simplified version of the code that scans for out-of-place items: \nwhile( theArray[ $^ { + + }$ leftPtr] $mathbf { Sigma } <$ pivot ) // find bigger item ;  // (nop)   \nwhile( theArray[--rightPtr] $>$ pivot ) // find smaller item ;  // (nop)   \nswap(leftPtr, rightPtr); // swap elements \nThe first while loop exits when an item larger than pivot is found; the second loop exits when an item smaller than pivot is found. When both these loops exit, both leftPtr and rightPtr point to items that are in the wrong sides of the array, so these items are swapped. \nAfter the swap, the two pointers continue on, again stopping at items that are in the wrong side of the array and swapping them. All this activity is nested in an outer while loop, as can be seen in the partitionIt() method in Listing 7.2. When the two pointers eventually meet, the partitioning process is complete and this outer while loop exits. \nYou can watch the pointers in action when you run the Partition Workshop applet with 100 bars. These pointers, represented by blue arrows, start at opposite ends of the array and move toward each other, stopping and swapping as they go. The bars between them are unpartitioned; those they’ve already passed over are partitioned. When they meet, the entire array is partitioned. \n\nHandling Unusual Data \nIf we were sure that there was a data item at the right end of the array that was smaller than the pivot value, and an item at the left end that was larger, the simplified while loops previously shown would work fine. Unfortunately, the algorithm may be called upon to partition data that isn’t so well organized. \nIf all the data is smaller than the pivot value, for example, the leftPtr variable will go all the way across the array, looking in vain for a larger item, and fall off the right end, creating an array index out of bounds exception. A similar fate will befall rightPtr if all the data is larger than the pivot value. \nTo avoid these problems, extra tests must be placed in the while loops to check for the ends of the array: leftPtr<right in the first loop and rightPtr>left in the second. You can see these tests in context in Listing 7.2. \nIn the section on quicksort, we’ll see that a clever pivot-selection process can eliminate these end-of-array tests. Eliminating code from inner loops is always a good idea if you want to make a program run faster. \nDelicate Code \nThe code in the while loops is rather delicate. For example, you might be tempted to remove the increment operators from the inner while loops and use them to replace the nop statements. (Nop refers to a statement consisting only of a semicolon, and means no operation). For example, you might try to change this: \nwhile(leftPtr $mathit { Delta } <$ right && theArray[++leftPtr] $mathit { Theta } _ { < }$ pivot) ;  // (nop) \nto this: \nwhile(leftPtr $angle { mathbf { xi } } < mathbf { xi }$ right && theArray[leftPtr] $angle { mathbf { xi } } < mathbf { xi }$ pivot) ++leftPtr; \nand similarly for the other inner while loop. These changes would make it possible for the initial values of the pointers to be left and right, which is somewhat clearer than left-1 and right $+ 1$ . \nHowever, these changes result in the pointers being incremented only when the condition is satisfied. The pointers must move in any case, so two extra statements within the outer while loop would be required to bump the pointers. The nop version is the most efficient solution. \nEqual Keys \nHere’s another subtle change you might be tempted to make in the partitionIt() code. If you run the partitionIt() method on items that are all equal to the pivot value, you will find that every comparison leads to a swap. Swapping items with equal keys seems like a waste of time. The $<$ and $>$ operators that compare pivot with the array elements in the while loops cause the extra swapping. However, suppose you try to fix this by replacing them with $< =$ and $> =$ operators. This indeed prevents the swapping of equal elements, but it also causes leftPtr and rightPtr to end up at the ends of the array when the algorithm has finished. As we’ll see in the section on quicksort, it’s good for the pointers to end up in the middle of the array, and very bad for them to end up at the ends. So if partitionIt() is going to be used for quicksort, the $<$ and $>$ operators are the right way to go, even if they cause some unnecessary swapping. \nEfficiency of the Partition Algorithm \nThe partition algorithm runs in O(N) time. It’s easy to see why this is so when running the Partition Workshop applet: The two pointers start at opposite ends of the array and move toward each other at a more or less constant rate, stopping and swapping as they go. When they meet, the partition is complete. If there were twice as many items to partition, the pointers would move at the same rate, but they would have twice as many items to compare and swap, so the process would take twice as long. Thus, the running time is proportional to N. \nMore specifically, for each partition there will be $_ { mathrm { N + 1 } }$ or $_ { mathrm { N } + 2 }$ comparisons. Every item will be encountered and used in a comparison by one or the other of the pointers, leading to N comparisons, but the pointers overshoot each other before they find out they’ve “crossed” or gone beyond each other, so there are one or two extra comparisons before the partition is complete. The number of comparisons is independent of how the data is arranged (except for the uncertainty between one or two extra comparisons at the end of the scan). \nThe number of swaps, however, does depend on how the data is arranged. If it’s inversely ordered, and the pivot value divides the items in half, then every pair of values must be swapped, which is $_ { mathrm { N } / 2 }$ swaps. (Remember in the Partition Workshop applet that the pivot value is selected randomly, so that the number of swaps for inversely sorted bars won’t always be exactly $_ { mathrm { N } / 2 }$ .) \nFor random data, there will be fewer than $_ { mathrm { N } / 2 }$ swaps in a partition, even if the pivot value is such that half the bars are shorter and half are taller. This is because some bars will already be in the right place (short bars on the left, tall bars on the right). If the pivot value is higher (or lower) than most of the bars, there will be even fewer swaps because only those few bars that are higher (or lower) than the pivot will need to be swapped. On average, for random data, about half the maximum number of swaps take place. \nAlthough there are fewer swaps than comparisons, they are both proportional to N. Thus, the partitioning process runs in O(N) time. Running the Workshop applet, you can see that for 12 random bars there are about 3 swaps and 14 comparisons, and for 100 random bars there are about 25 swaps and 102 comparisons. \nQuicksort \nQuicksort is undoubtedly the most popular sorting algorithm, and for good reason: In the majority of situations, it’s the fastest, operating in $mathrm { O ( N ^ { star } l o g N ) }$ time. (This is only true for internal or in-memory sorting; for sorting data in disk files, other algorithms may be better.) Quicksort was discovered by C.A.R. Hoare in 1962. \nTo understand quicksort, you should be familiar with the partitioning algorithm described in the preceding section. Basically, the quicksort algorithm operates by partitioning an array into two subarrays and then calling itself recursively to quicksort each of these subarrays. However, there are some embellishments we can make to this basic scheme. They have to do with the selection of the pivot and the sorting of small partitions. We’ll examine these refinements after we’ve looked at a simple version of the main algorithm. \nIt’s difficult to understand what quicksort is doing before you understand how it does it, so we’ll reverse our usual presentation and show the Java code for quicksort before presenting the QuickSort1 Workshop applet. \nThe Quicksort Algorithm \nThe code for a basic recursive quicksort method is fairly simple. Here’s an example: \npublic void recQuickSort(int left, int right) { if(right-left $< = ~ 0$ ) // if size is 1, return; // it’s already sorted else // size is 2 or larger { // partition range int partition $mathbf { tau } = mathbf { tau }$ partitionIt(left, right); recQuickSort(left, partition-1); // sort left side recQuickSort(partition+1, right);  // sort right side } } \nAs you can see, there are three basic steps: \n1. Partition the array or subarray into left (smaller keys) and right (larger keys) groups.",
        "chapter": "CHAPTER 7 Advanced Sorting",
        "section": "Shellsort",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 13,
        "node_index": 68
      }
    },
    {
      "node_id": "tb13_node69",
      "content": "Although there are fewer swaps than comparisons, they are both proportional to N. Thus, the partitioning process runs in O(N) time. Running the Workshop applet, you can see that for 12 random bars there are about 3 swaps and 14 comparisons, and for 100 random bars there are about 25 swaps and 102 comparisons. \nQuicksort \nQuicksort is undoubtedly the most popular sorting algorithm, and for good reason: In the majority of situations, it’s the fastest, operating in $mathrm { O ( N ^ { star } l o g N ) }$ time. (This is only true for internal or in-memory sorting; for sorting data in disk files, other algorithms may be better.) Quicksort was discovered by C.A.R. Hoare in 1962. \nTo understand quicksort, you should be familiar with the partitioning algorithm described in the preceding section. Basically, the quicksort algorithm operates by partitioning an array into two subarrays and then calling itself recursively to quicksort each of these subarrays. However, there are some embellishments we can make to this basic scheme. They have to do with the selection of the pivot and the sorting of small partitions. We’ll examine these refinements after we’ve looked at a simple version of the main algorithm. \nIt’s difficult to understand what quicksort is doing before you understand how it does it, so we’ll reverse our usual presentation and show the Java code for quicksort before presenting the QuickSort1 Workshop applet. \nThe Quicksort Algorithm \nThe code for a basic recursive quicksort method is fairly simple. Here’s an example: \npublic void recQuickSort(int left, int right) { if(right-left $< = ~ 0$ ) // if size is 1, return; // it’s already sorted else // size is 2 or larger { // partition range int partition $mathbf { tau } = mathbf { tau }$ partitionIt(left, right); recQuickSort(left, partition-1); // sort left side recQuickSort(partition+1, right);  // sort right side } } \nAs you can see, there are three basic steps: \n1. Partition the array or subarray into left (smaller keys) and right (larger keys) groups. \n2. Call ourselves to sort the left group.   \n3. Call ourselves again to sort the right group. \nAfter a partition, all the items in the left subarray are smaller than all those on the right. If we then sort the left subarray and sort the right subarray, the entire array will be sorted. How do we sort these subarrays? By calling ourself recursively. \nThe arguments to the recQuickSort() method determine the left and right ends of the array (or subarray) it’s supposed to sort. The method first checks if this array consists of only one element. If so, the array is by definition already sorted, and the method returns immediately. This is the base case in the recursion process. \nIf the array has two or more cells, the algorithm calls the partitionIt() method, described in the preceding section, to partition it. This method returns the index number of the partition: the left element in the right (larger keys) subarray. The partition marks the boundary between the subarrays. This situation is shown in Figure 7.8. \nAfter the array is partitioned, recQuickSort() calls itself recursively, once for the left part of its array, from left to partition-1, and once for the right, from partition+1 to right. Note that the data item at the index partition is not included in either of the recursive calls. Why not? Doesn’t it need to be sorted? The explanation lies in how the pivot value is chosen. \nChoosing a Pivot Value \nWhat pivot value should the partitionIt() method use? Here are some relevant ideas: \n• The pivot value should be the key value of an actual data item; this item is called the pivot.   \n• You can pick a data item to be the pivot more or less at random. For simplicity, let’s say we always pick the item on the right end of the subarray being partitioned.   \n• After the partition, if the pivot is inserted at the boundary between the left and right subarrays, it will be in its final sorted position. \nThis last point may sound unlikely, but remember that, because the pivot’s key value is used to partition the array, following the partition the left subarray holds items smaller than the pivot, and the right subarray holds items larger. The pivot starts out on the right, but if it could somehow be placed between these two subarrays, it would be in the correct place—that is, in its final sorted position. Figure 7.9 shows how this looks with a pivot whose key value is 36. \nThis figure is somewhat fanciful because you can’t actually take an array apart as we’ve shown. So how do we move the pivot to its proper place? \nWe could shift all the items in the right subarray to the right one cell to make room for the pivot. However, this is inefficient and unnecessary. Remember that all the \nitems in the right subarray, although they are larger than the pivot, are not yet sorted, so they can be moved around, within the right subarray, without affecting anything. Therefore, to simplify inserting the pivot in its proper place, we can simply swap the pivot (36) and the left item in the right subarray, which is 63. This swap places the pivot in its proper position between the left and right groups. The 63 is switched to the right end, but because it remains in the right (larger) group, the partitioning is undisturbed. This situation is shown in Figure 7.10. \nWhen it’s swapped into the partition’s location, the pivot is in its final resting place. All subsequent activity will take place on one side of it or on the other, but the pivot itself won’t be moved (or indeed even accessed) again. \nTo incorporate the pivot selection process into our recQuickSort() method, let’s make it an overt statement, and send the pivot value to partitionIt() as an argument. Here’s how that looks: \npublic void recQuickSort(int left, int right) { if(right-left $< =  0  d$ ) // if size $< = ~ 1$ , return; $bigstar / / bigstar$ already sorted else $bigstar / / bigstar$ size is 2 or larger { long pivot $mathbf { tau } = mathbf { tau }$ theArray[right]; // rightmost item // partition range int partition $mathbf { tau } = mathbf { tau }$ partitionIt(left, right, pivot); \nrecQuickSort(left, partition-1); // sort left side recQuickSort(partition+1, right);  // sort right side } }  // end recQuickSort() \nWhen we use this scheme of choosing the rightmost item in the array as the pivot, we’ll need to modify the partitionIt() method to exclude this rightmost item from the partitioning process; after all, we already know where it should go after the partitioning process is complete: at the partition, between the two groups. Also, after the partitioning process is completed, we need to swap the pivot from the right end into the partition’s location. Listing 7.3 shows the quickSort1.java program, which incorporates these features. \n// quickSort1.java   \n// demonstrates simple version of quick sort   \n// to run this program: $complement >$ java QuickSort1App   \n////////////////////////////////////////////////////////////////   \nclass ArrayIns { private long[] theArray; // ref to array theArray private int nElems; // number of data items   \n// public ArrayIns(int max) // constructor { theArray $mathbf { tau } = mathbf { tau }$ new long[max]; $bigstar / / bigstar$ create the array nElems $mathit { Theta } = mathit { Theta } varnothing$ ; $bigstar / / bigstar$ no items yet } public void insert(long value) // put element into array { theArray[nElems] $mathbf { tau } = mathbf { tau }$ value; $bigstar / / bigstar$ insert it nElems $^ { + + }$ ; $bigstar / / bigstar$ increment size } public void display() $bigstar / / bigstar$ displays array contents { System.out.print $\" A = \"$ ); for(int $scriptstyle mathbf { j } = 0$ ; j<nElems; ${ mathrm { j } } + +$ ) // for each element, System.out.print(theArray[j] $mathrm { ~  ~ { ~ mathsf ~ { ~ pi ~ } ~ } ~ } + mathrm { ~  ~ { ~ mathsf ~ { ~ pi ~ } ~ } ~ } ^ { mathsf { pi } } mathsf { Lambda } ^ { mathsf { pi } } )$ );  // display it System.out.println(“”); } \nLISTING 7.3 Continued \n// public void quickSort() { recQuickSort(0, nElems-1); }   \n/ public void recQuickSort(int left, int right) { if(right-left $scriptstyle < =  0$ ) // if size $< = ~ 1$ , return; $bigstar / / bigstar$ already sorted else // size is 2 or larger { long pivot $mathbf { tau } = mathbf { tau }$ theArray[right]; // rightmost item // partition range int partition $mathbf { tau } = mathbf { tau }$ partitionIt(left, right, pivot); recQuickSort(left, partition $^ { - 1 }$ ); // sort left side recQuickSort(partition+1, right);  // sort right side } }  // end recQuickSort()   \n// public int partitionIt(int left, int right, long pivot) { int leftPtr $mathbf { varepsilon } = mathbf { varepsilon } _ { 1 }$ eft-1; // left (after $^ { + + }$ ) int rightPtr $mathbf { tau } = mathbf { tau }$ right; // right-1 (after --) while(true) { // find bigger item while( theArray[++leftPtr] $mathit { Theta } < mathit { Theta }$ pivot ) ;  // (nop) // find smaller item while(rightPtr > 0 && theArray[--rightPtr] $>$ pivot) ;  // (nop) if(leftPtr $> =$ rightPtr) $bigstar / / bigstar$ if pointers cross, break; $bigstar / / bigstar$ partition done else $bigstar / / bigstar$ not crossed, so swap(leftPtr, rightPtr);  // swap elements }  // end while(true) swap(leftPtr, right); // restore pivot return leftPtr; // return pivot location }  // end partitionIt()   \n//- public void swap(int dex1, int dex2)  // swap two elements { long temp $mathbf { tau } = mathbf { tau }$ theArray[dex1]; // A into temp theArray[dex1] $mathbf { tau } = mathbf { tau }$ theArray[dex2]; // B into A theArray[dex2] $mathbf { tau } = mathbf { tau }$ temp; // temp into B }  // end swap(   \n//- }  // end class ArrayIns   \n////////////////////////////////////// ////////////////   \nclass QuickSort1App { public static void main(String[] args) { int maxSize = 16; // array size ArrayIns arr; arr $mathbf { tau } = mathbf { tau }$ new ArrayIns(maxSize);  // create array for(int $scriptstyle mathbf { j } = 0$ ; j<maxSize; ${ mathrm { j } } + +$ ) $1 1$ fill array with { // random numbers long ${ mathfrak { n } } =$ (int)(java.lang.Math.random $( ) ^ { star } 9 9 ^ { prime }$ ); arr.insert(n); } arr.display(); // display items arr.quickSort(); // quicksort them arr.display(); // display them again }  // end main() }  // end class QuickSort1App \n\nThe main() routine creates an object of type ArrayIns, inserts 16 random data items of type long in it, displays it, sorts it with the quickSort() method, and displays the results. Here’s some typical output: \n$A = 6 9$ 0 70 6 38 38 24 56 44 26 73 77 30 45 97 65   \n$A = 0$ 6 24 26 30 38 38 44 45 56 65 69 70 73 77 97 \nAn interesting aspect of the code in the partitionIt() method is that we’ve been able to remove the test for the end of the array in the first inner while loop. This test, seen in the earlier partitionIt() method in the partition.java program in Listing 7.2, was \nleftPtr $angle { mathbf { xi } } < mathbf { xi }$ right \nIt prevented leftPtr running off the right end of the array if no item there was larger than pivot. Why can we eliminate the test? Because we selected the rightmost item as the pivot, so leftPtr will always stop there. However, the test is still necessary for rightPtr in the second while loop. (Later we’ll see how this test can be eliminated as well.) \nChoosing the rightmost item as the pivot is thus not an entirely arbitrary choice; it speeds up the code by removing an unnecessary test. Picking the pivot from some other location would not provide this advantage. \nThe QuickSort1 Workshop Applet \nAt this point you know enough about the quicksort algorithm to understand the nuances of the QuickSort1 Workshop applet. \nThe Big Picture \nFor the big picture, use the Size button to set the applet to sort 100 random bars, and press the Run button. Following the sorting process, the display will look something like Figure 7.11. \nWatch how the algorithm partitions the array into two parts, then sorts each of these parts by partitioning it into two parts, and so on, creating smaller and smaller subarrays. \nWhen the sorting process is complete, each dotted line provides a visual record of one of the sorted subarrays. The horizontal range of the line shows which bars were part of the subarray, and its vertical position is the pivot value (the height of the \npivot). The total length of all these lines on the display is a measure of how much work the algorithm has done to sort the array; we’ll return to this topic later. \nEach dotted line (except the shortest ones) should have a line below it (probably separated by other, shorter lines) and a line above it that together add up to the same length as the original line (less one bar). These are the two partitions into which each subarray is divided. \nThe Details \nFor a more detailed examination of quicksort’s operation, switch to the 12-bar display in the QuickSort1 Workshop applet and step through the sorting process. You’ll see how the pivot value corresponds to the height of the pivot on the right side of the array and how the algorithm partitions the array, swaps the pivot into the space between the two sorted groups, sorts the shorter group (using many recursive calls), and then sorts the larger group. \nFigure 7.12 shows all the steps involved in sorting 12 bars. The horizontal brackets under the arrays show which subarray is being partitioned at each step, and the circled numbers show the order in which these partitions are created. A pivot being swapped into place is shown with a dotted arrow. The final position of the pivot is shown as a dotted cell to emphasize that this cell contains a sorted item that will not be changed thereafter. Horizontal brackets under single cells (steps 5, 6, 7, 11, and 12) are base case calls to recQuickSort(); they return immediately. \nSometimes, as in steps 4 and 10, the pivot ends up in its original position on the right side of the array being sorted. In this situation, there is only one subarray remaining to be sorted: the one to the left of the pivot. There is no second subarray to its right. \nThe different steps in Figure 7.12 occur at different levels of recursion, as shown in Table 7.3. The initial call from main() to recQuickSort() is the first level, recQuickSort() calling two new instances of itself is the second level, these two instances calling four more instances is the third level, and so on. \nThe order in which the partitions are created, corresponding to the step numbers, does not correspond with depth. It’s not the case that all the first-level partitions are done first, then all the second level ones, and so on. Instead, the left group at every level is handled before any of the right groups. \nIn theory there should be 8 steps in the fourth level and 16 in the fifth level, but in this small array we run out of items before these steps are necessary. \nThe number of levels in the table shows that with 12 data items, the machine stack needs enough space for 5 sets of arguments and return values; one for each recursion level. This is, as we’ll see later, somewhat greater than the logarithm to the base 2 of the number of items: $log _ { 2 } mathrm { N }$ . The size of the machine stack is determined by your particular system. Sorting very large numbers of data items using recursive procedures may cause this stack to overflow, leading to memory errors. \nThings to Notice \nHere are some details you may notice as you run the QuickSort1 Workshop applet. \nYou might think that a powerful algorithm like quicksort would not be able to handle subarrays as small as two or three items. However, this version of the quicksort algorithm is quite capable of sorting such small subarrays; leftScan and rightScan just don’t go very far before they meet. For this reason we don’t need to use a different sorting scheme for small subarrays. (Although, as we’ll see later, handling small subarrays differently may have advantages.) \nAt the end of each scan, the leftScan variable ends up pointing to the partition— that is, the left element of the right subarray. The pivot is then swapped with the partition to put the pivot in its proper place, as we’ve seen. As we noted, in steps 3 and 9 of Figure 7.12, leftScan ends up pointing to the pivot itself, so the swap has no effect. This may seem like a wasted swap; you might decide that leftScan should stop one bar sooner. However, it’s important that leftScan scan all the way to the pivot; otherwise, a swap would unsort the pivot and the partition. \nBe aware that leftScan and rightScan start at left-1 and right. This may look peculiar on the display, especially if left is 0; then leftScan will start at –1. Similarly, rightScan initially points to the pivot, which is not included in the partitioning process. These pointers start outside the subarray being partitioned because they will be incremented and decremented, respectively, before they’re used the first time. \nThe applet shows ranges as numbers in parentheses; for example, (2-5) means the subarray from index 2 to index 5. The range given in some of the messages may be negative: from a higher number to a lower one, such as Array partitioned; left (7-6), right (8-8). The (8-8) range means a single cell (8), but what does (7-6) mean? This range isn’t real; it simply reflects the values that left and right, the arguments to recQuickSort(), have when this method is called. Here’s the code in question: \nint partition $mathbf { tau } = mathbf { tau }$ partitionIt(left, right, pivot); recQuickSort(left, partition-1);   // sort left side recQuickSort(partition+1, right);  // sort right side \nIf partitionIt() is called with $mathtt { l e f t } = 7$ and right $= 8$ , for example, and happens to return 7 as the partition, then the range supplied in the first call to recQuickSort() will be (7-6) and the range to the second will be (8-8). This is normal. The base case in recQuickSort() is activated by array sizes less than 1 as well as by 1, so it will return immediately for negative ranges. Negative ranges are not shown in Figure 7.12, although they do cause (brief) calls to recQuickSort(). \nDegenerates to $mathsf { O } ( mathsf { N } ^ { 2 } )$ Performance \nIf you use the QuickSort1 Workshop applet to sort 100 inversely sorted bars, you’ll see that the algorithm runs much more slowly and that many more dotted horizontal lines are generated, indicating more and larger subarrays are being partitioned. What’s happening here? \nThe problem is in the selection of the pivot. Ideally, the pivot should be the median of the items being sorted. That is, half the items should be larger than the pivot, and half smaller. This would result in the array being partitioned into two subarrays of equal size. Having two equal subarrays is the optimum situation for the quicksort algorithm. If it has to sort one large and one small array, it’s less efficient because the larger subarray has to be subdivided more times. \nThe worst situation results when a subarray with N elements is divided into one subarray with 1 element and the other with N-1 elements. (This division into 1 cell and N-1 cells can also be seen in steps 3 and 9 in Figure 7.12.) If this 1 and N-1 division happens with every partition, then every element requires a separate partition step. This is in fact what takes place with inversely sorted data: In all the subarrays, the pivot is the smallest item, so every partition results in N-1 elements in one subarray and only the pivot in the other. \nTo see this unfortunate process in action, step through the QuickSort1 Workshop applet with 12 inversely sorted bars. Notice how many more steps are necessary than with random data. In this situation the advantage gained by the partitioning process is lost and the performance of the algorithm degenerates to $mathrm { O } ( mathrm { N } ^ { 2 } )$ . \nBesides being slow, there’s another potential problem when quicksort operates in $mathrm { O } ( mathrm { N } ^ { 2 } )$ time. When the number of partitions increases, the number of recursive function calls also increases. Every function call takes up room on the machine stack. If there are too many calls, the machine stack may overflow and paralyze the system. \nTo summarize: In the QuickSort1 applet, we select the rightmost element as the pivot. If the data is truly random, this isn’t too bad a choice because usually the pivot won’t be too close to either end of the array. However, when the data is sorted or inversely sorted, choosing the pivot from one end or the other is a bad idea. Can we improve on our approach to selecting the pivot? \n\nMedian-of-Three Partitioning \nMany schemes have been devised for picking a better pivot. The method should be simple but have a good chance of avoiding the largest or smallest value. Picking an element at random is simple but—as we’ve seen—doesn’t always result in a good selection. However, we could examine all the elements and actually calculate which one was the median. This would be the ideal pivot choice, but the process isn’t practical, as it would take more time than the sort itself. \nA compromise solution is to find the median of the first, last, and middle elements of the array, and use this for the pivot. Picking the median of the first, last, and middle elements is called the median-of-three approach and is shown in Figure 7.13. \nFinding the median of three items is obviously much faster than finding the median of all the items, and yet it successfully avoids picking the largest or smallest item in cases where the data is already sorted or inversely sorted. There are probably some pathological arrangements of data where the median-of-three scheme works poorly, but normally it’s a fast and effective technique for finding the pivot. \nBesides picking the pivot more effectively, the median-of-three approach has an additional benefit: We can dispense with the rightPtr>left test in the second inside while loop, leading to a small increase in the algorithm’s speed. How is this possible? \nThe test can be eliminated because we can use the median-of-three approach to not only select the pivot, but also to sort the three elements used in the selection process. Figure 7.14 shows this operation. \nWhen these three elements are sorted, and the median item is selected as the pivot, we are guaranteed that the element at the left end of the subarray is less than (or equal to) the pivot, and the element at the right end is greater than (or equal to) the pivot. This means that the leftPtr and rightPtr indices can’t step beyond the right or left ends of the array, respectively, even if we remove the leftPtr>right and rightPtr<left tests. (The pointer will stop, thinking it needs to swap the item, only to find that it has crossed the other pointer and the partition is complete.) The values at left and right act as sentinels to keep leftPtr and rightPtr confined to valid array values. \n\nAnother small benefit to median-of-three partitioning is that after the left, center, and right elements are sorted, the partition process doesn’t need to examine these elements again. The partition can begin at $vert mathsf { e f t } mathtt { t } + 1$ and right-1 because left and right have in effect already been partitioned. We know that left is in the correct partition because it’s on the left and it’s less than the pivot, and right is in the correct place because it’s on the right and it’s greater than the pivot. \nThus, median-of-three partitioning not only avoids $mathrm { O } ( mathrm { N } ^ { 2 } )$ performance for alreadysorted data, it also allows us to speed up the inner loops of the partitioning algorithm and reduce slightly the number of items that must be partitioned. \nThe quickSort2.java Program \nListing 7.4 shows the quickSort2.java program, which incorporates median-of-three partitioning. We use a separate method, medianOf3(), to sort the left, center, and right elements of a subarray. This method returns the value of the pivot, which is then sent to the partitionIt() method. \n// quickSort2.java   \n// demonstrates quick sort with median-of-three partitioning   \n// to run this program: $complement >$ java QuickSort2App   \n////////////////////////////////////////////////////////////////   \nclass ArrayIns { private long[] theArray; // ref to array theArray private int nElems; // number of data items   \n//- public ArrayIns(int max) // constructor { theArray $mathbf { tau } = mathbf { tau }$ new long[max]; $bigstar / / bigstar$ create the array nElems $mathit { Theta } = mathit { Theta } varnothing$ ; $bigstar / / bigstar$ no items yet }   \n// public void insert(long value) // put element into array { theArray[nElems] $mathbf { tau } = mathbf { tau }$ value; $bigstar / / bigstar$ insert it nElems $^ { + + }$ ; $bigstar / / bigstar$ increment size }   \n// public void display() $bigstar / / bigstar$ displays array contents { System.out.print ${ mathfrak { n } } mathsf { A } = \" qquad $ ); for(int $scriptstyle mathbf { j } = 0$ ; j<nElems; ${ mathrm { j } } + +$ ) // for each element, System.out.print(theArray[j] + “ “);  // display it System.out.println(“”); } public void quickSort() { recQuickSort(0, nElems-1); } public void recQuickSort(int left, int right) { int size $mathbf { tau } = mathbf { tau }$ right-left+1; if $( mathsf { s i z e  < } ) .$ ) // manual sort if small manualSort(left, right); else // quicksort if large { \nLISTING 7.4 Continued \nlong median $mathbf { tau } = mathbf { tau }$ medianOf3(left, right); int partition $mathbf { tau } = mathbf { tau }$ partitionIt(left, right, median); recQuickSort(left, partition-1); recQuickSort(partition+1, right); } }  // end recQuickSort()   \n//- public long medianOf3(int left, int right) { int center $mathbf { tau } = mathbf { tau }$ (left+right)/2; // order left & center if( theArray[left] $>$ theArray[center] ) swap(left, center); // order left & right if( theArray[left] $>$ theArray[right] ) swap(left, right); // order center & right if( theArray[center] $>$ theArray[right] ) swap(center, right); swap(center, right-1); // put pivot on right return theArray[right-1]; // return median value }  // end medianOf3()   \n//-- public void swap(int dex1, int dex2) // swap two elements { long temp $mathbf { tau } = mathbf { tau }$ theArray[dex1]; // A into temp theArray[dex1] $mathbf { tau } = mathbf { tau }$ theArray[dex2]; // B into A theArray[dex2] $mathbf { tau } = mathbf { tau }$ temp; // temp into B }  // end swap(   \n//- public int partitionIt(int left, int right, long pivot) { int leftPtr $mathbf { tau } = mathbf { tau }$ left; // right of first elem int rightPtr $mathbf { tau } = mathbf { tau }$ right - 1; // left of pivot while(true) { while( theArray[++leftPtr] $mathit { Theta } _ { < }$ pivot ) $1 1$ find bigger ; $bigstar / / bigstar$ (nop) while( theArray[--rightPtr] $>$ pivot ) $bigstar / / bigstar$ find smaller \n// (nop) if(leftPtr $> =$ rightPtr) // if pointers cross, break; // partition done else // not crossed, so swap(leftPtr, rightPtr); // swap elements }  // end while(true) swap(leftPtr, right-1); // restore pivot return leftPtr; // return pivot location }  // end partitionIt() // public void manualSort(int left, int right) { int si $mathsf { Omega } : mathsf { e } ^ { mathrm { ~ tiny ~ { ~ cdot ~ } ~ } } = mathsf { r } mathrm { i } mathsf { g h t } cdot mathsf {  { mathrm { - } } } 1 mathsf { e } ^ { mathsf { f } mathrm { Theta } + mathrm { Theta } }$ t+1; $mathrm { i } mathsf { f } left( mathsf { s i z e }  < =  1 right)$ ) return; // no sort necessary i $textsf { f } ( textsf { s i z e } = 2 )$ ) { // 2-sort left and right if( theArray[left] $>$ theArray[right] ) swap(left, right); return; } else // size is 3 { // 3-sort left, center, & right if( theArray[left] $>$ theArray[right-1] ) swap(left, right-1); // left, center if( theArray[left] $>$ theArray[right] ) swap(left, right); // left, right if( theArray[right-1] $>$ theArray[right] ) swap(right-1, right); // center, right } } // end manualSort() // \n}  // end class ArrayIns   \n/////////////////////////////////////// //////////////////   \nclass QuickSort2App { public static void main(String[] args) { int maxSize $mathbf { Sigma } = mathbf { Sigma }$ 16; // array size ArrayIns arr; // reference to array arr $mathbf { tau } = mathbf { tau }$ new ArrayIns(maxSize); // create the array \nfor(int $scriptstyle mathbf { j } = 0$ ; j<maxSize; $mathrm { j } + + mathrm { Delta }$ ) $bigstar / / bigstar$ fill array with { $bigstar / / bigstar$ random numbers long ${ mathfrak { n } } =$ (int)(java.lang.Math.random( $) star 9 9$ ); arr.insert(n); } arr.display(); // display items arr.quickSort(); // quicksort them arr.display(); // display them again }  // end main() }  // end class QuickSort2App \nThis program uses another new method, manualSort(), to sort subarrays of three or fewer elements. It returns immediately if the subarray is one cell (or less), swaps the cells if necessary if the range is 2, and sorts three cells if the range is 3. The recQuickSort() routine can’t be used to sort ranges of 2 or 3 because median partitioning requires at least four cells. \nThe main() routine and the output of quickSort2.java are similar to those of quickSort1.java. \nThe QuickSort2 Workshop Applet \nThe Quicksort2 Workshop applet demonstrates the quicksort algorithm using median-of-three partitioning. This applet is similar to the QuickSort1 Workshop applet, but starts off sorting the first, center, and left elements of each subarray and selecting the median of these as the pivot value. At least, it does this if the array size is greater than 3. If the subarray is two or three units, the applet simply sorts it “by hand” without partitioning or recursive calls. \nNotice the dramatic improvement in performance when the applet is used to sort 100 inversely ordered bars. No longer is every subarray partitioned into 1 cell and N-1 cells; instead, the subarrays are partitioned roughly in half. \nOther than this improvement for ordered data, the QuickSort2 Workshop applet produces results similar to QuickSort1. It is no faster when sorting random data; it’s advantages become evident only when sorting ordered data. \nHandling Small Partitions \nIf you use the median-of-three partitioning method, it follows that the quicksort algorithm won’t work for partitions of three or fewer items. The number 3 in this case is called a cutoff point. In the examples above we sorted subarrays of two or three items by hand. Is this the best way? \nUsing an Insertion Sort for Small Partitions \nAnother option for dealing with small partitions is to use the insertion sort. When you do this, you aren’t restricted to a cutoff of 3. You can set the cutoff to 10, 20, or any other number. It’s interesting to experiment with different values of the cutoff to see where the best performance lies. Knuth (see Appendix B) recommends a cutoff of 9. However, the optimum number depends on your computer, operating system, compiler (or interpreter), and so on. \nThe quickSort3.java program, shown in Listing 7.5, uses an insertion sort to handle subarrays of fewer than 10 cells. \nLISTING 7.5 The quickSort3.java Program \n// quickSort3.java   \n// demonstrates quick sort; uses insertion sort for cleanup   \n// to run this program: $complement >$ java QuickSort3App   \n////////////////////////////////////////////////////////////////   \nclass ArrayIns { private long[] theArray; // ref to array theArray private int nElems; // number of data items   \n// public ArrayIns(int max) // constructor { theArray $mathbf { tau } = mathbf { tau }$ new long[max]; // create the array nElems $= 0$ ; $bigstar / / bigstar$ no items yet } public void insert(long value) $bigstar / / bigstar$ put element into array { theArray[nElems] $mathbf { tau } = mathbf { tau }$ value; $bigstar / / bigstar$ insert it nElems $^ { + + }$ ; $bigstar / / bigstar$ increment size }   \n// public void display() $bigstar / / bigstar$ displays array contents { System.out.print $\" mathsf { A } = \"$ ); for(int $scriptstyle mathbf { j } = 0$ ; j<nElems; ${ mathrm { j } } + +$ ) // for each element, System.out.print(theArray[j] $mathrm { ~ bf ~ underline { ~ } { ~ underline { ~ } { ~ bf ~ epsilon ~ } ~ } ~ } + mathrm { ~ bf ~ underline { ~ } { ~ bf ~ underline { ~ } { ~ bf ~ epsilon ~ } ~ } ~ } mathrm { ~ bf ~ underline { ~ } { ~ bf ~ epsilon ~ } ~ } )$ ;  // display it System.out.println(“”); } public void quickSort() \nLISTING 7.5 Continued \n{ recQuickSort(0, nElems-1); $bigstar / / bigstar$ insertionSort(0, nElems-1); // the other option $}$   \n// public void recQuickSort(int left, int right) { int size $mathbf { tau } = mathbf { tau }$ right-left+1; i $mathsf { f } ( mathsf { s i z e } < 1 0 )$ // insertion sort if small insertionSort(left, right); else // quicksort if large { long median $mathbf { tau } = mathbf { tau }$ medianOf3(left, right); int partition $mathbf { tau } = mathbf { tau }$ partitionIt(left, right, median); recQuickSort(left, partition-1); recQuickSort(partition+1, right); } } $bigstar / / bigstar$ end recQuickSort() public long medianOf3(int left, int right) { int center $mathbf { tau } = mathbf { tau }$ (left+right)/2; // order left & center if( theArray[left] $>$ theArray[center] ) swap(left, center); // order left & right if( theArray[left] $>$ theArray[right] ) swap(left, right); // order center & right if( theArray[center] $>$ theArray[right] ) swap(center, right); swap(center, right-1); // put pivot on right return theArray[right-1]; // return median value }  // end medianOf3()   \n// public void swap(int dex1, int dex2) // swap two elements { long temp $mathbf { tau } = mathbf { tau }$ theArray[dex1]; // A into temp theArray[dex1] $mathbf { Sigma } = mathbf { Sigma }$ theArray[dex2]; // B into A theArray[dex2] $mathbf { tau } = mathbf { tau }$ temp; // temp into B }  // end swap(   \n//- public int partitionIt(int left, int right, long pivot) { int leftPtr $mathbf { tau } = mathbf { tau }$ left; // right of first elem int rightPtr $mathbf { tau } = mathbf { tau }$ right - 1; // left of pivot while(true) { while( theArray $^ { + + }$ leftPtr] $mathit { Delta } <$ pivot )  // find bigger ; // (nop) while( theArray[--rightPtr] $>$ pivot ) $1 1$ find smaller ; // (nop) if(leftPtr $> =$ rightPtr) // if pointers cross, break; // partition done else // not crossed, so swap(leftPtr, rightPtr);  // swap elements }  // end while(true) swap(leftPtr, right-1); // restore pivot return leftPtr; // return pivot location }  // end partitionIt()   \n// // insertion sort public void insertionSort(int left, int right) { int in, out; //  sorted on left of out for( $0 mathsf { u t } = mathsf { l e f t } + 1$ ; out $< =$ right; out++) { long temp $mathbf { tau } = mathbf { tau }$ theArray[out]; $1 1$ remove marked item in $mathbf { tau } = mathbf { tau }$ out; $1 1$ start shifts at out // until one is smaller, while(in>left && theArray[in-1] $> =$ temp) { theArray[in] $mathbf { tau } = mathbf { tau }$ theArray[in-1]; // shift item to righ --in; // go left one position } theArray[in] $mathbf { tau } = mathbf { tau }$ temp; // insert marked item }  // end for } // end insertionSort()   \n//- } // end class ArrayIns \n\nLISTING 7.5 Continued \n///////////// //////// \nclass QuickSort3App { public static void main(String[] args) { int maxSize $= 1 6$ ; // array size ArrayIns arr; $bigstar / / bigstar$ reference to array arr $mathbf { tau } = mathbf { tau }$ new ArrayIns(maxSize); $bigstar / / bigstar$ create the array for(int $scriptstyle { mathrm { j } } = 0$ ; j<maxSize; ${ mathrm { j } } + +$ ) $bigstar / / bigstar$ fill array with { $bigstar / / bigstar$ random numbers long ${ mathfrak { n } } =$ (int)(java.lang.Math.random $( ) ^ { star } 9 9 $ ); arr.insert(n); } arr.display(); // display items arr.quickSort(); $bigstar / / bigstar$ quicksort them arr.display(); $bigstar / / bigstar$ display them again }  // end main() \nUsing the insertion sort for small subarrays turns out to be the fastest approach on our particular installation, but it is not much faster than sorting subarrays of three or fewer cells by hand, as in quickSort2.java. The numbers of comparisons and copies are reduced substantially in the quicksort phase, but are increased by an almost equal amount in the insertion sort, so the time savings are not dramatic. However, this approach is probably worthwhile if you are trying to squeeze the last ounce of performance out of quicksort. \nInsertion Sort Following Quicksort \nAnother option is to completely quicksort the array without bothering to sort partitions smaller than the cutoff. This is shown with a commented-out line in the quickSort() method. (If this call is used, the call to insertionSort() should be removed from recQuickSort().) When quicksort is finished, the array will be almost sorted. You then apply the insertion sort to the entire array. The insertion sort is supposed to operate efficiently on almost-sorted arrays, and this approach is recommended by some experts, but on our installation it runs very slowly. The insertion sort appears to be happier doing a lot of small sorts than one big one. \nRemoving Recursion \nAnother embellishment recommended by many writers is removing recursion from the quicksort algorithm. This involves rewriting the algorithm to store deferred \nsubarray bounds (left and right) on a stack, and using a loop instead of recursion to oversee the partitioning of smaller and smaller subarrays. The idea in doing this is to speed up the program by removing method calls. However, this idea arose with older compilers and computer architectures, which imposed a large time penalty for each method call. It’s not clear that removing recursion is much of an improvement for modern systems, which handle method calls more efficiently. \nEfficiency of Quicksort \nWe’ve said that quicksort operates in $mathrm { O ( N ^ { star } l o g N ) }$ time. As we saw in the discussion of mergesort in Chapter 6, this is generally true of the divide-and-conquer algorithms, in which a recursive method divides a range of items into two groups and then calls itself to handle each group. In this situation the logarithm actually has a base of 2: The running time is proportional to $mathrm { N } ^ { star } mathrm { l o g } _ { 2 } mathrm { N }$ . \nYou can get an idea of the validity of this $mathrm { N } ^ { star } mathrm { l o g } _ { 2 } mathrm { N }$ running time for quicksort by running one of the quickSort Workshop applets with 100 random bars and examining the resulting dotted horizontal lines. \nEach dotted line represents an array or subarray being partitioned: the pointers leftScan and rightScan moving toward each other, comparing each data item and swapping when appropriate. We saw in the “Partitioning” section that a single partition runs in O(N) time. This tells us that the total length of all the dotted lines is proportional to the running time of quicksort. But how long are all the lines? Measuring them with a ruler on the screen would be tedious, but we can visualize them a different way. \nThere is always 1 line that runs the entire width of the graph, spanning N bars. This results from the first partition. There will also be 2 lines (one below and one above the first line) that have an average length of $mathrm { N } / 2$ bars; together they are again N bars long. Then there will be 4 lines with an average length of $mathrm { N } / 4$ that again total N bars, then 8 lines, 16 lines, and so on. Figure 7.15 shows how this looks for 1, 2, 4, and 8 lines. \nIn this figure solid horizontal lines represent the dotted horizontal lines in the quicksort applets, and captions like N/4 cells long indicate average, not actual, line lengths. The circled numbers on the left show the order in which the lines are created. \nEach series of lines (the eight $_ { mathrm { N } / 8 }$ lines, for example) corresponds to a level of recursion. The initial call to recQuickSort() is the first level and makes the first line; the two calls from within the first call—the second level of recursion—make the next two lines; and so on. If we assume we start with 100 cells, the results are shown in Table 7.4. \nWhere does this division process stop? If we keep dividing 100 by 2, and count how many times we do this, we get the series 100, 50, 25, 12, 6, 3, 1, which is about seven levels of recursion. This looks about right on the workshop applets: If you pick some point on the graph and count all the dotted lines directly above and below it, there will be an average of approximately seven. (In Figure 7.15, because not all levels of recursion are shown, only four lines intersect any vertical slice of the graph.) \nTable 7.4 shows a total of 652 cells. This is only an approximation because of roundoff errors, but it’s close to 100 times the logarithm to the base 2 of 100, which is 6.65. Thus, this informal analysis suggests the validity of the $mathrm { N } ^ { star } mathrm { l o g } _ { 2 } mathrm { N }$ running time for quicksort. \nMore specifically, in the section on partitioning, we found that there should be $_ { mathrm { N } + 2 }$ comparisons and fewer than $mathrm { N } / 2$ swaps. Multiplying these quantities by $log _ { 2 } mathrm { N }$ for various values of $mathrm { Delta N }$ gives the results shown in Table 7.5. \nThe $log _ { 2 } mathrm { N }$ quantity used in Table 7.5 is actually true only in the best-case scenario, where each subarray is partitioned exactly in half. For random data the figure is slightly greater. Nevertheless, the QuickSort1 and QuickSort2 Workshop applets approximate these results for 12 and 100 bars, as you can see by running them and observing the Swaps and Comparisons fields. \nBecause they have different cutoff points and handle the resulting small partitions differently, QuickSort1 performs fewer swaps but more comparisons than QuickSort2. The number of swaps shown in Table 7.5 is the maximum (which assumes the data is inversely sorted). For random data the actual number of swaps turns out to be onehalf to two-thirds of the figures shown. \nRadix Sort \nWe’ll close this chapter by briefly mentioning a sort that uses a different approach. The sorts we’ve looked at so far treat the key as a simple numerical value that is compared with other values to sort the data. The radix sort disassembles the key into digits and arranges the data items according to the value of the digits. Amazingly, no comparisons are necessary.",
      "metadata": {
        "content": "Although there are fewer swaps than comparisons, they are both proportional to N. Thus, the partitioning process runs in O(N) time. Running the Workshop applet, you can see that for 12 random bars there are about 3 swaps and 14 comparisons, and for 100 random bars there are about 25 swaps and 102 comparisons. \nQuicksort \nQuicksort is undoubtedly the most popular sorting algorithm, and for good reason: In the majority of situations, it’s the fastest, operating in $mathrm { O ( N ^ { star } l o g N ) }$ time. (This is only true for internal or in-memory sorting; for sorting data in disk files, other algorithms may be better.) Quicksort was discovered by C.A.R. Hoare in 1962. \nTo understand quicksort, you should be familiar with the partitioning algorithm described in the preceding section. Basically, the quicksort algorithm operates by partitioning an array into two subarrays and then calling itself recursively to quicksort each of these subarrays. However, there are some embellishments we can make to this basic scheme. They have to do with the selection of the pivot and the sorting of small partitions. We’ll examine these refinements after we’ve looked at a simple version of the main algorithm. \nIt’s difficult to understand what quicksort is doing before you understand how it does it, so we’ll reverse our usual presentation and show the Java code for quicksort before presenting the QuickSort1 Workshop applet. \nThe Quicksort Algorithm \nThe code for a basic recursive quicksort method is fairly simple. Here’s an example: \npublic void recQuickSort(int left, int right) { if(right-left $< = ~ 0$ ) // if size is 1, return; // it’s already sorted else // size is 2 or larger { // partition range int partition $mathbf { tau } = mathbf { tau }$ partitionIt(left, right); recQuickSort(left, partition-1); // sort left side recQuickSort(partition+1, right);  // sort right side } } \nAs you can see, there are three basic steps: \n1. Partition the array or subarray into left (smaller keys) and right (larger keys) groups. \n2. Call ourselves to sort the left group.   \n3. Call ourselves again to sort the right group. \nAfter a partition, all the items in the left subarray are smaller than all those on the right. If we then sort the left subarray and sort the right subarray, the entire array will be sorted. How do we sort these subarrays? By calling ourself recursively. \nThe arguments to the recQuickSort() method determine the left and right ends of the array (or subarray) it’s supposed to sort. The method first checks if this array consists of only one element. If so, the array is by definition already sorted, and the method returns immediately. This is the base case in the recursion process. \nIf the array has two or more cells, the algorithm calls the partitionIt() method, described in the preceding section, to partition it. This method returns the index number of the partition: the left element in the right (larger keys) subarray. The partition marks the boundary between the subarrays. This situation is shown in Figure 7.8. \nAfter the array is partitioned, recQuickSort() calls itself recursively, once for the left part of its array, from left to partition-1, and once for the right, from partition+1 to right. Note that the data item at the index partition is not included in either of the recursive calls. Why not? Doesn’t it need to be sorted? The explanation lies in how the pivot value is chosen. \nChoosing a Pivot Value \nWhat pivot value should the partitionIt() method use? Here are some relevant ideas: \n• The pivot value should be the key value of an actual data item; this item is called the pivot.   \n• You can pick a data item to be the pivot more or less at random. For simplicity, let’s say we always pick the item on the right end of the subarray being partitioned.   \n• After the partition, if the pivot is inserted at the boundary between the left and right subarrays, it will be in its final sorted position. \nThis last point may sound unlikely, but remember that, because the pivot’s key value is used to partition the array, following the partition the left subarray holds items smaller than the pivot, and the right subarray holds items larger. The pivot starts out on the right, but if it could somehow be placed between these two subarrays, it would be in the correct place—that is, in its final sorted position. Figure 7.9 shows how this looks with a pivot whose key value is 36. \nThis figure is somewhat fanciful because you can’t actually take an array apart as we’ve shown. So how do we move the pivot to its proper place? \nWe could shift all the items in the right subarray to the right one cell to make room for the pivot. However, this is inefficient and unnecessary. Remember that all the \nitems in the right subarray, although they are larger than the pivot, are not yet sorted, so they can be moved around, within the right subarray, without affecting anything. Therefore, to simplify inserting the pivot in its proper place, we can simply swap the pivot (36) and the left item in the right subarray, which is 63. This swap places the pivot in its proper position between the left and right groups. The 63 is switched to the right end, but because it remains in the right (larger) group, the partitioning is undisturbed. This situation is shown in Figure 7.10. \nWhen it’s swapped into the partition’s location, the pivot is in its final resting place. All subsequent activity will take place on one side of it or on the other, but the pivot itself won’t be moved (or indeed even accessed) again. \nTo incorporate the pivot selection process into our recQuickSort() method, let’s make it an overt statement, and send the pivot value to partitionIt() as an argument. Here’s how that looks: \npublic void recQuickSort(int left, int right) { if(right-left $< =  0  d$ ) // if size $< = ~ 1$ , return; $bigstar / / bigstar$ already sorted else $bigstar / / bigstar$ size is 2 or larger { long pivot $mathbf { tau } = mathbf { tau }$ theArray[right]; // rightmost item // partition range int partition $mathbf { tau } = mathbf { tau }$ partitionIt(left, right, pivot); \nrecQuickSort(left, partition-1); // sort left side recQuickSort(partition+1, right);  // sort right side } }  // end recQuickSort() \nWhen we use this scheme of choosing the rightmost item in the array as the pivot, we’ll need to modify the partitionIt() method to exclude this rightmost item from the partitioning process; after all, we already know where it should go after the partitioning process is complete: at the partition, between the two groups. Also, after the partitioning process is completed, we need to swap the pivot from the right end into the partition’s location. Listing 7.3 shows the quickSort1.java program, which incorporates these features. \n// quickSort1.java   \n// demonstrates simple version of quick sort   \n// to run this program: $complement >$ java QuickSort1App   \n////////////////////////////////////////////////////////////////   \nclass ArrayIns { private long[] theArray; // ref to array theArray private int nElems; // number of data items   \n// public ArrayIns(int max) // constructor { theArray $mathbf { tau } = mathbf { tau }$ new long[max]; $bigstar / / bigstar$ create the array nElems $mathit { Theta } = mathit { Theta } varnothing$ ; $bigstar / / bigstar$ no items yet } public void insert(long value) // put element into array { theArray[nElems] $mathbf { tau } = mathbf { tau }$ value; $bigstar / / bigstar$ insert it nElems $^ { + + }$ ; $bigstar / / bigstar$ increment size } public void display() $bigstar / / bigstar$ displays array contents { System.out.print $\" A = \"$ ); for(int $scriptstyle mathbf { j } = 0$ ; j<nElems; ${ mathrm { j } } + +$ ) // for each element, System.out.print(theArray[j] $mathrm { ~  ~ { ~ mathsf ~ { ~ pi ~ } ~ } ~ } + mathrm { ~  ~ { ~ mathsf ~ { ~ pi ~ } ~ } ~ } ^ { mathsf { pi } } mathsf { Lambda } ^ { mathsf { pi } } )$ );  // display it System.out.println(“”); } \nLISTING 7.3 Continued \n// public void quickSort() { recQuickSort(0, nElems-1); }   \n/ public void recQuickSort(int left, int right) { if(right-left $scriptstyle < =  0$ ) // if size $< = ~ 1$ , return; $bigstar / / bigstar$ already sorted else // size is 2 or larger { long pivot $mathbf { tau } = mathbf { tau }$ theArray[right]; // rightmost item // partition range int partition $mathbf { tau } = mathbf { tau }$ partitionIt(left, right, pivot); recQuickSort(left, partition $^ { - 1 }$ ); // sort left side recQuickSort(partition+1, right);  // sort right side } }  // end recQuickSort()   \n// public int partitionIt(int left, int right, long pivot) { int leftPtr $mathbf { varepsilon } = mathbf { varepsilon } _ { 1 }$ eft-1; // left (after $^ { + + }$ ) int rightPtr $mathbf { tau } = mathbf { tau }$ right; // right-1 (after --) while(true) { // find bigger item while( theArray[++leftPtr] $mathit { Theta } < mathit { Theta }$ pivot ) ;  // (nop) // find smaller item while(rightPtr > 0 && theArray[--rightPtr] $>$ pivot) ;  // (nop) if(leftPtr $> =$ rightPtr) $bigstar / / bigstar$ if pointers cross, break; $bigstar / / bigstar$ partition done else $bigstar / / bigstar$ not crossed, so swap(leftPtr, rightPtr);  // swap elements }  // end while(true) swap(leftPtr, right); // restore pivot return leftPtr; // return pivot location }  // end partitionIt()   \n//- public void swap(int dex1, int dex2)  // swap two elements { long temp $mathbf { tau } = mathbf { tau }$ theArray[dex1]; // A into temp theArray[dex1] $mathbf { tau } = mathbf { tau }$ theArray[dex2]; // B into A theArray[dex2] $mathbf { tau } = mathbf { tau }$ temp; // temp into B }  // end swap(   \n//- }  // end class ArrayIns   \n////////////////////////////////////// ////////////////   \nclass QuickSort1App { public static void main(String[] args) { int maxSize = 16; // array size ArrayIns arr; arr $mathbf { tau } = mathbf { tau }$ new ArrayIns(maxSize);  // create array for(int $scriptstyle mathbf { j } = 0$ ; j<maxSize; ${ mathrm { j } } + +$ ) $1 1$ fill array with { // random numbers long ${ mathfrak { n } } =$ (int)(java.lang.Math.random $( ) ^ { star } 9 9 ^ { prime }$ ); arr.insert(n); } arr.display(); // display items arr.quickSort(); // quicksort them arr.display(); // display them again }  // end main() }  // end class QuickSort1App \n\nThe main() routine creates an object of type ArrayIns, inserts 16 random data items of type long in it, displays it, sorts it with the quickSort() method, and displays the results. Here’s some typical output: \n$A = 6 9$ 0 70 6 38 38 24 56 44 26 73 77 30 45 97 65   \n$A = 0$ 6 24 26 30 38 38 44 45 56 65 69 70 73 77 97 \nAn interesting aspect of the code in the partitionIt() method is that we’ve been able to remove the test for the end of the array in the first inner while loop. This test, seen in the earlier partitionIt() method in the partition.java program in Listing 7.2, was \nleftPtr $angle { mathbf { xi } } < mathbf { xi }$ right \nIt prevented leftPtr running off the right end of the array if no item there was larger than pivot. Why can we eliminate the test? Because we selected the rightmost item as the pivot, so leftPtr will always stop there. However, the test is still necessary for rightPtr in the second while loop. (Later we’ll see how this test can be eliminated as well.) \nChoosing the rightmost item as the pivot is thus not an entirely arbitrary choice; it speeds up the code by removing an unnecessary test. Picking the pivot from some other location would not provide this advantage. \nThe QuickSort1 Workshop Applet \nAt this point you know enough about the quicksort algorithm to understand the nuances of the QuickSort1 Workshop applet. \nThe Big Picture \nFor the big picture, use the Size button to set the applet to sort 100 random bars, and press the Run button. Following the sorting process, the display will look something like Figure 7.11. \nWatch how the algorithm partitions the array into two parts, then sorts each of these parts by partitioning it into two parts, and so on, creating smaller and smaller subarrays. \nWhen the sorting process is complete, each dotted line provides a visual record of one of the sorted subarrays. The horizontal range of the line shows which bars were part of the subarray, and its vertical position is the pivot value (the height of the \npivot). The total length of all these lines on the display is a measure of how much work the algorithm has done to sort the array; we’ll return to this topic later. \nEach dotted line (except the shortest ones) should have a line below it (probably separated by other, shorter lines) and a line above it that together add up to the same length as the original line (less one bar). These are the two partitions into which each subarray is divided. \nThe Details \nFor a more detailed examination of quicksort’s operation, switch to the 12-bar display in the QuickSort1 Workshop applet and step through the sorting process. You’ll see how the pivot value corresponds to the height of the pivot on the right side of the array and how the algorithm partitions the array, swaps the pivot into the space between the two sorted groups, sorts the shorter group (using many recursive calls), and then sorts the larger group. \nFigure 7.12 shows all the steps involved in sorting 12 bars. The horizontal brackets under the arrays show which subarray is being partitioned at each step, and the circled numbers show the order in which these partitions are created. A pivot being swapped into place is shown with a dotted arrow. The final position of the pivot is shown as a dotted cell to emphasize that this cell contains a sorted item that will not be changed thereafter. Horizontal brackets under single cells (steps 5, 6, 7, 11, and 12) are base case calls to recQuickSort(); they return immediately. \nSometimes, as in steps 4 and 10, the pivot ends up in its original position on the right side of the array being sorted. In this situation, there is only one subarray remaining to be sorted: the one to the left of the pivot. There is no second subarray to its right. \nThe different steps in Figure 7.12 occur at different levels of recursion, as shown in Table 7.3. The initial call from main() to recQuickSort() is the first level, recQuickSort() calling two new instances of itself is the second level, these two instances calling four more instances is the third level, and so on. \nThe order in which the partitions are created, corresponding to the step numbers, does not correspond with depth. It’s not the case that all the first-level partitions are done first, then all the second level ones, and so on. Instead, the left group at every level is handled before any of the right groups. \nIn theory there should be 8 steps in the fourth level and 16 in the fifth level, but in this small array we run out of items before these steps are necessary. \nThe number of levels in the table shows that with 12 data items, the machine stack needs enough space for 5 sets of arguments and return values; one for each recursion level. This is, as we’ll see later, somewhat greater than the logarithm to the base 2 of the number of items: $log _ { 2 } mathrm { N }$ . The size of the machine stack is determined by your particular system. Sorting very large numbers of data items using recursive procedures may cause this stack to overflow, leading to memory errors. \nThings to Notice \nHere are some details you may notice as you run the QuickSort1 Workshop applet. \nYou might think that a powerful algorithm like quicksort would not be able to handle subarrays as small as two or three items. However, this version of the quicksort algorithm is quite capable of sorting such small subarrays; leftScan and rightScan just don’t go very far before they meet. For this reason we don’t need to use a different sorting scheme for small subarrays. (Although, as we’ll see later, handling small subarrays differently may have advantages.) \nAt the end of each scan, the leftScan variable ends up pointing to the partition— that is, the left element of the right subarray. The pivot is then swapped with the partition to put the pivot in its proper place, as we’ve seen. As we noted, in steps 3 and 9 of Figure 7.12, leftScan ends up pointing to the pivot itself, so the swap has no effect. This may seem like a wasted swap; you might decide that leftScan should stop one bar sooner. However, it’s important that leftScan scan all the way to the pivot; otherwise, a swap would unsort the pivot and the partition. \nBe aware that leftScan and rightScan start at left-1 and right. This may look peculiar on the display, especially if left is 0; then leftScan will start at –1. Similarly, rightScan initially points to the pivot, which is not included in the partitioning process. These pointers start outside the subarray being partitioned because they will be incremented and decremented, respectively, before they’re used the first time. \nThe applet shows ranges as numbers in parentheses; for example, (2-5) means the subarray from index 2 to index 5. The range given in some of the messages may be negative: from a higher number to a lower one, such as Array partitioned; left (7-6), right (8-8). The (8-8) range means a single cell (8), but what does (7-6) mean? This range isn’t real; it simply reflects the values that left and right, the arguments to recQuickSort(), have when this method is called. Here’s the code in question: \nint partition $mathbf { tau } = mathbf { tau }$ partitionIt(left, right, pivot); recQuickSort(left, partition-1);   // sort left side recQuickSort(partition+1, right);  // sort right side \nIf partitionIt() is called with $mathtt { l e f t } = 7$ and right $= 8$ , for example, and happens to return 7 as the partition, then the range supplied in the first call to recQuickSort() will be (7-6) and the range to the second will be (8-8). This is normal. The base case in recQuickSort() is activated by array sizes less than 1 as well as by 1, so it will return immediately for negative ranges. Negative ranges are not shown in Figure 7.12, although they do cause (brief) calls to recQuickSort(). \nDegenerates to $mathsf { O } ( mathsf { N } ^ { 2 } )$ Performance \nIf you use the QuickSort1 Workshop applet to sort 100 inversely sorted bars, you’ll see that the algorithm runs much more slowly and that many more dotted horizontal lines are generated, indicating more and larger subarrays are being partitioned. What’s happening here? \nThe problem is in the selection of the pivot. Ideally, the pivot should be the median of the items being sorted. That is, half the items should be larger than the pivot, and half smaller. This would result in the array being partitioned into two subarrays of equal size. Having two equal subarrays is the optimum situation for the quicksort algorithm. If it has to sort one large and one small array, it’s less efficient because the larger subarray has to be subdivided more times. \nThe worst situation results when a subarray with N elements is divided into one subarray with 1 element and the other with N-1 elements. (This division into 1 cell and N-1 cells can also be seen in steps 3 and 9 in Figure 7.12.) If this 1 and N-1 division happens with every partition, then every element requires a separate partition step. This is in fact what takes place with inversely sorted data: In all the subarrays, the pivot is the smallest item, so every partition results in N-1 elements in one subarray and only the pivot in the other. \nTo see this unfortunate process in action, step through the QuickSort1 Workshop applet with 12 inversely sorted bars. Notice how many more steps are necessary than with random data. In this situation the advantage gained by the partitioning process is lost and the performance of the algorithm degenerates to $mathrm { O } ( mathrm { N } ^ { 2 } )$ . \nBesides being slow, there’s another potential problem when quicksort operates in $mathrm { O } ( mathrm { N } ^ { 2 } )$ time. When the number of partitions increases, the number of recursive function calls also increases. Every function call takes up room on the machine stack. If there are too many calls, the machine stack may overflow and paralyze the system. \nTo summarize: In the QuickSort1 applet, we select the rightmost element as the pivot. If the data is truly random, this isn’t too bad a choice because usually the pivot won’t be too close to either end of the array. However, when the data is sorted or inversely sorted, choosing the pivot from one end or the other is a bad idea. Can we improve on our approach to selecting the pivot? \n\nMedian-of-Three Partitioning \nMany schemes have been devised for picking a better pivot. The method should be simple but have a good chance of avoiding the largest or smallest value. Picking an element at random is simple but—as we’ve seen—doesn’t always result in a good selection. However, we could examine all the elements and actually calculate which one was the median. This would be the ideal pivot choice, but the process isn’t practical, as it would take more time than the sort itself. \nA compromise solution is to find the median of the first, last, and middle elements of the array, and use this for the pivot. Picking the median of the first, last, and middle elements is called the median-of-three approach and is shown in Figure 7.13. \nFinding the median of three items is obviously much faster than finding the median of all the items, and yet it successfully avoids picking the largest or smallest item in cases where the data is already sorted or inversely sorted. There are probably some pathological arrangements of data where the median-of-three scheme works poorly, but normally it’s a fast and effective technique for finding the pivot. \nBesides picking the pivot more effectively, the median-of-three approach has an additional benefit: We can dispense with the rightPtr>left test in the second inside while loop, leading to a small increase in the algorithm’s speed. How is this possible? \nThe test can be eliminated because we can use the median-of-three approach to not only select the pivot, but also to sort the three elements used in the selection process. Figure 7.14 shows this operation. \nWhen these three elements are sorted, and the median item is selected as the pivot, we are guaranteed that the element at the left end of the subarray is less than (or equal to) the pivot, and the element at the right end is greater than (or equal to) the pivot. This means that the leftPtr and rightPtr indices can’t step beyond the right or left ends of the array, respectively, even if we remove the leftPtr>right and rightPtr<left tests. (The pointer will stop, thinking it needs to swap the item, only to find that it has crossed the other pointer and the partition is complete.) The values at left and right act as sentinels to keep leftPtr and rightPtr confined to valid array values. \n\nAnother small benefit to median-of-three partitioning is that after the left, center, and right elements are sorted, the partition process doesn’t need to examine these elements again. The partition can begin at $vert mathsf { e f t } mathtt { t } + 1$ and right-1 because left and right have in effect already been partitioned. We know that left is in the correct partition because it’s on the left and it’s less than the pivot, and right is in the correct place because it’s on the right and it’s greater than the pivot. \nThus, median-of-three partitioning not only avoids $mathrm { O } ( mathrm { N } ^ { 2 } )$ performance for alreadysorted data, it also allows us to speed up the inner loops of the partitioning algorithm and reduce slightly the number of items that must be partitioned. \nThe quickSort2.java Program \nListing 7.4 shows the quickSort2.java program, which incorporates median-of-three partitioning. We use a separate method, medianOf3(), to sort the left, center, and right elements of a subarray. This method returns the value of the pivot, which is then sent to the partitionIt() method. \n// quickSort2.java   \n// demonstrates quick sort with median-of-three partitioning   \n// to run this program: $complement >$ java QuickSort2App   \n////////////////////////////////////////////////////////////////   \nclass ArrayIns { private long[] theArray; // ref to array theArray private int nElems; // number of data items   \n//- public ArrayIns(int max) // constructor { theArray $mathbf { tau } = mathbf { tau }$ new long[max]; $bigstar / / bigstar$ create the array nElems $mathit { Theta } = mathit { Theta } varnothing$ ; $bigstar / / bigstar$ no items yet }   \n// public void insert(long value) // put element into array { theArray[nElems] $mathbf { tau } = mathbf { tau }$ value; $bigstar / / bigstar$ insert it nElems $^ { + + }$ ; $bigstar / / bigstar$ increment size }   \n// public void display() $bigstar / / bigstar$ displays array contents { System.out.print ${ mathfrak { n } } mathsf { A } = \" qquad $ ); for(int $scriptstyle mathbf { j } = 0$ ; j<nElems; ${ mathrm { j } } + +$ ) // for each element, System.out.print(theArray[j] + “ “);  // display it System.out.println(“”); } public void quickSort() { recQuickSort(0, nElems-1); } public void recQuickSort(int left, int right) { int size $mathbf { tau } = mathbf { tau }$ right-left+1; if $( mathsf { s i z e  < } ) .$ ) // manual sort if small manualSort(left, right); else // quicksort if large { \nLISTING 7.4 Continued \nlong median $mathbf { tau } = mathbf { tau }$ medianOf3(left, right); int partition $mathbf { tau } = mathbf { tau }$ partitionIt(left, right, median); recQuickSort(left, partition-1); recQuickSort(partition+1, right); } }  // end recQuickSort()   \n//- public long medianOf3(int left, int right) { int center $mathbf { tau } = mathbf { tau }$ (left+right)/2; // order left & center if( theArray[left] $>$ theArray[center] ) swap(left, center); // order left & right if( theArray[left] $>$ theArray[right] ) swap(left, right); // order center & right if( theArray[center] $>$ theArray[right] ) swap(center, right); swap(center, right-1); // put pivot on right return theArray[right-1]; // return median value }  // end medianOf3()   \n//-- public void swap(int dex1, int dex2) // swap two elements { long temp $mathbf { tau } = mathbf { tau }$ theArray[dex1]; // A into temp theArray[dex1] $mathbf { tau } = mathbf { tau }$ theArray[dex2]; // B into A theArray[dex2] $mathbf { tau } = mathbf { tau }$ temp; // temp into B }  // end swap(   \n//- public int partitionIt(int left, int right, long pivot) { int leftPtr $mathbf { tau } = mathbf { tau }$ left; // right of first elem int rightPtr $mathbf { tau } = mathbf { tau }$ right - 1; // left of pivot while(true) { while( theArray[++leftPtr] $mathit { Theta } _ { < }$ pivot ) $1 1$ find bigger ; $bigstar / / bigstar$ (nop) while( theArray[--rightPtr] $>$ pivot ) $bigstar / / bigstar$ find smaller \n// (nop) if(leftPtr $> =$ rightPtr) // if pointers cross, break; // partition done else // not crossed, so swap(leftPtr, rightPtr); // swap elements }  // end while(true) swap(leftPtr, right-1); // restore pivot return leftPtr; // return pivot location }  // end partitionIt() // public void manualSort(int left, int right) { int si $mathsf { Omega } : mathsf { e } ^ { mathrm { ~ tiny ~ { ~ cdot ~ } ~ } } = mathsf { r } mathrm { i } mathsf { g h t } cdot mathsf {  { mathrm { - } } } 1 mathsf { e } ^ { mathsf { f } mathrm { Theta } + mathrm { Theta } }$ t+1; $mathrm { i } mathsf { f } left( mathsf { s i z e }  < =  1 right)$ ) return; // no sort necessary i $textsf { f } ( textsf { s i z e } = 2 )$ ) { // 2-sort left and right if( theArray[left] $>$ theArray[right] ) swap(left, right); return; } else // size is 3 { // 3-sort left, center, & right if( theArray[left] $>$ theArray[right-1] ) swap(left, right-1); // left, center if( theArray[left] $>$ theArray[right] ) swap(left, right); // left, right if( theArray[right-1] $>$ theArray[right] ) swap(right-1, right); // center, right } } // end manualSort() // \n}  // end class ArrayIns   \n/////////////////////////////////////// //////////////////   \nclass QuickSort2App { public static void main(String[] args) { int maxSize $mathbf { Sigma } = mathbf { Sigma }$ 16; // array size ArrayIns arr; // reference to array arr $mathbf { tau } = mathbf { tau }$ new ArrayIns(maxSize); // create the array \nfor(int $scriptstyle mathbf { j } = 0$ ; j<maxSize; $mathrm { j } + + mathrm { Delta }$ ) $bigstar / / bigstar$ fill array with { $bigstar / / bigstar$ random numbers long ${ mathfrak { n } } =$ (int)(java.lang.Math.random( $) star 9 9$ ); arr.insert(n); } arr.display(); // display items arr.quickSort(); // quicksort them arr.display(); // display them again }  // end main() }  // end class QuickSort2App \nThis program uses another new method, manualSort(), to sort subarrays of three or fewer elements. It returns immediately if the subarray is one cell (or less), swaps the cells if necessary if the range is 2, and sorts three cells if the range is 3. The recQuickSort() routine can’t be used to sort ranges of 2 or 3 because median partitioning requires at least four cells. \nThe main() routine and the output of quickSort2.java are similar to those of quickSort1.java. \nThe QuickSort2 Workshop Applet \nThe Quicksort2 Workshop applet demonstrates the quicksort algorithm using median-of-three partitioning. This applet is similar to the QuickSort1 Workshop applet, but starts off sorting the first, center, and left elements of each subarray and selecting the median of these as the pivot value. At least, it does this if the array size is greater than 3. If the subarray is two or three units, the applet simply sorts it “by hand” without partitioning or recursive calls. \nNotice the dramatic improvement in performance when the applet is used to sort 100 inversely ordered bars. No longer is every subarray partitioned into 1 cell and N-1 cells; instead, the subarrays are partitioned roughly in half. \nOther than this improvement for ordered data, the QuickSort2 Workshop applet produces results similar to QuickSort1. It is no faster when sorting random data; it’s advantages become evident only when sorting ordered data. \nHandling Small Partitions \nIf you use the median-of-three partitioning method, it follows that the quicksort algorithm won’t work for partitions of three or fewer items. The number 3 in this case is called a cutoff point. In the examples above we sorted subarrays of two or three items by hand. Is this the best way? \nUsing an Insertion Sort for Small Partitions \nAnother option for dealing with small partitions is to use the insertion sort. When you do this, you aren’t restricted to a cutoff of 3. You can set the cutoff to 10, 20, or any other number. It’s interesting to experiment with different values of the cutoff to see where the best performance lies. Knuth (see Appendix B) recommends a cutoff of 9. However, the optimum number depends on your computer, operating system, compiler (or interpreter), and so on. \nThe quickSort3.java program, shown in Listing 7.5, uses an insertion sort to handle subarrays of fewer than 10 cells. \nLISTING 7.5 The quickSort3.java Program \n// quickSort3.java   \n// demonstrates quick sort; uses insertion sort for cleanup   \n// to run this program: $complement >$ java QuickSort3App   \n////////////////////////////////////////////////////////////////   \nclass ArrayIns { private long[] theArray; // ref to array theArray private int nElems; // number of data items   \n// public ArrayIns(int max) // constructor { theArray $mathbf { tau } = mathbf { tau }$ new long[max]; // create the array nElems $= 0$ ; $bigstar / / bigstar$ no items yet } public void insert(long value) $bigstar / / bigstar$ put element into array { theArray[nElems] $mathbf { tau } = mathbf { tau }$ value; $bigstar / / bigstar$ insert it nElems $^ { + + }$ ; $bigstar / / bigstar$ increment size }   \n// public void display() $bigstar / / bigstar$ displays array contents { System.out.print $\" mathsf { A } = \"$ ); for(int $scriptstyle mathbf { j } = 0$ ; j<nElems; ${ mathrm { j } } + +$ ) // for each element, System.out.print(theArray[j] $mathrm { ~ bf ~ underline { ~ } { ~ underline { ~ } { ~ bf ~ epsilon ~ } ~ } ~ } + mathrm { ~ bf ~ underline { ~ } { ~ bf ~ underline { ~ } { ~ bf ~ epsilon ~ } ~ } ~ } mathrm { ~ bf ~ underline { ~ } { ~ bf ~ epsilon ~ } ~ } )$ ;  // display it System.out.println(“”); } public void quickSort() \nLISTING 7.5 Continued \n{ recQuickSort(0, nElems-1); $bigstar / / bigstar$ insertionSort(0, nElems-1); // the other option $}$   \n// public void recQuickSort(int left, int right) { int size $mathbf { tau } = mathbf { tau }$ right-left+1; i $mathsf { f } ( mathsf { s i z e } < 1 0 )$ // insertion sort if small insertionSort(left, right); else // quicksort if large { long median $mathbf { tau } = mathbf { tau }$ medianOf3(left, right); int partition $mathbf { tau } = mathbf { tau }$ partitionIt(left, right, median); recQuickSort(left, partition-1); recQuickSort(partition+1, right); } } $bigstar / / bigstar$ end recQuickSort() public long medianOf3(int left, int right) { int center $mathbf { tau } = mathbf { tau }$ (left+right)/2; // order left & center if( theArray[left] $>$ theArray[center] ) swap(left, center); // order left & right if( theArray[left] $>$ theArray[right] ) swap(left, right); // order center & right if( theArray[center] $>$ theArray[right] ) swap(center, right); swap(center, right-1); // put pivot on right return theArray[right-1]; // return median value }  // end medianOf3()   \n// public void swap(int dex1, int dex2) // swap two elements { long temp $mathbf { tau } = mathbf { tau }$ theArray[dex1]; // A into temp theArray[dex1] $mathbf { Sigma } = mathbf { Sigma }$ theArray[dex2]; // B into A theArray[dex2] $mathbf { tau } = mathbf { tau }$ temp; // temp into B }  // end swap(   \n//- public int partitionIt(int left, int right, long pivot) { int leftPtr $mathbf { tau } = mathbf { tau }$ left; // right of first elem int rightPtr $mathbf { tau } = mathbf { tau }$ right - 1; // left of pivot while(true) { while( theArray $^ { + + }$ leftPtr] $mathit { Delta } <$ pivot )  // find bigger ; // (nop) while( theArray[--rightPtr] $>$ pivot ) $1 1$ find smaller ; // (nop) if(leftPtr $> =$ rightPtr) // if pointers cross, break; // partition done else // not crossed, so swap(leftPtr, rightPtr);  // swap elements }  // end while(true) swap(leftPtr, right-1); // restore pivot return leftPtr; // return pivot location }  // end partitionIt()   \n// // insertion sort public void insertionSort(int left, int right) { int in, out; //  sorted on left of out for( $0 mathsf { u t } = mathsf { l e f t } + 1$ ; out $< =$ right; out++) { long temp $mathbf { tau } = mathbf { tau }$ theArray[out]; $1 1$ remove marked item in $mathbf { tau } = mathbf { tau }$ out; $1 1$ start shifts at out // until one is smaller, while(in>left && theArray[in-1] $> =$ temp) { theArray[in] $mathbf { tau } = mathbf { tau }$ theArray[in-1]; // shift item to righ --in; // go left one position } theArray[in] $mathbf { tau } = mathbf { tau }$ temp; // insert marked item }  // end for } // end insertionSort()   \n//- } // end class ArrayIns \n\nLISTING 7.5 Continued \n///////////// //////// \nclass QuickSort3App { public static void main(String[] args) { int maxSize $= 1 6$ ; // array size ArrayIns arr; $bigstar / / bigstar$ reference to array arr $mathbf { tau } = mathbf { tau }$ new ArrayIns(maxSize); $bigstar / / bigstar$ create the array for(int $scriptstyle { mathrm { j } } = 0$ ; j<maxSize; ${ mathrm { j } } + +$ ) $bigstar / / bigstar$ fill array with { $bigstar / / bigstar$ random numbers long ${ mathfrak { n } } =$ (int)(java.lang.Math.random $( ) ^ { star } 9 9 $ ); arr.insert(n); } arr.display(); // display items arr.quickSort(); $bigstar / / bigstar$ quicksort them arr.display(); $bigstar / / bigstar$ display them again }  // end main() \nUsing the insertion sort for small subarrays turns out to be the fastest approach on our particular installation, but it is not much faster than sorting subarrays of three or fewer cells by hand, as in quickSort2.java. The numbers of comparisons and copies are reduced substantially in the quicksort phase, but are increased by an almost equal amount in the insertion sort, so the time savings are not dramatic. However, this approach is probably worthwhile if you are trying to squeeze the last ounce of performance out of quicksort. \nInsertion Sort Following Quicksort \nAnother option is to completely quicksort the array without bothering to sort partitions smaller than the cutoff. This is shown with a commented-out line in the quickSort() method. (If this call is used, the call to insertionSort() should be removed from recQuickSort().) When quicksort is finished, the array will be almost sorted. You then apply the insertion sort to the entire array. The insertion sort is supposed to operate efficiently on almost-sorted arrays, and this approach is recommended by some experts, but on our installation it runs very slowly. The insertion sort appears to be happier doing a lot of small sorts than one big one. \nRemoving Recursion \nAnother embellishment recommended by many writers is removing recursion from the quicksort algorithm. This involves rewriting the algorithm to store deferred \nsubarray bounds (left and right) on a stack, and using a loop instead of recursion to oversee the partitioning of smaller and smaller subarrays. The idea in doing this is to speed up the program by removing method calls. However, this idea arose with older compilers and computer architectures, which imposed a large time penalty for each method call. It’s not clear that removing recursion is much of an improvement for modern systems, which handle method calls more efficiently. \nEfficiency of Quicksort \nWe’ve said that quicksort operates in $mathrm { O ( N ^ { star } l o g N ) }$ time. As we saw in the discussion of mergesort in Chapter 6, this is generally true of the divide-and-conquer algorithms, in which a recursive method divides a range of items into two groups and then calls itself to handle each group. In this situation the logarithm actually has a base of 2: The running time is proportional to $mathrm { N } ^ { star } mathrm { l o g } _ { 2 } mathrm { N }$ . \nYou can get an idea of the validity of this $mathrm { N } ^ { star } mathrm { l o g } _ { 2 } mathrm { N }$ running time for quicksort by running one of the quickSort Workshop applets with 100 random bars and examining the resulting dotted horizontal lines. \nEach dotted line represents an array or subarray being partitioned: the pointers leftScan and rightScan moving toward each other, comparing each data item and swapping when appropriate. We saw in the “Partitioning” section that a single partition runs in O(N) time. This tells us that the total length of all the dotted lines is proportional to the running time of quicksort. But how long are all the lines? Measuring them with a ruler on the screen would be tedious, but we can visualize them a different way. \nThere is always 1 line that runs the entire width of the graph, spanning N bars. This results from the first partition. There will also be 2 lines (one below and one above the first line) that have an average length of $mathrm { N } / 2$ bars; together they are again N bars long. Then there will be 4 lines with an average length of $mathrm { N } / 4$ that again total N bars, then 8 lines, 16 lines, and so on. Figure 7.15 shows how this looks for 1, 2, 4, and 8 lines. \nIn this figure solid horizontal lines represent the dotted horizontal lines in the quicksort applets, and captions like N/4 cells long indicate average, not actual, line lengths. The circled numbers on the left show the order in which the lines are created. \nEach series of lines (the eight $_ { mathrm { N } / 8 }$ lines, for example) corresponds to a level of recursion. The initial call to recQuickSort() is the first level and makes the first line; the two calls from within the first call—the second level of recursion—make the next two lines; and so on. If we assume we start with 100 cells, the results are shown in Table 7.4. \nWhere does this division process stop? If we keep dividing 100 by 2, and count how many times we do this, we get the series 100, 50, 25, 12, 6, 3, 1, which is about seven levels of recursion. This looks about right on the workshop applets: If you pick some point on the graph and count all the dotted lines directly above and below it, there will be an average of approximately seven. (In Figure 7.15, because not all levels of recursion are shown, only four lines intersect any vertical slice of the graph.) \nTable 7.4 shows a total of 652 cells. This is only an approximation because of roundoff errors, but it’s close to 100 times the logarithm to the base 2 of 100, which is 6.65. Thus, this informal analysis suggests the validity of the $mathrm { N } ^ { star } mathrm { l o g } _ { 2 } mathrm { N }$ running time for quicksort. \nMore specifically, in the section on partitioning, we found that there should be $_ { mathrm { N } + 2 }$ comparisons and fewer than $mathrm { N } / 2$ swaps. Multiplying these quantities by $log _ { 2 } mathrm { N }$ for various values of $mathrm { Delta N }$ gives the results shown in Table 7.5. \nThe $log _ { 2 } mathrm { N }$ quantity used in Table 7.5 is actually true only in the best-case scenario, where each subarray is partitioned exactly in half. For random data the figure is slightly greater. Nevertheless, the QuickSort1 and QuickSort2 Workshop applets approximate these results for 12 and 100 bars, as you can see by running them and observing the Swaps and Comparisons fields. \nBecause they have different cutoff points and handle the resulting small partitions differently, QuickSort1 performs fewer swaps but more comparisons than QuickSort2. The number of swaps shown in Table 7.5 is the maximum (which assumes the data is inversely sorted). For random data the actual number of swaps turns out to be onehalf to two-thirds of the figures shown. \nRadix Sort \nWe’ll close this chapter by briefly mentioning a sort that uses a different approach. The sorts we’ve looked at so far treat the key as a simple numerical value that is compared with other values to sort the data. The radix sort disassembles the key into digits and arranges the data items according to the value of the digits. Amazingly, no comparisons are necessary.",
        "chapter": "CHAPTER 7 Advanced Sorting",
        "section": "Quicksort",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 13,
        "node_index": 69
      }
    },
    {
      "node_id": "tb13_node66",
      "content": "Experiments \nCarrying out these experiments will help to provide insights into the topics covered in the chapter. No programming is involved. \n1. In the triangle.java program (Listing 6.1), remove the code for the base case (the if $scriptstyle left[  n = = 1 right]$ ), the return 1;, and the else). Then run the program and see what happens.   \n2. Use the Towers Workshop applet in manual mode to solve the puzzle with seven or more disks.   \n3. Rewrite the main() part of mergeSort.java (Listing 6.6) so you can fill the array with hundreds of thousands of random numbers. Run the program to sort these numbers and compare its speed with the sorts in Chapter 3, “Simple Sorting.” \nProgramming Projects \nWriting programs that solve the Programming Projects helps to solidify your understanding of the material and demonstrates how the chapter’s concepts are applied. (As noted in the Introduction, qualified instructors may obtain completed solutions to the Programming Projects on the publisher’s Web site.) \n6.1 Suppose you buy a budget-priced pocket PC and discover that the chip inside can’t do multiplication, only addition. You program your way out of this quandary by writing a recursive method, mult(), that performs multiplication of x and y by adding x to itself y times. Its arguments are x and y and its return value is the product of x and y. Write such a method and a main() program to call it. Does the addition take place when the method calls itself or when it returns? \n6.2 In Chapter 8, “Binary Trees,” we’ll look at binary trees, where every branch has (potentially) exactly two sub-branches. If we draw a binary tree on the screen using characters, we might have 1 branch on the top row, 2 on the next row, then 4, 8, 16, and so on. Here’s what that looks like for a tree 16 characters wide: \n----X---- ---X-----X---X---X---X--X-X-X-X-X-X-X-XXXXXXXXXXXXXXXXX",
      "metadata": {
        "content": "Experiments \nCarrying out these experiments will help to provide insights into the topics covered in the chapter. No programming is involved. \n1. In the triangle.java program (Listing 6.1), remove the code for the base case (the if $scriptstyle left[  n = = 1 right]$ ), the return 1;, and the else). Then run the program and see what happens.   \n2. Use the Towers Workshop applet in manual mode to solve the puzzle with seven or more disks.   \n3. Rewrite the main() part of mergeSort.java (Listing 6.6) so you can fill the array with hundreds of thousands of random numbers. Run the program to sort these numbers and compare its speed with the sorts in Chapter 3, “Simple Sorting.” \nProgramming Projects \nWriting programs that solve the Programming Projects helps to solidify your understanding of the material and demonstrates how the chapter’s concepts are applied. (As noted in the Introduction, qualified instructors may obtain completed solutions to the Programming Projects on the publisher’s Web site.) \n6.1 Suppose you buy a budget-priced pocket PC and discover that the chip inside can’t do multiplication, only addition. You program your way out of this quandary by writing a recursive method, mult(), that performs multiplication of x and y by adding x to itself y times. Its arguments are x and y and its return value is the product of x and y. Write such a method and a main() program to call it. Does the addition take place when the method calls itself or when it returns? \n6.2 In Chapter 8, “Binary Trees,” we’ll look at binary trees, where every branch has (potentially) exactly two sub-branches. If we draw a binary tree on the screen using characters, we might have 1 branch on the top row, 2 on the next row, then 4, 8, 16, and so on. Here’s what that looks like for a tree 16 characters wide: \n----X---- ---X-----X---X---X---X--X-X-X-X-X-X-X-XXXXXXXXXXXXXXXXX",
        "chapter": "CHAPTER 6 Recursion",
        "section": "Experiments",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 13,
        "node_index": 66
      }
    },
    {
      "node_id": "tb13_node65",
      "content": "Questions \nThese questions are intended as a self-test for readers. Answers may be found in Appendix C. \n1. If the user enters 10 in the triangle.java program (Listing 6.1), what is the maximum number of “copies” of the triangle() method (actually just copies of its argument) that exist at any one time? \n2. Where are the copies of the argument, mentioned in question 1, stored? \na. in a variable in the triangle() method \nb. in a field of the TriangleApp class c. in a variable of the getString() method d. on a stack   \n3. Assume the user enters 10 as in question 1. What is the value of n when the triangle() method first returns a value other than 1?   \n4. Assume the same situation as in question 1. What is the value of n when the triangle() method is about to return to main()?   \n5. True or false: In the triangle() method, the return values are stored on the stack.   \n6. In the anagram.java program (Listing 6.2), at a certain depth of recursion, a version of the doAnagram() method is working with the string “led”. When this method calls a new version of itself, what letters will the new version be working with?   \n7. We’ve seen that recursion can take the place of a loop, as in the loop-oriented orderedArray.java program (Listing 2.4) and the recursive binarySearch.java program (Listing 6.3). Which of the following is not true? a. Both programs divide the range repeatedly in half. b. If the key is not found, the loop version returns because the range bounds cross, but the recursive version occurs because it reaches the bottom recursion level. c. If the key is found, the loop version returns from the entire method, whereas the recursive version returns from only one level of recursion. d. In the recursive version the range to be searched must be specified in the arguments, while in the loop version it need not be. \n8. In the recFind() method in the binarySearch.java program (Listing 6.3), what takes the place of the loop in the non-recursive version? \na. the recFind() method b. arguments to recFind() c. recursive calls to recFind() d. the call from main() to recFind() \n9. The binarySearch.java program is an example of the approach to solving a problem.   \n10. What gets smaller as you make repeated recursive calls in the redFind() method?   \n11. What becomes smaller with repeated recursive calls in the towers.java program (Listing 6.4)? \n12. The algorithm in the towers.java program involves \na. “trees” that are data storage devices.   \nb. secretly putting small disks under large disks.   \nc. changing which columns are the source and destination.   \nd. moving one small disk and then a stack of larger disks. \n13. Which is not true about the merge() method in the merge.java program (Listing 6.5)? \na. Its algorithm can handle arrays of different sizes.   \nb. It must search the target array to find where to put the next item.   \nc. It is not recursive.   \nd. It continuously takes the smallest item irrespective of what array it’s in. \n14. The disadvantage of mergesort is that \na. it is not recursive.   \nb. it uses more memory.   \nc. although faster than the insertion sort, it is much slower than quicksort.   \nd. it is complicated to implement. \n15. Besides a loop, a can often be used instead of recursion. \nExperiments \nCarrying out these experiments will help to provide insights into the topics covered in the chapter. No programming is involved. \n1. In the triangle.java program (Listing 6.1), remove the code for the base case (the if $scriptstyle left[  n = = 1 right]$ ), the return 1;, and the else). Then run the program and see what happens.   \n2. Use the Towers Workshop applet in manual mode to solve the puzzle with seven or more disks.   \n3. Rewrite the main() part of mergeSort.java (Listing 6.6) so you can fill the array with hundreds of thousands of random numbers. Run the program to sort these numbers and compare its speed with the sorts in Chapter 3, “Simple Sorting.” \nProgramming Projects \nWriting programs that solve the Programming Projects helps to solidify your understanding of the material and demonstrates how the chapter’s concepts are applied. (As noted in the Introduction, qualified instructors may obtain completed solutions to the Programming Projects on the publisher’s Web site.) \n6.1 Suppose you buy a budget-priced pocket PC and discover that the chip inside can’t do multiplication, only addition. You program your way out of this quandary by writing a recursive method, mult(), that performs multiplication of x and y by adding x to itself y times. Its arguments are x and y and its return value is the product of x and y. Write such a method and a main() program to call it. Does the addition take place when the method calls itself or when it returns? \n6.2 In Chapter 8, “Binary Trees,” we’ll look at binary trees, where every branch has (potentially) exactly two sub-branches. If we draw a binary tree on the screen using characters, we might have 1 branch on the top row, 2 on the next row, then 4, 8, 16, and so on. Here’s what that looks like for a tree 16 characters wide: \n----X---- ---X-----X---X---X---X--X-X-X-X-X-X-X-XXXXXXXXXXXXXXXXX",
      "metadata": {
        "content": "Questions \nThese questions are intended as a self-test for readers. Answers may be found in Appendix C. \n1. If the user enters 10 in the triangle.java program (Listing 6.1), what is the maximum number of “copies” of the triangle() method (actually just copies of its argument) that exist at any one time? \n2. Where are the copies of the argument, mentioned in question 1, stored? \na. in a variable in the triangle() method \nb. in a field of the TriangleApp class c. in a variable of the getString() method d. on a stack   \n3. Assume the user enters 10 as in question 1. What is the value of n when the triangle() method first returns a value other than 1?   \n4. Assume the same situation as in question 1. What is the value of n when the triangle() method is about to return to main()?   \n5. True or false: In the triangle() method, the return values are stored on the stack.   \n6. In the anagram.java program (Listing 6.2), at a certain depth of recursion, a version of the doAnagram() method is working with the string “led”. When this method calls a new version of itself, what letters will the new version be working with?   \n7. We’ve seen that recursion can take the place of a loop, as in the loop-oriented orderedArray.java program (Listing 2.4) and the recursive binarySearch.java program (Listing 6.3). Which of the following is not true? a. Both programs divide the range repeatedly in half. b. If the key is not found, the loop version returns because the range bounds cross, but the recursive version occurs because it reaches the bottom recursion level. c. If the key is found, the loop version returns from the entire method, whereas the recursive version returns from only one level of recursion. d. In the recursive version the range to be searched must be specified in the arguments, while in the loop version it need not be. \n8. In the recFind() method in the binarySearch.java program (Listing 6.3), what takes the place of the loop in the non-recursive version? \na. the recFind() method b. arguments to recFind() c. recursive calls to recFind() d. the call from main() to recFind() \n9. The binarySearch.java program is an example of the approach to solving a problem.   \n10. What gets smaller as you make repeated recursive calls in the redFind() method?   \n11. What becomes smaller with repeated recursive calls in the towers.java program (Listing 6.4)? \n12. The algorithm in the towers.java program involves \na. “trees” that are data storage devices.   \nb. secretly putting small disks under large disks.   \nc. changing which columns are the source and destination.   \nd. moving one small disk and then a stack of larger disks. \n13. Which is not true about the merge() method in the merge.java program (Listing 6.5)? \na. Its algorithm can handle arrays of different sizes.   \nb. It must search the target array to find where to put the next item.   \nc. It is not recursive.   \nd. It continuously takes the smallest item irrespective of what array it’s in. \n14. The disadvantage of mergesort is that \na. it is not recursive.   \nb. it uses more memory.   \nc. although faster than the insertion sort, it is much slower than quicksort.   \nd. it is complicated to implement. \n15. Besides a loop, a can often be used instead of recursion. \nExperiments \nCarrying out these experiments will help to provide insights into the topics covered in the chapter. No programming is involved. \n1. In the triangle.java program (Listing 6.1), remove the code for the base case (the if $scriptstyle left[  n = = 1 right]$ ), the return 1;, and the else). Then run the program and see what happens.   \n2. Use the Towers Workshop applet in manual mode to solve the puzzle with seven or more disks.   \n3. Rewrite the main() part of mergeSort.java (Listing 6.6) so you can fill the array with hundreds of thousands of random numbers. Run the program to sort these numbers and compare its speed with the sorts in Chapter 3, “Simple Sorting.” \nProgramming Projects \nWriting programs that solve the Programming Projects helps to solidify your understanding of the material and demonstrates how the chapter’s concepts are applied. (As noted in the Introduction, qualified instructors may obtain completed solutions to the Programming Projects on the publisher’s Web site.) \n6.1 Suppose you buy a budget-priced pocket PC and discover that the chip inside can’t do multiplication, only addition. You program your way out of this quandary by writing a recursive method, mult(), that performs multiplication of x and y by adding x to itself y times. Its arguments are x and y and its return value is the product of x and y. Write such a method and a main() program to call it. Does the addition take place when the method calls itself or when it returns? \n6.2 In Chapter 8, “Binary Trees,” we’ll look at binary trees, where every branch has (potentially) exactly two sub-branches. If we draw a binary tree on the screen using characters, we might have 1 branch on the top row, 2 on the next row, then 4, 8, 16, and so on. Here’s what that looks like for a tree 16 characters wide: \n----X---- ---X-----X---X---X---X--X-X-X-X-X-X-X-XXXXXXXXXXXXXXXXX",
        "chapter": "CHAPTER 6 Recursion",
        "section": "Questions",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 13,
        "node_index": 65
      }
    },
    {
      "node_id": "tb1_node16",
      "content": "1.5 Quicksort \nQuicksort is another recursive sorting algorithm, discovered by Tony Hoare in 1959 and first published in 1961. In this algorithm, the hard work is splitting the array into smaller subarrays before recursion, so that merging the sorted subarrays is trivial. \n1. Choose a pivot element from the array.   \n2. Partition the array into three subarrays containing the elements smaller than the pivot, the pivot element itself, and the elements larger than the pivot.   \n3. Recursively quicksort the first and last subarrays. Input: S O R T I N G E X A M P L   \nChoose a pivot: S O R T I N G E X A M P L Partition: A G O E I N L M P T X S R Recurse Left: A E G I L M N O P T X S R   \nRecurse Right: A E G I L M N O P R S T X \n\nMore detailed pseudocode is given in Figure 1.8. In the Partition subroutine, the input parameter $p$ is the index of the pivot element in the unsorted array; the subroutine partitions the array and returns the new index of the pivot element. There are many different efficient partitioning algorithms; the one I’m presenting here is attributed to Nico Lomuto.6 The variable $ell$ counts the number of items in the array that are ℓess than the pivot element. \nPartition(A[1 .. n], p): swap A[p] A[n] QuickSort(A[1 .. n]): ℓ 0 #items < pivot if $( n > 1 )$ 0 for $i gets 1$ to $n - 1$ Choose a pivot element A[p] if A[i] < A[n] r Partition(A, p) ℓ ℓ + 1 QuickSort(A[1 .. r 1]) Recurse! QuickSort(A[r + 1 .. n]) Recurse! swap $A [ ell ] longleftrightarrow A [ i ]$ $s mathrm { w a p } A [ n ] longleftrightarrow A [ ell + 1 ]$ return ℓ + 1 \nCorrectness \nJust like mergesort, proving that QuickSort is correct requires two separate induction proofs: one to prove that Partition correctly partitions the array, and the other to prove that QuickSort correctly sorts assuming Partition is correct. To prove Partition is correct, we need to prove the following loop invariant: At the end of each iteration of the main loop, everything in the subarray $A [ 1 ldots ell ]$ is ℓess than $A [ n ]$ , and nothing in the subarray $A [ ell + 1 ldots i ]$ is less than $boldsymbol { A } [ n ]$ . I’ll leave the remaining straightforward but tedious details as exercises for the reader. \n\nAnalysis \nThe analysis of quicksort is also similar to that of mergesort. Partition clearly runs in $O ( n )$ time, because it’s a simple for-loop with constant work per iteration. For QuickSort, we get a recurrence that depends on $r$ , the rank of the chosen pivot element: \nIf we could somehow always magically choose the pivot to be the median element of the array $A$ , we would have $r = lceil n / 2 rceil$ , the two subproblems would be as close to the same size as possible, the recurrence would become \nand we’d have $T ( n ) = O ( n log n )$ using either the recursion tree method or the even simpler “Oh yeah, we already solved that recurrence for mergesort” method. \nIn fact, as we will see later in this chapter, we can actually locate the median element in an unsorted array in linear time, but the algorithm is fairly complicated, and the hidden constant in the $O ( cdot )$ notation is large enough to make the resulting sorting algorithm impractical. In practice, most programmers settle for something simple, like choosing the first or last element of the array. In this case, $r$ can take any value between 1 and $n$ , so we have \nIn the worst case, the two subproblems are completely unbalanced—either $r = 1$ or $r = n$ —and the recurrence becomes $T ( n ) leq T ( n - 1 ) + O ( n )$ . The solution is $T ( n ) = O ( n ^ { 2 } )$ . \nAnother common heuristic is called “median of three”—choose three elements (usually at the beginning, middle, and end of the array), and take the median of those three elements as the pivot. Although this heuristic is somewhat more efficient in practice than just choosing one element, especially when the array is already (nearly) sorted, we can still have $r = 2$ or $r = n - 1$ in the worst case. With the median-of-three heuristic, the recurrence becomes $T ( n ) leq T ( 1 ) + T ( n - 2 ) + O ( n )$ , whose solution is still $T ( n ) = O ( n ^ { 2 } )$ .",
      "metadata": {
        "content": "1.5 Quicksort \nQuicksort is another recursive sorting algorithm, discovered by Tony Hoare in 1959 and first published in 1961. In this algorithm, the hard work is splitting the array into smaller subarrays before recursion, so that merging the sorted subarrays is trivial. \n1. Choose a pivot element from the array.   \n2. Partition the array into three subarrays containing the elements smaller than the pivot, the pivot element itself, and the elements larger than the pivot.   \n3. Recursively quicksort the first and last subarrays. Input: S O R T I N G E X A M P L   \nChoose a pivot: S O R T I N G E X A M P L Partition: A G O E I N L M P T X S R Recurse Left: A E G I L M N O P T X S R   \nRecurse Right: A E G I L M N O P R S T X \n\nMore detailed pseudocode is given in Figure 1.8. In the Partition subroutine, the input parameter $p$ is the index of the pivot element in the unsorted array; the subroutine partitions the array and returns the new index of the pivot element. There are many different efficient partitioning algorithms; the one I’m presenting here is attributed to Nico Lomuto.6 The variable $ell$ counts the number of items in the array that are ℓess than the pivot element. \nPartition(A[1 .. n], p): swap A[p] A[n] QuickSort(A[1 .. n]): ℓ 0 #items < pivot if $( n > 1 )$ 0 for $i gets 1$ to $n - 1$ Choose a pivot element A[p] if A[i] < A[n] r Partition(A, p) ℓ ℓ + 1 QuickSort(A[1 .. r 1]) Recurse! QuickSort(A[r + 1 .. n]) Recurse! swap $A [ ell ] longleftrightarrow A [ i ]$ $s mathrm { w a p } A [ n ] longleftrightarrow A [ ell + 1 ]$ return ℓ + 1 \nCorrectness \nJust like mergesort, proving that QuickSort is correct requires two separate induction proofs: one to prove that Partition correctly partitions the array, and the other to prove that QuickSort correctly sorts assuming Partition is correct. To prove Partition is correct, we need to prove the following loop invariant: At the end of each iteration of the main loop, everything in the subarray $A [ 1 ldots ell ]$ is ℓess than $A [ n ]$ , and nothing in the subarray $A [ ell + 1 ldots i ]$ is less than $boldsymbol { A } [ n ]$ . I’ll leave the remaining straightforward but tedious details as exercises for the reader. \n\nAnalysis \nThe analysis of quicksort is also similar to that of mergesort. Partition clearly runs in $O ( n )$ time, because it’s a simple for-loop with constant work per iteration. For QuickSort, we get a recurrence that depends on $r$ , the rank of the chosen pivot element: \nIf we could somehow always magically choose the pivot to be the median element of the array $A$ , we would have $r = lceil n / 2 rceil$ , the two subproblems would be as close to the same size as possible, the recurrence would become \nand we’d have $T ( n ) = O ( n log n )$ using either the recursion tree method or the even simpler “Oh yeah, we already solved that recurrence for mergesort” method. \nIn fact, as we will see later in this chapter, we can actually locate the median element in an unsorted array in linear time, but the algorithm is fairly complicated, and the hidden constant in the $O ( cdot )$ notation is large enough to make the resulting sorting algorithm impractical. In practice, most programmers settle for something simple, like choosing the first or last element of the array. In this case, $r$ can take any value between 1 and $n$ , so we have \nIn the worst case, the two subproblems are completely unbalanced—either $r = 1$ or $r = n$ —and the recurrence becomes $T ( n ) leq T ( n - 1 ) + O ( n )$ . The solution is $T ( n ) = O ( n ^ { 2 } )$ . \nAnother common heuristic is called “median of three”—choose three elements (usually at the beginning, middle, and end of the array), and take the median of those three elements as the pivot. Although this heuristic is somewhat more efficient in practice than just choosing one element, especially when the array is already (nearly) sorted, we can still have $r = 2$ or $r = n - 1$ in the worst case. With the median-of-three heuristic, the recurrence becomes $T ( n ) leq T ( 1 ) + T ( n - 2 ) + O ( n )$ , whose solution is still $T ( n ) = O ( n ^ { 2 } )$ .",
        "chapter": "Recursion",
        "section": "Quicksort",
        "subsection": "Correctness",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 16
      }
    }
  ],
  "output": {
    "question": "In the context of recursive sorting algorithms, how does the concept of recursion depth in mergesort (as implied by its divide-and-conquer structure) relate to the role of leaves in the Build-Max-Heap procedure used in heapsort?",
    "correct_answer": "In mergesort, the recursion depth determines how many times the array is divided into subarrays until base cases (typically single-element arrays) are reached. This is analogous to how in Build-Max-Heap, the leaves of the heap (elements in subarray A[⌊n/2⌋+1:n]) are already considered 1-element heaps—they represent the base cases in the bottom-up heap construction process. Both concepts illustrate how recursive algorithms and heap-based algorithms handle their simplest cases: in mergesort through recursive division until trivial subarrays, and in heapsort by starting with leaves as trivial heaps before building larger ones.",
    "distractors": [
      "Recursion depth in mergesort refers to the number of swaps performed, while leaves in Build-Max-Heap are ignored during sorting.",
      "Recursion depth determines the time complexity of quicksort, whereas leaves in Build-Max-Heap are only relevant for insertion sort.",
      "Recursion depth is related to stack memory usage in all algorithms, but leaves in Build-Max-Heap are solely about tree visualization without algorithmic significance."
    ],
    "rationale": "Step 1: Identify relevant snippets. Snippet [10] describes mergesort's recursive divide-and-conquer steps, implying recursion depth through repeated division. Snippet [2] describes Build-Max-Heap, noting that leaves (subarray A[⌊n/2⌋+1:n]) are 1-element heaps initially. Step 2: Connect concepts. Both involve base cases: mergesort recursively divides until subarrays are trivially sorted (single elements), analogous to how heap leaves are trivial heaps. Step 3: Formulate question. The question links recursion depth (from mergesort's structure) to the role of leaves (from heap construction), requiring synthesis of recursive and heap-based sorting concepts. Step 4: Ensure multi-hop. The answer must explain how recursion depth relates to leaves as base cases, drawing from both snippets without asking for simple definitions."
  },
  "score": {
    "relevance": 1.0,
    "faithfulness": 0.0,
    "integration": 1.0,
    "complexity": 1.0,
    "overall": 0.8
  }
}
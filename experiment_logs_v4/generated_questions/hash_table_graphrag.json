{
  "context": [
    {
      "node_id": "tb1_node72",
      "content": "Comparison \nTable 5.1 summarizes the performance of the various standard graph data structures. Stars∗ indicate expected amortized time bounds for maintaining dynamic hash tables.9 \nIn light of this comparison, one might reasonably wonder why anyone would ever use an adjacency matrix; after all, adjacency lists with hash tables support the same operations in the same time, using less space. The main reason is that for sufficiently dense graphs, adjacency matrices are simpler and more efficient in practice, because they avoid the overhead of chasing pointers and computing hash functions; they’re just contiguous blocks of memory. \nSimilarly, why would anyone use linked lists in an adjacency list structure to store neighbors, instead of balanced binary search trees or hash tables? Although the primary reason in practice is almost surely tradition—If they were good enough for Donald Knuth’s code, they should be good enough for yours!—there are more principled arguments. One is that standard adjacency lists are in fact good enough for most applications. Most standard graph algorithms never (or rarely) actually ask whether an arbitrary edge is present or absent, or attempt to insert or delete edges, and so optimizing the data structures to support those operations is unnecessary. \nBut in my opinion, the most compelling reason for both standard data structures is that many graphs are implicitly represented by adjacency matrices and standard adjacency lists. For example: \n• Intersection graphs are usually represented as a list of the underlying geometric objects. As long as we can test whether two objects intersect in constant time, we can apply any graph algorithm to an intersection graph by pretending that the input graph is stored explicitly as an adjacency matrix. • Any data structure composed from records with pointers between them can be seen as a directed graph. Graph algorithms can be applied to these data structures by pretending that the graph is stored in a standard adjacency list. \n• Similarly, we can apply any graph algorithm to a configuration graph as though it were represented as a standard adjacency list, provided we can enumerate all possible moves from a given configuration in constant time each. \nFor the last two examples, we can enumerate the edges leaving any vertex in time proportional to its degree, but we cannot necessarily determine in constant time if two vertices are adjacent. (Is there a pointer from this record to that record? Can we get from this configuration to that configuration in one move?) Moreover, we usually don’t have the luxury of reorganizing the pointers in each record or the moves out of a given configuration into a more efficient data structure. Thus, a standard adjacency list, with neighbors stored in linked lists, is the appropriate model data structure. \nIn the rest of this book, unless explicitly stated otherwise, all time bounds for graph algorithms assume that the input graph is represented by a standard adjacency list. Similarly, unless explicitly stated otherwise, when an exercise asks you to design and analyze a graph algorithm, you should assume that the input graph is represented in a standard adjacency list. \n5.5 Whatever-First Search \nSo far we have only discussed local operations on graphs; arguably the most fundamental global question we can ask about graphs is reachability. Given a graph $G$ and a vertex s in $G$ , the reachability question asks which vertices are reachable from $s$ ; that is, for which vertices $nu$ is there a path from $s$ to $nu ?$ For now, let’s consider only undirected graphs; I’ll consider directed graphs briefly at the end of this section. For undirected graphs, the vertices reachable from s are precisely the vertices in the same component as $s$ . \nPerhaps the most natural reachability algorithm—at least for people like us who are used to thinking recursively—is depth-first search. This algorithm can be written either recursively or iteratively. It’s exactly the same algorithm either way; the only difference is that we can actually see the “recursion” stack in the non-recursive version.",
      "metadata": {
        "content": "Comparison \nTable 5.1 summarizes the performance of the various standard graph data structures. Stars∗ indicate expected amortized time bounds for maintaining dynamic hash tables.9 \nIn light of this comparison, one might reasonably wonder why anyone would ever use an adjacency matrix; after all, adjacency lists with hash tables support the same operations in the same time, using less space. The main reason is that for sufficiently dense graphs, adjacency matrices are simpler and more efficient in practice, because they avoid the overhead of chasing pointers and computing hash functions; they’re just contiguous blocks of memory. \nSimilarly, why would anyone use linked lists in an adjacency list structure to store neighbors, instead of balanced binary search trees or hash tables? Although the primary reason in practice is almost surely tradition—If they were good enough for Donald Knuth’s code, they should be good enough for yours!—there are more principled arguments. One is that standard adjacency lists are in fact good enough for most applications. Most standard graph algorithms never (or rarely) actually ask whether an arbitrary edge is present or absent, or attempt to insert or delete edges, and so optimizing the data structures to support those operations is unnecessary. \nBut in my opinion, the most compelling reason for both standard data structures is that many graphs are implicitly represented by adjacency matrices and standard adjacency lists. For example: \n• Intersection graphs are usually represented as a list of the underlying geometric objects. As long as we can test whether two objects intersect in constant time, we can apply any graph algorithm to an intersection graph by pretending that the input graph is stored explicitly as an adjacency matrix. • Any data structure composed from records with pointers between them can be seen as a directed graph. Graph algorithms can be applied to these data structures by pretending that the graph is stored in a standard adjacency list. \n• Similarly, we can apply any graph algorithm to a configuration graph as though it were represented as a standard adjacency list, provided we can enumerate all possible moves from a given configuration in constant time each. \nFor the last two examples, we can enumerate the edges leaving any vertex in time proportional to its degree, but we cannot necessarily determine in constant time if two vertices are adjacent. (Is there a pointer from this record to that record? Can we get from this configuration to that configuration in one move?) Moreover, we usually don’t have the luxury of reorganizing the pointers in each record or the moves out of a given configuration into a more efficient data structure. Thus, a standard adjacency list, with neighbors stored in linked lists, is the appropriate model data structure. \nIn the rest of this book, unless explicitly stated otherwise, all time bounds for graph algorithms assume that the input graph is represented by a standard adjacency list. Similarly, unless explicitly stated otherwise, when an exercise asks you to design and analyze a graph algorithm, you should assume that the input graph is represented in a standard adjacency list. \n5.5 Whatever-First Search \nSo far we have only discussed local operations on graphs; arguably the most fundamental global question we can ask about graphs is reachability. Given a graph $G$ and a vertex s in $G$ , the reachability question asks which vertices are reachable from $s$ ; that is, for which vertices $nu$ is there a path from $s$ to $nu ?$ For now, let’s consider only undirected graphs; I’ll consider directed graphs briefly at the end of this section. For undirected graphs, the vertices reachable from s are precisely the vertices in the same component as $s$ . \nPerhaps the most natural reachability algorithm—at least for people like us who are used to thinking recursively—is depth-first search. This algorithm can be written either recursively or iteratively. It’s exactly the same algorithm either way; the only difference is that we can actually see the “recursion” stack in the non-recursive version.",
        "chapter": "Basic Graph Algorithms",
        "section": "Data Structures",
        "subsection": "Comparison",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 72
      }
    },
    {
      "node_id": "tb1_node68",
      "content": "Of course, there are many other familiar examples of graphs, like board games (dating to antiquity); vertices and edges of convex polyhedra (formally studied by ancient Greek philosophers, but much older); visualizations of star patterns (already developed in East Asia by the 7th century ce); knight’s tours (described by al-Adli, Rudrat.a, al-Suli, and others in the 9th and 10th centuries), mazes (introduced in their modern form by Giovanni Fontana circa 1420); geodetic triangulations (introduced by Gemma Frisius in 1533, and used to calculate the circumference of the earth by Willebrod Snell in 1615 and to define the meter in 1799), Leonhard Euler’s well-known partial2 solution to the Bridges of Königsburg puzzle (1735); telegraph and other communication networks (first proposed in 1753, developed by Ronalds, Schilling, Gauss, Weber, and others in the early 1800s, and deployed worldwide by the late 1800s); electrical circuits (formalized in the early 1800s by Ohm, Maxwell, Kirchhoff, and others); molecular structural formulas (introduced independently by August Kekulé in 1857 and Archibald Couper in 1858); social networks (first studied in the mid-1930s by sociologist Jacob Moreno); digital electronic circuits (proposed by Charles Sanders Peirce in 1886, and cast into their modern form by Claude Shannon in 1937); and yeah, okay, if you insist, the modern internet. \nThe word “graph” for the abstract mathematical was coined by James Sylvester in $^ { 1 8 7 8 }$ , who adapted Kekulé’s “chemicographs” to describe certain algebraic invariants, at the suggestion of his colleague William Clifford. The word “tree” was first used for connected acyclic graphs by Arthur Cayley in 1857, although the abstract concept of trees had already been used by Gustav Kirchhoff and Karl von Staudt ten years earlier. The zeroth book on graph theory was published by André Sainte-Laguë in 1926; Dénes Kőnig published the first graph theory book ten years later. \n5.2 Basic Definitions \nFormally, a (simple) graph is a pair of sets $( V , E )$ , where $V$ is an arbitrary non-empty finite set, whose elements are called vertices3 or nodes, and $E$ is a set of pairs of elements of $V$ , which we call edges. In an undirected graph, the edges are unordered pairs, or just sets of size two; I usually write $boldsymbol { u } nu$ instead of ${ u , nu }$ to denote the undirected edge between $u$ and $nu$ . In a directed graph, the edges are ordered pairs of vertices; I usually write $mathbf { Delta } u to nu$ instead of $( u , nu )$ to denote the directed edge from $u$ to $nu$ . \nFollowing standard (but admittedly confusing) practice, I will also use $V$ to denote the number of vertices in a graph, and $E$ to denote the number of edges. Thus, in any undirected graph we have $begin{array} { r } { 0 leq E leq binom { V } { 2 } } end{array}$ , and in any directed graph we have $0 leq E leq V ( V - 1 )$ . \nThe endpoints of an edge $u nu$ or $u {  } nu$ are its vertices $u$ and $nu$ . We distinguish the endpoints of a directed edge $u {  } nu$ \u0001by calling $u$ the tail and $nu$ the head. \nThe definition of a graph as a p\u0001air of sets forbids multiple undirected edges with the same endpoints, or multiple directed edges with the same head and the same tail. (The same directed graph can contain both a directed edge $u {  } nu$ and its reversal $nu {  } u .$ .) Similarly, the definition of an undirected edge as a\u0001set of vertices forbids\u0001an undirected edge from a vertex to itself. Graphs without loops and parallel edges are often called simple graphs; non-simple graphs are sometimes called multigraphs. Despite the formal definitional gap, most algorithms for simple graphs extend to multigraphs with little or no modification, and for that reason, I see no need for a formal definition here. \nFor any edge uv in an undirected graph, we call $u$ a neighbor of $nu$ and vice versa, and we say that $u$ and $nu$ are adjacent. The degree of a node is its number of neighbors. In directed graphs, we distinguish two kinds of neighbors. For any directed edge $u {  } nu$ , we call $u$ a predecessor of $nu$ , and we call $nu$ a successor of $u$ . The in-degree\u0001of a vertex is its number of predecessors; the out-degree is its number of successors. \nA graph $G ^ { prime } = ( V ^ { prime } , E ^ { prime } )$ is a subgraph of $G = ( V , E )$ if $V ^ { prime } subseteq V$ and $E ^ { prime } subseteq E$ . A proper subgraph of $G$ is any subgraph other than $G$ itself. \nA walk in an undirected graph $G$ is a sequence of vertices, where each adjacent pair of vertices are adjacent in $G$ ; informally, we can also think of a walk as a sequence of edges. A walk is called a path if it visits each vertex at most once. For any two vertices $u$ and $nu$ in a graph $G$ , we say that $nu$ is reachable from $u$ if $G$ contains a walk (and therefore a path) between $u$ and $nu$ . An undirected graph is connected if every vertex is reachable from every other vertex. Every undirected graph consists of one or more components, which are its maximal connected subgraphs; two vertices are in the same component if and only if there is a path between them.4 \nA walk is closed if it starts and ends at the same vertex; a cycle is a closed walk that enters and leaves each vertex at most once. An undirected graph is acyclic if no subgraph is a cycle; acyclic graphs are also called forests. A tree is a connected acyclic graph, or equivalently, one component of a forest. A spanning tree of an undirected graph $G$ is a subgraph that is a tree and contains every vertex of $G$ . A graph has a spanning tree if and only if it is connected. A spanning forest of $G$ is a collection of spanning trees, one for each component of $G$ . \n\nDirected graphs require slightly different definitions. A directed walk is a sequence of vertices $nu _ { 0 } {  } nu _ { 1 } {  } nu _ { 2 } {  } cdots {  } nu _ { ell }$ such that $nu _ { i - 1 } {  } nu _ { i }$ is a directed edge for every index $i$ ; directed\u0001path\u0001s an\u0001d dir\u0001ected cycles are de\u0001fined similarly. Vertex $nu$ is reachable from vertex $u$ in a directed graph $G$ if and only if $G$ contains a directed walk (and therefore a directed path) from $u$ to $nu$ . A directed graph is strongly connected if every vertex is reachable from every other vertex. A directed graph is acyclic if it does not contain a directed cycle; directed acyclic graphs are often called dags. \n5.3 Representations and Examples \nThe most common way to visually represent graphs is by drawing them. A drawing of a graph maps each vertex to a point in the plane (typically drawn as a small circle or some other shape) and each edge to a curve or straight line segment between the two vertices. A graph is planar if it has a drawing where no two edges cross; such a drawing is also called an embedding.5 The same graph can have many different drawings, so it is important not to confuse a particular drawing with the graph itself. In particular, planar graphs can have non-planar drawings! \nHowever, drawings are far from the only useful representation of graphs. For example, the intersection graph of a collection of geometric objects has a node for every object and an edge for every intersecting pair of objects. Whether a particular graph can be represented as an intersection graph depends on what kind of object you want to use for the vertices. Different types of objects—line segments, rectangles, circles, etc.—define different classes of graphs. One particularly useful type of intersection graph is an interval graph, whose vertices are intervals on the real line, with an edge between any two intervals that overlap.",
      "metadata": {
        "content": "Of course, there are many other familiar examples of graphs, like board games (dating to antiquity); vertices and edges of convex polyhedra (formally studied by ancient Greek philosophers, but much older); visualizations of star patterns (already developed in East Asia by the 7th century ce); knight’s tours (described by al-Adli, Rudrat.a, al-Suli, and others in the 9th and 10th centuries), mazes (introduced in their modern form by Giovanni Fontana circa 1420); geodetic triangulations (introduced by Gemma Frisius in 1533, and used to calculate the circumference of the earth by Willebrod Snell in 1615 and to define the meter in 1799), Leonhard Euler’s well-known partial2 solution to the Bridges of Königsburg puzzle (1735); telegraph and other communication networks (first proposed in 1753, developed by Ronalds, Schilling, Gauss, Weber, and others in the early 1800s, and deployed worldwide by the late 1800s); electrical circuits (formalized in the early 1800s by Ohm, Maxwell, Kirchhoff, and others); molecular structural formulas (introduced independently by August Kekulé in 1857 and Archibald Couper in 1858); social networks (first studied in the mid-1930s by sociologist Jacob Moreno); digital electronic circuits (proposed by Charles Sanders Peirce in 1886, and cast into their modern form by Claude Shannon in 1937); and yeah, okay, if you insist, the modern internet. \nThe word “graph” for the abstract mathematical was coined by James Sylvester in $^ { 1 8 7 8 }$ , who adapted Kekulé’s “chemicographs” to describe certain algebraic invariants, at the suggestion of his colleague William Clifford. The word “tree” was first used for connected acyclic graphs by Arthur Cayley in 1857, although the abstract concept of trees had already been used by Gustav Kirchhoff and Karl von Staudt ten years earlier. The zeroth book on graph theory was published by André Sainte-Laguë in 1926; Dénes Kőnig published the first graph theory book ten years later. \n5.2 Basic Definitions \nFormally, a (simple) graph is a pair of sets $( V , E )$ , where $V$ is an arbitrary non-empty finite set, whose elements are called vertices3 or nodes, and $E$ is a set of pairs of elements of $V$ , which we call edges. In an undirected graph, the edges are unordered pairs, or just sets of size two; I usually write $boldsymbol { u } nu$ instead of ${ u , nu }$ to denote the undirected edge between $u$ and $nu$ . In a directed graph, the edges are ordered pairs of vertices; I usually write $mathbf { Delta } u to nu$ instead of $( u , nu )$ to denote the directed edge from $u$ to $nu$ . \nFollowing standard (but admittedly confusing) practice, I will also use $V$ to denote the number of vertices in a graph, and $E$ to denote the number of edges. Thus, in any undirected graph we have $begin{array} { r } { 0 leq E leq binom { V } { 2 } } end{array}$ , and in any directed graph we have $0 leq E leq V ( V - 1 )$ . \nThe endpoints of an edge $u nu$ or $u {  } nu$ are its vertices $u$ and $nu$ . We distinguish the endpoints of a directed edge $u {  } nu$ \u0001by calling $u$ the tail and $nu$ the head. \nThe definition of a graph as a p\u0001air of sets forbids multiple undirected edges with the same endpoints, or multiple directed edges with the same head and the same tail. (The same directed graph can contain both a directed edge $u {  } nu$ and its reversal $nu {  } u .$ .) Similarly, the definition of an undirected edge as a\u0001set of vertices forbids\u0001an undirected edge from a vertex to itself. Graphs without loops and parallel edges are often called simple graphs; non-simple graphs are sometimes called multigraphs. Despite the formal definitional gap, most algorithms for simple graphs extend to multigraphs with little or no modification, and for that reason, I see no need for a formal definition here. \nFor any edge uv in an undirected graph, we call $u$ a neighbor of $nu$ and vice versa, and we say that $u$ and $nu$ are adjacent. The degree of a node is its number of neighbors. In directed graphs, we distinguish two kinds of neighbors. For any directed edge $u {  } nu$ , we call $u$ a predecessor of $nu$ , and we call $nu$ a successor of $u$ . The in-degree\u0001of a vertex is its number of predecessors; the out-degree is its number of successors. \nA graph $G ^ { prime } = ( V ^ { prime } , E ^ { prime } )$ is a subgraph of $G = ( V , E )$ if $V ^ { prime } subseteq V$ and $E ^ { prime } subseteq E$ . A proper subgraph of $G$ is any subgraph other than $G$ itself. \nA walk in an undirected graph $G$ is a sequence of vertices, where each adjacent pair of vertices are adjacent in $G$ ; informally, we can also think of a walk as a sequence of edges. A walk is called a path if it visits each vertex at most once. For any two vertices $u$ and $nu$ in a graph $G$ , we say that $nu$ is reachable from $u$ if $G$ contains a walk (and therefore a path) between $u$ and $nu$ . An undirected graph is connected if every vertex is reachable from every other vertex. Every undirected graph consists of one or more components, which are its maximal connected subgraphs; two vertices are in the same component if and only if there is a path between them.4 \nA walk is closed if it starts and ends at the same vertex; a cycle is a closed walk that enters and leaves each vertex at most once. An undirected graph is acyclic if no subgraph is a cycle; acyclic graphs are also called forests. A tree is a connected acyclic graph, or equivalently, one component of a forest. A spanning tree of an undirected graph $G$ is a subgraph that is a tree and contains every vertex of $G$ . A graph has a spanning tree if and only if it is connected. A spanning forest of $G$ is a collection of spanning trees, one for each component of $G$ . \n\nDirected graphs require slightly different definitions. A directed walk is a sequence of vertices $nu _ { 0 } {  } nu _ { 1 } {  } nu _ { 2 } {  } cdots {  } nu _ { ell }$ such that $nu _ { i - 1 } {  } nu _ { i }$ is a directed edge for every index $i$ ; directed\u0001path\u0001s an\u0001d dir\u0001ected cycles are de\u0001fined similarly. Vertex $nu$ is reachable from vertex $u$ in a directed graph $G$ if and only if $G$ contains a directed walk (and therefore a directed path) from $u$ to $nu$ . A directed graph is strongly connected if every vertex is reachable from every other vertex. A directed graph is acyclic if it does not contain a directed cycle; directed acyclic graphs are often called dags. \n5.3 Representations and Examples \nThe most common way to visually represent graphs is by drawing them. A drawing of a graph maps each vertex to a point in the plane (typically drawn as a small circle or some other shape) and each edge to a curve or straight line segment between the two vertices. A graph is planar if it has a drawing where no two edges cross; such a drawing is also called an embedding.5 The same graph can have many different drawings, so it is important not to confuse a particular drawing with the graph itself. In particular, planar graphs can have non-planar drawings! \nHowever, drawings are far from the only useful representation of graphs. For example, the intersection graph of a collection of geometric objects has a node for every object and an edge for every intersecting pair of objects. Whether a particular graph can be represented as an intersection graph depends on what kind of object you want to use for the vertices. Different types of objects—line segments, rectangles, circles, etc.—define different classes of graphs. One particularly useful type of intersection graph is an interval graph, whose vertices are intervals on the real line, with an edge between any two intervals that overlap.",
        "chapter": "Basic Graph Algorithms",
        "section": "Basic Definitions",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 68
      }
    },
    {
      "node_id": "tb1_node73",
      "content": "Every pair $( nu , p a r e n t ( nu ) )$ with $p a r e n t ( nu ) neq emptyset$ is actually an edge in the underlying graph $G$ . We claim that for any marked vertex $nu$ , the path of parent edges $nu {  } p a r e n t ( nu ) {  } p a r e n t ( p a r e n t ( nu ) ) {  } cdots$ eventually leads back to $s$ ; we prove this clai\u0001m by induct\u0001ion on the order in\u0001which vertices are marked. Trivially $s$ is reachable from s, so let $nu$ be any other marked vertex. The parent of $nu$ must be marked before $nu$ is marked, so the inductive hypothesis implies that the parent path paren $cdot ( nu ) {  } p a r e n t ( p a r e n t ( nu ) ) {  } cdots$ leads to $s$ ; adding one more parent edge $s {  } p a r e n t ( s )$ est\u0001ablishes the claim. \nThe previous claim implies that every vertex marked by the algorithm is reachable from s, and that the set of all parent edges forms a connected graph. Because every marked node except $s$ has a unique parent, the number of parent edges is exactly one less than the number of marked vertices. We conclude that the parent edges form a tree. □ \nAnalysis \nThe running time of the traversal algorithm depends on what data structure we use for the “bag”, but we can make a few general observations. Let $T$ is the time required to insert a single item into the bag or delete a single item from the bag. The for loop $( dag )$ is executed exactly once for each marked vertex, and therefore at most $V$ times. Each edge $u nu$ in the component of $s$ is put into the bag exactly twice; once as the pair $left( u , nu right)$ and once as the pair $( nu , u )$ , so line $( { star } { star } )$ is executed at most $2 E$ times. Finally, we can’t take more things out of the bag than we put in, so line $( { star } )$ is executed at most $2 E + 1$ times. Thus, assuming the underlying graph $G$ is stored in a standard adjacency list, WhateverFirstSearch runs in $O ( V + E T )$ time. (If $G$ is stored in an adjacency matrix, the running time of WhateverFirstSearch increases to $O ( V ^ { 2 } + E T ) .$ ) \n5.6 Important Variants \nStack: Depth-First \nIf we implement the “bag” using a stack, we recover our original depth-first search algorithm. Stacks support insertions (push) and deletions (pop) in $O ( 1 )$ time each, so the algorithm runs in $O ( V + E )$ time. The spanning tree formed by the parent edges is called a depth-first spanning tree. The exact shape of the tree depends on the start vertex and on the order that neighbors are visited inside the for loop $( dagger )$ , but in general, depth-first spanning trees are long and skinny. We will consider several important properties and applications of depth-first search in Chapter 6.",
      "metadata": {
        "content": "Every pair $( nu , p a r e n t ( nu ) )$ with $p a r e n t ( nu ) neq emptyset$ is actually an edge in the underlying graph $G$ . We claim that for any marked vertex $nu$ , the path of parent edges $nu {  } p a r e n t ( nu ) {  } p a r e n t ( p a r e n t ( nu ) ) {  } cdots$ eventually leads back to $s$ ; we prove this clai\u0001m by induct\u0001ion on the order in\u0001which vertices are marked. Trivially $s$ is reachable from s, so let $nu$ be any other marked vertex. The parent of $nu$ must be marked before $nu$ is marked, so the inductive hypothesis implies that the parent path paren $cdot ( nu ) {  } p a r e n t ( p a r e n t ( nu ) ) {  } cdots$ leads to $s$ ; adding one more parent edge $s {  } p a r e n t ( s )$ est\u0001ablishes the claim. \nThe previous claim implies that every vertex marked by the algorithm is reachable from s, and that the set of all parent edges forms a connected graph. Because every marked node except $s$ has a unique parent, the number of parent edges is exactly one less than the number of marked vertices. We conclude that the parent edges form a tree. □ \nAnalysis \nThe running time of the traversal algorithm depends on what data structure we use for the “bag”, but we can make a few general observations. Let $T$ is the time required to insert a single item into the bag or delete a single item from the bag. The for loop $( dag )$ is executed exactly once for each marked vertex, and therefore at most $V$ times. Each edge $u nu$ in the component of $s$ is put into the bag exactly twice; once as the pair $left( u , nu right)$ and once as the pair $( nu , u )$ , so line $( { star } { star } )$ is executed at most $2 E$ times. Finally, we can’t take more things out of the bag than we put in, so line $( { star } )$ is executed at most $2 E + 1$ times. Thus, assuming the underlying graph $G$ is stored in a standard adjacency list, WhateverFirstSearch runs in $O ( V + E T )$ time. (If $G$ is stored in an adjacency matrix, the running time of WhateverFirstSearch increases to $O ( V ^ { 2 } + E T ) .$ ) \n5.6 Important Variants \nStack: Depth-First \nIf we implement the “bag” using a stack, we recover our original depth-first search algorithm. Stacks support insertions (push) and deletions (pop) in $O ( 1 )$ time each, so the algorithm runs in $O ( V + E )$ time. The spanning tree formed by the parent edges is called a depth-first spanning tree. The exact shape of the tree depends on the start vertex and on the order that neighbors are visited inside the for loop $( dagger )$ , but in general, depth-first spanning trees are long and skinny. We will consider several important properties and applications of depth-first search in Chapter 6.",
        "chapter": "Basic Graph Algorithms",
        "section": "Whatever-First Search",
        "subsection": "Analysis",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 73
      }
    },
    {
      "node_id": "tb1_node69",
      "content": "Directed graphs require slightly different definitions. A directed walk is a sequence of vertices $nu _ { 0 } {  } nu _ { 1 } {  } nu _ { 2 } {  } cdots {  } nu _ { ell }$ such that $nu _ { i - 1 } {  } nu _ { i }$ is a directed edge for every index $i$ ; directed\u0001path\u0001s an\u0001d dir\u0001ected cycles are de\u0001fined similarly. Vertex $nu$ is reachable from vertex $u$ in a directed graph $G$ if and only if $G$ contains a directed walk (and therefore a directed path) from $u$ to $nu$ . A directed graph is strongly connected if every vertex is reachable from every other vertex. A directed graph is acyclic if it does not contain a directed cycle; directed acyclic graphs are often called dags. \n5.3 Representations and Examples \nThe most common way to visually represent graphs is by drawing them. A drawing of a graph maps each vertex to a point in the plane (typically drawn as a small circle or some other shape) and each edge to a curve or straight line segment between the two vertices. A graph is planar if it has a drawing where no two edges cross; such a drawing is also called an embedding.5 The same graph can have many different drawings, so it is important not to confuse a particular drawing with the graph itself. In particular, planar graphs can have non-planar drawings! \nHowever, drawings are far from the only useful representation of graphs. For example, the intersection graph of a collection of geometric objects has a node for every object and an edge for every intersecting pair of objects. Whether a particular graph can be represented as an intersection graph depends on what kind of object you want to use for the vertices. Different types of objects—line segments, rectangles, circles, etc.—define different classes of graphs. One particularly useful type of intersection graph is an interval graph, whose vertices are intervals on the real line, with an edge between any two intervals that overlap. \n\nAnother good example is the dependency graph of a recursive algorithm. Dependency graphs are directed acyclic graphs. The vertices are all the distinct recursive subproblems that arise when executing the algorithm on a particular input. There is an edge from one subproblem to another if evaluating the second subproblem requires a recursive evaluation of the first. For example, for the Fibonacci recurrence \nthe vertices of the dependency graph are the integers $0 , 1 , 2 , ldots , n$ , and the edges are the pairs $( i - 1 ) { to } i$ and $( i - 2 ) substack {  i }$ for every integer $i$ between 2 and $n$ . \nAs a more complex example, recall the recurrence for the edit distance problem from Chapter 3: \nThe dependency graph of this recurrence is an $m times n$ grid of vertices $( i , j )$ connected by vertical edges $( i - 1 , j ) { to } ( i , j )$ , horizontal edges $( i , j - 1 ) {  } ( i , j )$ , and diagonal edges $( i - 1 , j - 1 ) { to } ( i , j )$ .\u0001Dynamic programming works effic\u0001iently for any recurrence that has a r\u0001easonably small dependency graph; a proper evaluation order ensures that each subproblem is visited after its predecessors. \nAnother interesting example is the configuration graph of a game, puzzle, or mechanism like tic-tac-toe, checkers, the Rubik’s Cube, the Tower of Hanoi, or a Turing machine. The vertices of the configuration graph are all the valid configurations of the puzzle; there is an edge from one configuration to another if it is possible to transform one configuration into the other with a single simple “move”. (Obviously, the precise definition depends on what moves are allowed.) Even for reasonably simple mechanisms, the configuration graph can be extremely complex, and we typically only have access to local information about the configuration graph. \nConfiguration graphs are close relatives of the game trees we considered in Chapter 2, but with one crucial difference. Each state of a game appears exactly once in its configuration graph, but can appear many times in its game tree. In short, configuration graphs are memoized game trees! \nFinite-state automata used in formal language theory can be modeled as labeled directed graphs. Recall that a deterministic finite-state automaton is formally defined as a 5-tuple $M = ( Sigma , Q , s , A , delta )$ , where $Sigma$ is a finite set called the alphabet, $Q$ is a finite set of states, $s in Q$ is the start state, $A subseteq Q$ is the set of accepting states, and $delta colon Q times Sigma to Q$ is a transition function. But it is often more useful to think of $M$ as a directed graph $G _ { M }$ whose vertices are the states $Q$ , and whose edges have the form $q to delta ( q , a )$ for every state $q in Q$ and symbol $a in Sigma$ . Basic questions about the la\u0001nguage $L ( M )$ accepted by $M$ can then be phrased as questions about the graph $G _ { M }$ . For example, $L ( M ) = emptyset$ if and only if no accepting state/vertex is reachable from the start state/vertex s. \n\nFinally, sometimes one graph can be used to implicitly represent other larger graphs. A good example of this implicit representation is the subset construction, which is normally used to convert NFAs into DFAs, but can be applied to arbitrary directed graphs as follows. Given any directed graph $G = left( V , E right)$ , we can define a new directed graph $G ^ { prime } = ( 2 ^ { V } , E ^ { prime } )$ whose vertices are all subsets of vertices in $V$ , and whose edges $E ^ { prime }$ are defined as follows: \nWe can mechanically translate this definition into an algorithm to construct $G ^ { prime }$ from $G$ , but strictly speaking, this construction is unnecessary, because $G$ is already an implicit representation of $G ^ { prime }$ . \nIt’s important not to confuse any of these examples/representations with the actual formal definition: A graph is a pair of sets $( V , E )$ , where $V$ is an arbitrary non-empty finite set, and $E$ is a set of pairs (either ordered or unordered) of elements of $V$ . In short: A graph is a set of pairs of things. \n5.4 Data Structures \nIn practice, graphs are usually represented by one of two standard data structures: adjacency lists and adjacency matrices. At a high level, both data structures are arrays indexed by vertices; this requires that each vertex has a unique integer identifier between 1 and $V$ . In a formal sense, these integers are the vertices. \nAdjacency Lists \nBy far the most common data structure for storing graphs is the adjacency list. An adjacency list is an array of lists, each containing the neighbors of one of the vertices (or the out-neighbors if the graph is directed).6 For undirected graphs, each edge $u nu$ is stored twice, once in u’s neighbor list and once in v’s neighbor list; for directed graphs, each edge $u {  } nu$ is stored only once, in the neighbor list of the tail $u$ . For both types of gr\u0001aphs, the overall space required for an adjacency list is $O ( V + E )$ .",
      "metadata": {
        "content": "Directed graphs require slightly different definitions. A directed walk is a sequence of vertices $nu _ { 0 } {  } nu _ { 1 } {  } nu _ { 2 } {  } cdots {  } nu _ { ell }$ such that $nu _ { i - 1 } {  } nu _ { i }$ is a directed edge for every index $i$ ; directed\u0001path\u0001s an\u0001d dir\u0001ected cycles are de\u0001fined similarly. Vertex $nu$ is reachable from vertex $u$ in a directed graph $G$ if and only if $G$ contains a directed walk (and therefore a directed path) from $u$ to $nu$ . A directed graph is strongly connected if every vertex is reachable from every other vertex. A directed graph is acyclic if it does not contain a directed cycle; directed acyclic graphs are often called dags. \n5.3 Representations and Examples \nThe most common way to visually represent graphs is by drawing them. A drawing of a graph maps each vertex to a point in the plane (typically drawn as a small circle or some other shape) and each edge to a curve or straight line segment between the two vertices. A graph is planar if it has a drawing where no two edges cross; such a drawing is also called an embedding.5 The same graph can have many different drawings, so it is important not to confuse a particular drawing with the graph itself. In particular, planar graphs can have non-planar drawings! \nHowever, drawings are far from the only useful representation of graphs. For example, the intersection graph of a collection of geometric objects has a node for every object and an edge for every intersecting pair of objects. Whether a particular graph can be represented as an intersection graph depends on what kind of object you want to use for the vertices. Different types of objects—line segments, rectangles, circles, etc.—define different classes of graphs. One particularly useful type of intersection graph is an interval graph, whose vertices are intervals on the real line, with an edge between any two intervals that overlap. \n\nAnother good example is the dependency graph of a recursive algorithm. Dependency graphs are directed acyclic graphs. The vertices are all the distinct recursive subproblems that arise when executing the algorithm on a particular input. There is an edge from one subproblem to another if evaluating the second subproblem requires a recursive evaluation of the first. For example, for the Fibonacci recurrence \nthe vertices of the dependency graph are the integers $0 , 1 , 2 , ldots , n$ , and the edges are the pairs $( i - 1 ) { to } i$ and $( i - 2 ) substack {  i }$ for every integer $i$ between 2 and $n$ . \nAs a more complex example, recall the recurrence for the edit distance problem from Chapter 3: \nThe dependency graph of this recurrence is an $m times n$ grid of vertices $( i , j )$ connected by vertical edges $( i - 1 , j ) { to } ( i , j )$ , horizontal edges $( i , j - 1 ) {  } ( i , j )$ , and diagonal edges $( i - 1 , j - 1 ) { to } ( i , j )$ .\u0001Dynamic programming works effic\u0001iently for any recurrence that has a r\u0001easonably small dependency graph; a proper evaluation order ensures that each subproblem is visited after its predecessors. \nAnother interesting example is the configuration graph of a game, puzzle, or mechanism like tic-tac-toe, checkers, the Rubik’s Cube, the Tower of Hanoi, or a Turing machine. The vertices of the configuration graph are all the valid configurations of the puzzle; there is an edge from one configuration to another if it is possible to transform one configuration into the other with a single simple “move”. (Obviously, the precise definition depends on what moves are allowed.) Even for reasonably simple mechanisms, the configuration graph can be extremely complex, and we typically only have access to local information about the configuration graph. \nConfiguration graphs are close relatives of the game trees we considered in Chapter 2, but with one crucial difference. Each state of a game appears exactly once in its configuration graph, but can appear many times in its game tree. In short, configuration graphs are memoized game trees! \nFinite-state automata used in formal language theory can be modeled as labeled directed graphs. Recall that a deterministic finite-state automaton is formally defined as a 5-tuple $M = ( Sigma , Q , s , A , delta )$ , where $Sigma$ is a finite set called the alphabet, $Q$ is a finite set of states, $s in Q$ is the start state, $A subseteq Q$ is the set of accepting states, and $delta colon Q times Sigma to Q$ is a transition function. But it is often more useful to think of $M$ as a directed graph $G _ { M }$ whose vertices are the states $Q$ , and whose edges have the form $q to delta ( q , a )$ for every state $q in Q$ and symbol $a in Sigma$ . Basic questions about the la\u0001nguage $L ( M )$ accepted by $M$ can then be phrased as questions about the graph $G _ { M }$ . For example, $L ( M ) = emptyset$ if and only if no accepting state/vertex is reachable from the start state/vertex s. \n\nFinally, sometimes one graph can be used to implicitly represent other larger graphs. A good example of this implicit representation is the subset construction, which is normally used to convert NFAs into DFAs, but can be applied to arbitrary directed graphs as follows. Given any directed graph $G = left( V , E right)$ , we can define a new directed graph $G ^ { prime } = ( 2 ^ { V } , E ^ { prime } )$ whose vertices are all subsets of vertices in $V$ , and whose edges $E ^ { prime }$ are defined as follows: \nWe can mechanically translate this definition into an algorithm to construct $G ^ { prime }$ from $G$ , but strictly speaking, this construction is unnecessary, because $G$ is already an implicit representation of $G ^ { prime }$ . \nIt’s important not to confuse any of these examples/representations with the actual formal definition: A graph is a pair of sets $( V , E )$ , where $V$ is an arbitrary non-empty finite set, and $E$ is a set of pairs (either ordered or unordered) of elements of $V$ . In short: A graph is a set of pairs of things. \n5.4 Data Structures \nIn practice, graphs are usually represented by one of two standard data structures: adjacency lists and adjacency matrices. At a high level, both data structures are arrays indexed by vertices; this requires that each vertex has a unique integer identifier between 1 and $V$ . In a formal sense, these integers are the vertices. \nAdjacency Lists \nBy far the most common data structure for storing graphs is the adjacency list. An adjacency list is an array of lists, each containing the neighbors of one of the vertices (or the out-neighbors if the graph is directed).6 For undirected graphs, each edge $u nu$ is stored twice, once in u’s neighbor list and once in v’s neighbor list; for directed graphs, each edge $u {  } nu$ is stored only once, in the neighbor list of the tail $u$ . For both types of gr\u0001aphs, the overall space required for an adjacency list is $O ( V + E )$ .",
        "chapter": "Basic Graph Algorithms",
        "section": "Representations and Examples",
        "subsection": "N/A",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 69
      }
    },
    {
      "node_id": "tb1_node71",
      "content": "There are several different ways to represent these neighbor lists, but the standard implementation uses a simple singly-linked list. The resulting data structure allows us to list the (out-)neighbors of a node $nu$ in $O ( 1 + deg ( nu ) )$ time; just scan v’s neighbor list. Similarly, we can determine whether $u {  } nu$ is an edge in $O ( 1 + deg ( u ) )$ time by scanning the neighbor list of $u$ . For undir\u0001ected graphs, we can improve the time to $O ( 1 + operatorname* { m i n } { deg ( u ) , deg ( nu ) } )$ by simultaneously scanning the neighbor lists of both $u$ and $nu$ , stopping either when we locate the edge or when we fall of the end of a list. \nOf course, linked lists are not the only data structure we could use; any other structure that supports searching, listing, insertion, and deletion will do. For example, we can reduce the time to determine whether uv is an edge to $O ( 1 + log ( deg ( u ) ) )$ by using a balanced binary search tree to store the neighbors of $u$ , or even to $O ( 1 )$ time by using an appropriately constructed hash table.7 \nOne common implementation of adjacency lists is the adjacency array, which uses a single array to store all edge records, with the records of edges incident to each vertex in a contiguous interval, and with a separate array storing the index of the first edge incident to each vertex. Moreover, it is useful to keep the intervals for each vertex in sorted order, as shown in Figure 5.10, so that we can check in $O ( log mathrm { d e g } ( u ) )$ time whether two vertices $u$ and $nu$ are adjacent. \nAdjacency Matrices \nThe other standard data structure for graphs is the adjacency matrix,8 first proposed by Georges Brunel in $1 8 9 4$ . The adjacency matrix of a graph $G$ is a $V times V$ matrix of 0s and 1s, normally represented by a two-dimensional array $A [ 1 ldots V , 1 ldots V ]$ , where each entry indicates whether a particular edge is present in $G$ . Specifically, for all vertices $u$ and $nu$ : \n• if the graph is undirected, then $A [ u , nu ] : = 1$ if and only if $u nu in E$ , and • if the graph is directed, then $A [ u , nu ] : = 1$ if and only if $u {  } nu in E$ . \n\nFor undirected graphs, the adjacency matrix is always symmetric, meaning $A [ u , nu ] = A [ nu , u ]$ for all vertices $u$ and $nu$ , because $u nu$ and $nu u$ are just different names for the same edge, and the diagonal entries $A [ u , u ]$ are all zeros. For directed graphs, the adjacency matrix may or may not be symmetric, and the diagonal entries may or may not be zero. \nGiven an adjacency matrix, we can decide in $Theta ( 1 )$ time whether two vertices are connected by an edge just by looking in the appropriate slot in the matrix. We can also list all the neighbors of a vertex in $Theta ( V )$ time by scanning the corresponding row (or column). This running time is optimal in the worst case, but even if a vertex has few neighbors, we still have to scan the entire row to find them all. Similarly, adjacency matrices require $Theta ( V ^ { 2 } )$ space, regardless of how many edges the graph actually has, so they are only space-efficient for very dense graphs. \nComparison \nTable 5.1 summarizes the performance of the various standard graph data structures. Stars∗ indicate expected amortized time bounds for maintaining dynamic hash tables.9 \nIn light of this comparison, one might reasonably wonder why anyone would ever use an adjacency matrix; after all, adjacency lists with hash tables support the same operations in the same time, using less space. The main reason is that for sufficiently dense graphs, adjacency matrices are simpler and more efficient in practice, because they avoid the overhead of chasing pointers and computing hash functions; they’re just contiguous blocks of memory. \nSimilarly, why would anyone use linked lists in an adjacency list structure to store neighbors, instead of balanced binary search trees or hash tables? Although the primary reason in practice is almost surely tradition—If they were good enough for Donald Knuth’s code, they should be good enough for yours!—there are more principled arguments. One is that standard adjacency lists are in fact good enough for most applications. Most standard graph algorithms never (or rarely) actually ask whether an arbitrary edge is present or absent, or attempt to insert or delete edges, and so optimizing the data structures to support those operations is unnecessary. \nBut in my opinion, the most compelling reason for both standard data structures is that many graphs are implicitly represented by adjacency matrices and standard adjacency lists. For example: \n• Intersection graphs are usually represented as a list of the underlying geometric objects. As long as we can test whether two objects intersect in constant time, we can apply any graph algorithm to an intersection graph by pretending that the input graph is stored explicitly as an adjacency matrix. • Any data structure composed from records with pointers between them can be seen as a directed graph. Graph algorithms can be applied to these data structures by pretending that the graph is stored in a standard adjacency list.",
      "metadata": {
        "content": "There are several different ways to represent these neighbor lists, but the standard implementation uses a simple singly-linked list. The resulting data structure allows us to list the (out-)neighbors of a node $nu$ in $O ( 1 + deg ( nu ) )$ time; just scan v’s neighbor list. Similarly, we can determine whether $u {  } nu$ is an edge in $O ( 1 + deg ( u ) )$ time by scanning the neighbor list of $u$ . For undir\u0001ected graphs, we can improve the time to $O ( 1 + operatorname* { m i n } { deg ( u ) , deg ( nu ) } )$ by simultaneously scanning the neighbor lists of both $u$ and $nu$ , stopping either when we locate the edge or when we fall of the end of a list. \nOf course, linked lists are not the only data structure we could use; any other structure that supports searching, listing, insertion, and deletion will do. For example, we can reduce the time to determine whether uv is an edge to $O ( 1 + log ( deg ( u ) ) )$ by using a balanced binary search tree to store the neighbors of $u$ , or even to $O ( 1 )$ time by using an appropriately constructed hash table.7 \nOne common implementation of adjacency lists is the adjacency array, which uses a single array to store all edge records, with the records of edges incident to each vertex in a contiguous interval, and with a separate array storing the index of the first edge incident to each vertex. Moreover, it is useful to keep the intervals for each vertex in sorted order, as shown in Figure 5.10, so that we can check in $O ( log mathrm { d e g } ( u ) )$ time whether two vertices $u$ and $nu$ are adjacent. \nAdjacency Matrices \nThe other standard data structure for graphs is the adjacency matrix,8 first proposed by Georges Brunel in $1 8 9 4$ . The adjacency matrix of a graph $G$ is a $V times V$ matrix of 0s and 1s, normally represented by a two-dimensional array $A [ 1 ldots V , 1 ldots V ]$ , where each entry indicates whether a particular edge is present in $G$ . Specifically, for all vertices $u$ and $nu$ : \n• if the graph is undirected, then $A [ u , nu ] : = 1$ if and only if $u nu in E$ , and • if the graph is directed, then $A [ u , nu ] : = 1$ if and only if $u {  } nu in E$ . \n\nFor undirected graphs, the adjacency matrix is always symmetric, meaning $A [ u , nu ] = A [ nu , u ]$ for all vertices $u$ and $nu$ , because $u nu$ and $nu u$ are just different names for the same edge, and the diagonal entries $A [ u , u ]$ are all zeros. For directed graphs, the adjacency matrix may or may not be symmetric, and the diagonal entries may or may not be zero. \nGiven an adjacency matrix, we can decide in $Theta ( 1 )$ time whether two vertices are connected by an edge just by looking in the appropriate slot in the matrix. We can also list all the neighbors of a vertex in $Theta ( V )$ time by scanning the corresponding row (or column). This running time is optimal in the worst case, but even if a vertex has few neighbors, we still have to scan the entire row to find them all. Similarly, adjacency matrices require $Theta ( V ^ { 2 } )$ space, regardless of how many edges the graph actually has, so they are only space-efficient for very dense graphs. \nComparison \nTable 5.1 summarizes the performance of the various standard graph data structures. Stars∗ indicate expected amortized time bounds for maintaining dynamic hash tables.9 \nIn light of this comparison, one might reasonably wonder why anyone would ever use an adjacency matrix; after all, adjacency lists with hash tables support the same operations in the same time, using less space. The main reason is that for sufficiently dense graphs, adjacency matrices are simpler and more efficient in practice, because they avoid the overhead of chasing pointers and computing hash functions; they’re just contiguous blocks of memory. \nSimilarly, why would anyone use linked lists in an adjacency list structure to store neighbors, instead of balanced binary search trees or hash tables? Although the primary reason in practice is almost surely tradition—If they were good enough for Donald Knuth’s code, they should be good enough for yours!—there are more principled arguments. One is that standard adjacency lists are in fact good enough for most applications. Most standard graph algorithms never (or rarely) actually ask whether an arbitrary edge is present or absent, or attempt to insert or delete edges, and so optimizing the data structures to support those operations is unnecessary. \nBut in my opinion, the most compelling reason for both standard data structures is that many graphs are implicitly represented by adjacency matrices and standard adjacency lists. For example: \n• Intersection graphs are usually represented as a list of the underlying geometric objects. As long as we can test whether two objects intersect in constant time, we can apply any graph algorithm to an intersection graph by pretending that the input graph is stored explicitly as an adjacency matrix. • Any data structure composed from records with pointers between them can be seen as a directed graph. Graph algorithms can be applied to these data structures by pretending that the graph is stored in a standard adjacency list.",
        "chapter": "Basic Graph Algorithms",
        "section": "Data Structures",
        "subsection": "Adjacency Matrices",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 71
      }
    },
    {
      "node_id": "tb1_node70",
      "content": "Finally, sometimes one graph can be used to implicitly represent other larger graphs. A good example of this implicit representation is the subset construction, which is normally used to convert NFAs into DFAs, but can be applied to arbitrary directed graphs as follows. Given any directed graph $G = left( V , E right)$ , we can define a new directed graph $G ^ { prime } = ( 2 ^ { V } , E ^ { prime } )$ whose vertices are all subsets of vertices in $V$ , and whose edges $E ^ { prime }$ are defined as follows: \nWe can mechanically translate this definition into an algorithm to construct $G ^ { prime }$ from $G$ , but strictly speaking, this construction is unnecessary, because $G$ is already an implicit representation of $G ^ { prime }$ . \nIt’s important not to confuse any of these examples/representations with the actual formal definition: A graph is a pair of sets $( V , E )$ , where $V$ is an arbitrary non-empty finite set, and $E$ is a set of pairs (either ordered or unordered) of elements of $V$ . In short: A graph is a set of pairs of things. \n5.4 Data Structures \nIn practice, graphs are usually represented by one of two standard data structures: adjacency lists and adjacency matrices. At a high level, both data structures are arrays indexed by vertices; this requires that each vertex has a unique integer identifier between 1 and $V$ . In a formal sense, these integers are the vertices. \nAdjacency Lists \nBy far the most common data structure for storing graphs is the adjacency list. An adjacency list is an array of lists, each containing the neighbors of one of the vertices (or the out-neighbors if the graph is directed).6 For undirected graphs, each edge $u nu$ is stored twice, once in u’s neighbor list and once in v’s neighbor list; for directed graphs, each edge $u {  } nu$ is stored only once, in the neighbor list of the tail $u$ . For both types of gr\u0001aphs, the overall space required for an adjacency list is $O ( V + E )$ . \nThere are several different ways to represent these neighbor lists, but the standard implementation uses a simple singly-linked list. The resulting data structure allows us to list the (out-)neighbors of a node $nu$ in $O ( 1 + deg ( nu ) )$ time; just scan v’s neighbor list. Similarly, we can determine whether $u {  } nu$ is an edge in $O ( 1 + deg ( u ) )$ time by scanning the neighbor list of $u$ . For undir\u0001ected graphs, we can improve the time to $O ( 1 + operatorname* { m i n } { deg ( u ) , deg ( nu ) } )$ by simultaneously scanning the neighbor lists of both $u$ and $nu$ , stopping either when we locate the edge or when we fall of the end of a list. \nOf course, linked lists are not the only data structure we could use; any other structure that supports searching, listing, insertion, and deletion will do. For example, we can reduce the time to determine whether uv is an edge to $O ( 1 + log ( deg ( u ) ) )$ by using a balanced binary search tree to store the neighbors of $u$ , or even to $O ( 1 )$ time by using an appropriately constructed hash table.7 \nOne common implementation of adjacency lists is the adjacency array, which uses a single array to store all edge records, with the records of edges incident to each vertex in a contiguous interval, and with a separate array storing the index of the first edge incident to each vertex. Moreover, it is useful to keep the intervals for each vertex in sorted order, as shown in Figure 5.10, so that we can check in $O ( log mathrm { d e g } ( u ) )$ time whether two vertices $u$ and $nu$ are adjacent. \nAdjacency Matrices \nThe other standard data structure for graphs is the adjacency matrix,8 first proposed by Georges Brunel in $1 8 9 4$ . The adjacency matrix of a graph $G$ is a $V times V$ matrix of 0s and 1s, normally represented by a two-dimensional array $A [ 1 ldots V , 1 ldots V ]$ , where each entry indicates whether a particular edge is present in $G$ . Specifically, for all vertices $u$ and $nu$ : \n• if the graph is undirected, then $A [ u , nu ] : = 1$ if and only if $u nu in E$ , and • if the graph is directed, then $A [ u , nu ] : = 1$ if and only if $u {  } nu in E$ .",
      "metadata": {
        "content": "Finally, sometimes one graph can be used to implicitly represent other larger graphs. A good example of this implicit representation is the subset construction, which is normally used to convert NFAs into DFAs, but can be applied to arbitrary directed graphs as follows. Given any directed graph $G = left( V , E right)$ , we can define a new directed graph $G ^ { prime } = ( 2 ^ { V } , E ^ { prime } )$ whose vertices are all subsets of vertices in $V$ , and whose edges $E ^ { prime }$ are defined as follows: \nWe can mechanically translate this definition into an algorithm to construct $G ^ { prime }$ from $G$ , but strictly speaking, this construction is unnecessary, because $G$ is already an implicit representation of $G ^ { prime }$ . \nIt’s important not to confuse any of these examples/representations with the actual formal definition: A graph is a pair of sets $( V , E )$ , where $V$ is an arbitrary non-empty finite set, and $E$ is a set of pairs (either ordered or unordered) of elements of $V$ . In short: A graph is a set of pairs of things. \n5.4 Data Structures \nIn practice, graphs are usually represented by one of two standard data structures: adjacency lists and adjacency matrices. At a high level, both data structures are arrays indexed by vertices; this requires that each vertex has a unique integer identifier between 1 and $V$ . In a formal sense, these integers are the vertices. \nAdjacency Lists \nBy far the most common data structure for storing graphs is the adjacency list. An adjacency list is an array of lists, each containing the neighbors of one of the vertices (or the out-neighbors if the graph is directed).6 For undirected graphs, each edge $u nu$ is stored twice, once in u’s neighbor list and once in v’s neighbor list; for directed graphs, each edge $u {  } nu$ is stored only once, in the neighbor list of the tail $u$ . For both types of gr\u0001aphs, the overall space required for an adjacency list is $O ( V + E )$ . \nThere are several different ways to represent these neighbor lists, but the standard implementation uses a simple singly-linked list. The resulting data structure allows us to list the (out-)neighbors of a node $nu$ in $O ( 1 + deg ( nu ) )$ time; just scan v’s neighbor list. Similarly, we can determine whether $u {  } nu$ is an edge in $O ( 1 + deg ( u ) )$ time by scanning the neighbor list of $u$ . For undir\u0001ected graphs, we can improve the time to $O ( 1 + operatorname* { m i n } { deg ( u ) , deg ( nu ) } )$ by simultaneously scanning the neighbor lists of both $u$ and $nu$ , stopping either when we locate the edge or when we fall of the end of a list. \nOf course, linked lists are not the only data structure we could use; any other structure that supports searching, listing, insertion, and deletion will do. For example, we can reduce the time to determine whether uv is an edge to $O ( 1 + log ( deg ( u ) ) )$ by using a balanced binary search tree to store the neighbors of $u$ , or even to $O ( 1 )$ time by using an appropriately constructed hash table.7 \nOne common implementation of adjacency lists is the adjacency array, which uses a single array to store all edge records, with the records of edges incident to each vertex in a contiguous interval, and with a separate array storing the index of the first edge incident to each vertex. Moreover, it is useful to keep the intervals for each vertex in sorted order, as shown in Figure 5.10, so that we can check in $O ( log mathrm { d e g } ( u ) )$ time whether two vertices $u$ and $nu$ are adjacent. \nAdjacency Matrices \nThe other standard data structure for graphs is the adjacency matrix,8 first proposed by Georges Brunel in $1 8 9 4$ . The adjacency matrix of a graph $G$ is a $V times V$ matrix of 0s and 1s, normally represented by a two-dimensional array $A [ 1 ldots V , 1 ldots V ]$ , where each entry indicates whether a particular edge is present in $G$ . Specifically, for all vertices $u$ and $nu$ : \n• if the graph is undirected, then $A [ u , nu ] : = 1$ if and only if $u nu in E$ , and • if the graph is directed, then $A [ u , nu ] : = 1$ if and only if $u {  } nu in E$ .",
        "chapter": "Basic Graph Algorithms",
        "section": "Data Structures",
        "subsection": "Adjacency Lists",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 70
      }
    },
    {
      "node_id": "tb1_node74",
      "content": "Every pair $( nu , p a r e n t ( nu ) )$ with $p a r e n t ( nu ) neq emptyset$ is actually an edge in the underlying graph $G$ . We claim that for any marked vertex $nu$ , the path of parent edges $nu {  } p a r e n t ( nu ) {  } p a r e n t ( p a r e n t ( nu ) ) {  } cdots$ eventually leads back to $s$ ; we prove this clai\u0001m by induct\u0001ion on the order in\u0001which vertices are marked. Trivially $s$ is reachable from s, so let $nu$ be any other marked vertex. The parent of $nu$ must be marked before $nu$ is marked, so the inductive hypothesis implies that the parent path paren $cdot ( nu ) {  } p a r e n t ( p a r e n t ( nu ) ) {  } cdots$ leads to $s$ ; adding one more parent edge $s {  } p a r e n t ( s )$ est\u0001ablishes the claim. \nThe previous claim implies that every vertex marked by the algorithm is reachable from s, and that the set of all parent edges forms a connected graph. Because every marked node except $s$ has a unique parent, the number of parent edges is exactly one less than the number of marked vertices. We conclude that the parent edges form a tree. □ \nAnalysis \nThe running time of the traversal algorithm depends on what data structure we use for the “bag”, but we can make a few general observations. Let $T$ is the time required to insert a single item into the bag or delete a single item from the bag. The for loop $( dag )$ is executed exactly once for each marked vertex, and therefore at most $V$ times. Each edge $u nu$ in the component of $s$ is put into the bag exactly twice; once as the pair $left( u , nu right)$ and once as the pair $( nu , u )$ , so line $( { star } { star } )$ is executed at most $2 E$ times. Finally, we can’t take more things out of the bag than we put in, so line $( { star } )$ is executed at most $2 E + 1$ times. Thus, assuming the underlying graph $G$ is stored in a standard adjacency list, WhateverFirstSearch runs in $O ( V + E T )$ time. (If $G$ is stored in an adjacency matrix, the running time of WhateverFirstSearch increases to $O ( V ^ { 2 } + E T ) .$ ) \n5.6 Important Variants \nStack: Depth-First \nIf we implement the “bag” using a stack, we recover our original depth-first search algorithm. Stacks support insertions (push) and deletions (pop) in $O ( 1 )$ time each, so the algorithm runs in $O ( V + E )$ time. The spanning tree formed by the parent edges is called a depth-first spanning tree. The exact shape of the tree depends on the start vertex and on the order that neighbors are visited inside the for loop $( dagger )$ , but in general, depth-first spanning trees are long and skinny. We will consider several important properties and applications of depth-first search in Chapter 6. \n\nQueue: Breadth-First \nIf we implement the “bag” using a queue, we get a different graph-traversal algorithm called breadth-first search. Queues support insertions (push) and deletions (pull) in $O ( 1 )$ time each, so the algorithm runs in $O ( V + E )$ time. In this case, the breadth-first spanning tree formed by the parent edges contains shortest paths from the start vertex $s$ to every other vertex in its component; we will consider shortest paths in detail in Chapter 8. Again, the exact shape of a breadth-first spanning tree depends on the start vertex and on the order that neighbors are visited in the for loop $( dag )$ , but in general, breadth-first spanning trees are short and bushy. \nPriority Queue: Best-First \nFinally, if we implement the “bag” using a priority queue, we get yet another family of algorithms called best-first search. Because the priority queue stores at most one copy of each edge, inserting an edge or extracting the minimumpriority edge requires $O ( log E )$ time, which implies that best-first search runs in $O ( V + E log E )$ time. \nI describe best-first search as a “family of algorithms”, rather than a single algorithm, because there are different methods to assign priorities to the edges, and these choices lead to different algorithmic behavior. I’ll describe three well-known variants below, but there are many others. In all three examples, we assume that every edge $u nu$ or $u {  } nu$ in the input graph has a non-negative weight $w ( u nu )$ or $w ( u to nu )$ . \nFirst, if the input\u0001graph is undirected and we use the weight of each edge as its priority, best-first search constructs the minimum spanning tree of the component of s. Surprisingly, as long as all the edge weights are distinct, the resulting tree does not depend on the start vertex or the order that neighbors are visited; in this case, the minimum spanning tree is actually unique. This instantiation of best-first search is commonly (but, as usual, incorrectly) known as Prim’s algorithm; we’ll discuss this and other minimum-spanning-trees in more detail in Chapter 7.",
      "metadata": {
        "content": "Every pair $( nu , p a r e n t ( nu ) )$ with $p a r e n t ( nu ) neq emptyset$ is actually an edge in the underlying graph $G$ . We claim that for any marked vertex $nu$ , the path of parent edges $nu {  } p a r e n t ( nu ) {  } p a r e n t ( p a r e n t ( nu ) ) {  } cdots$ eventually leads back to $s$ ; we prove this clai\u0001m by induct\u0001ion on the order in\u0001which vertices are marked. Trivially $s$ is reachable from s, so let $nu$ be any other marked vertex. The parent of $nu$ must be marked before $nu$ is marked, so the inductive hypothesis implies that the parent path paren $cdot ( nu ) {  } p a r e n t ( p a r e n t ( nu ) ) {  } cdots$ leads to $s$ ; adding one more parent edge $s {  } p a r e n t ( s )$ est\u0001ablishes the claim. \nThe previous claim implies that every vertex marked by the algorithm is reachable from s, and that the set of all parent edges forms a connected graph. Because every marked node except $s$ has a unique parent, the number of parent edges is exactly one less than the number of marked vertices. We conclude that the parent edges form a tree. □ \nAnalysis \nThe running time of the traversal algorithm depends on what data structure we use for the “bag”, but we can make a few general observations. Let $T$ is the time required to insert a single item into the bag or delete a single item from the bag. The for loop $( dag )$ is executed exactly once for each marked vertex, and therefore at most $V$ times. Each edge $u nu$ in the component of $s$ is put into the bag exactly twice; once as the pair $left( u , nu right)$ and once as the pair $( nu , u )$ , so line $( { star } { star } )$ is executed at most $2 E$ times. Finally, we can’t take more things out of the bag than we put in, so line $( { star } )$ is executed at most $2 E + 1$ times. Thus, assuming the underlying graph $G$ is stored in a standard adjacency list, WhateverFirstSearch runs in $O ( V + E T )$ time. (If $G$ is stored in an adjacency matrix, the running time of WhateverFirstSearch increases to $O ( V ^ { 2 } + E T ) .$ ) \n5.6 Important Variants \nStack: Depth-First \nIf we implement the “bag” using a stack, we recover our original depth-first search algorithm. Stacks support insertions (push) and deletions (pop) in $O ( 1 )$ time each, so the algorithm runs in $O ( V + E )$ time. The spanning tree formed by the parent edges is called a depth-first spanning tree. The exact shape of the tree depends on the start vertex and on the order that neighbors are visited inside the for loop $( dagger )$ , but in general, depth-first spanning trees are long and skinny. We will consider several important properties and applications of depth-first search in Chapter 6. \n\nQueue: Breadth-First \nIf we implement the “bag” using a queue, we get a different graph-traversal algorithm called breadth-first search. Queues support insertions (push) and deletions (pull) in $O ( 1 )$ time each, so the algorithm runs in $O ( V + E )$ time. In this case, the breadth-first spanning tree formed by the parent edges contains shortest paths from the start vertex $s$ to every other vertex in its component; we will consider shortest paths in detail in Chapter 8. Again, the exact shape of a breadth-first spanning tree depends on the start vertex and on the order that neighbors are visited in the for loop $( dag )$ , but in general, breadth-first spanning trees are short and bushy. \nPriority Queue: Best-First \nFinally, if we implement the “bag” using a priority queue, we get yet another family of algorithms called best-first search. Because the priority queue stores at most one copy of each edge, inserting an edge or extracting the minimumpriority edge requires $O ( log E )$ time, which implies that best-first search runs in $O ( V + E log E )$ time. \nI describe best-first search as a “family of algorithms”, rather than a single algorithm, because there are different methods to assign priorities to the edges, and these choices lead to different algorithmic behavior. I’ll describe three well-known variants below, but there are many others. In all three examples, we assume that every edge $u nu$ or $u {  } nu$ in the input graph has a non-negative weight $w ( u nu )$ or $w ( u to nu )$ . \nFirst, if the input\u0001graph is undirected and we use the weight of each edge as its priority, best-first search constructs the minimum spanning tree of the component of s. Surprisingly, as long as all the edge weights are distinct, the resulting tree does not depend on the start vertex or the order that neighbors are visited; in this case, the minimum spanning tree is actually unique. This instantiation of best-first search is commonly (but, as usual, incorrectly) known as Prim’s algorithm; we’ll discuss this and other minimum-spanning-trees in more detail in Chapter 7.",
        "chapter": "Basic Graph Algorithms",
        "section": "Important Variants",
        "subsection": "Stack: Depth-First",
        "subsubsection": "N/A",
        "textbook_id": 1,
        "node_index": 74
      }
    }
  ],
  "output": {
    "question": "In the context of graph data structures, snippet [1] mentions that dynamic hash tables have expected amortized time bounds, while snippet [5] discusses representing neighbor lists for graph nodes. Considering that hash tables can be used to implement adjacency lists, how might the amortized time complexity of dynamic hash tables affect the efficiency of operations like listing all neighbors of a node when compared to using a simple singly-linked list for the neighbor list?",
    "correct_answer": "Using dynamic hash tables for adjacency lists can provide O(1) expected amortized time for edge lookups and insertions, which is faster than the O(deg(v)) time to scan a singly-linked list for neighbor listing, but hash tables may have higher constant factors and memory overhead.",
    "distractors": [
      "Dynamic hash tables always guarantee O(1) worst-case time for all operations, making them strictly better than linked lists for neighbor listing.",
      "Singly-linked lists provide O(1) time for neighbor listing regardless of degree, so they are always more efficient than hash tables.",
      "Hash tables cannot be used for adjacency lists because they do not support sequential access required for listing all neighbors."
    ],
    "rationale": "Step 1: Identify relevant concepts from snippet [1]: It mentions 'expected amortized time bounds for maintaining dynamic hash tables' in the context of comparing graph data structures. Step 2: Identify relevant concepts from snippet [5]: It discusses 'standard implementation uses a simple singly-linked list' for neighbor lists and that 'list the (out-)neighbors of a node ν in O(1 + deg(ν)) time'. Step 3: Connect the concepts: Both snippets relate to graph representations—snippet [1] hints at hash tables as a data structure option, and snippet [5] describes linked lists for adjacency lists. The question synthesizes this by asking how the amortized complexity of hash tables (from snippet [1]) impacts operations like neighbor listing compared to linked lists (from snippet [5]), requiring analysis of time efficiency trade-offs in graph implementations."
  },
  "score": {
    "relevance": 1.0,
    "faithfulness": 1.0,
    "integration": 1.0,
    "complexity": 1.0,
    "overall": 1.0
  }
}